\documentclass[uplatex,dvipdfmx]{jsreport}
\title{数理統計学}
\author{司馬博文}
\date{\today}
\pagestyle{headings} \setcounter{secnumdepth}{4}
%\input{/home/hirofumi/StatisticalNature/preamble_no_fonts.tex}
\input{/Users/hirofumi.shiba48/StatisticalNature/preamble_no_fonts.tex}
\usepackage[math]{anttor}
\begin{document}
\tableofcontents

\chapter{線型推測論}

\begin{quotation}
    線型推測論が統計モデルの線形代数である．

    Andersonによる標準的教科書"An Introduction to Multivariate Statistical Analysis" (1958)が多変量解析の，
    George BoxとJenkinsによるTime Series Analysis, Forecasting and Control" (1976)が時系列解析の最初の金字塔であるが，
    これらはいずれも線形モデルを扱っている．
    大規模計算機時代を迎えて以降，非線形モデルが発展の中心となったが，応用における一次近似と，その後の非線形解析の方向性を見るための試金石としての意味で，線形モデルの意義は不動である．
    例えば，数学的に整理された解析力学を差し置いて，工学の場面ではNewton的力学がもっとも重要になることに等しい．
    実際，$X$のデザインを非線形にすることが可能であるから，線形性の仮定は応用上は強い制約ではない．

    また，正規近似の手法で用いられる，正規分布から派生する確率分布（カイ2乗分布，$t$分布，$F$分布）についてまとめる．
\end{quotation}

\section{射影行列と逆行列}

\subsection{射影}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    Euclid空間$\R^n$では，任意の自己作用素は有界でである．
    よって，冪等律を満たす行列を射影行列といい，さらに自己共役であるものを直交射影という．
\end{tcolorbox}

\begin{lemma}[射影の特徴付け]
    線型写像$P:\R^n\to\R^n$について，
    \begin{enumerate}
        \item $P\circ P=P$．
        \item $\R^n=\Im P\oplus\Ker P$と表せる．
    \end{enumerate}
    また，次の２条件とも同値．
    \begin{enumerate}\setcounter{enumi}{2}
        \item $Q\in\GL_n(\R)$が存在して，$Q^{-1}PQ=\diag[1,\cdots,1,0,\cdots,0]$．
        \item $\rank P+\rank(I_n-P)=n$．
    \end{enumerate}
    なお，このとき$\rank P=\Tr P=r$である．
\end{lemma}

\begin{lemma}
    射影$P\in M_n(\R)$について，
    \begin{enumerate}
        \item $P$は直交射影である．
        \item $P^\top=P$．
    \end{enumerate}
\end{lemma}

\subsection{一般化逆行列}

\begin{notation}\mbox{}
    \begin{enumerate}
        \item 行列$A\in M_{mn}(\R)$の列ベクトルではられる線型空間を$L[A]\subset\R^n$と表す．
        \item $X\in M_{np}(\R)$に対して，$L[X]$が定める直交射影を$P_X$で表す．
        \item $H\in M_{pq}(\R)$は$L[H]\subset L[X^\top]$かつ$\rank H=q$を満たすとする．
        \[L[X|H]:=X((\Im H)^\perp)=\Brace{X\gamma\in\R^n\mid\gamma\in\Ker H^\top=(\Im H)^\perp}\]
        への直交射影を$P_{X|H}$で表す．$r:=\rank X$とする．
        なお，$\forall_{T\in B(H)}\;\Ker T^*=(\Im T)^\perp$なので，$\Ker H^\top=(\Im H)^\perp$．
    \end{enumerate}
\end{notation}

\begin{definition}[generalized inverse matrix]
    行列$A\in M_{mn}(\R)$に対して，$AA^-A=A$を満足する$A^-\in M_{nm}(\R)$を，$A$の\textbf{一般化逆行列}という．
\end{definition}

\begin{lemma}
    行列$A\in M_{mn}(\R)$について，
    \begin{enumerate}
        \item 一般逆$A^-$は存在する．
        \item 一意とは限らない．
    \end{enumerate}
\end{lemma}
\begin{proof}\mbox{}
    \begin{enumerate}
        \item ある$B\in\GL_m(\R),C\in\GL_n(\R)$について，$D:=BAC=\diag[1,\cdots,1,0,\cdots,0]$．$A^-:=CD^\top B$とおくと，
        $A=B^{-1}DC^{-1}$より，
        $AA^-A=ACD^\top BA=B^{-1}DC^{-1}CD^\top BB^{-1}DC^{-1}=B^{-1}DD^\top DC^{-1}=B^{-1}DC^{-1}=A$．
        \item $A:=(1\;0;0\;0)$について，$A$自身も，$B:=(1\;1;0\;0)$も一般逆である．
    \end{enumerate}
\end{proof}

\begin{theorem}
    $X\in M_{np}(\R)$とする．$X^\top X$の任意の一般化逆行列$(X^\top X)^-$に対して，$P_X=X(X^\top X)^-X^\top$．
\end{theorem}

\begin{theorem}
    $X\in M_{np}(\R)$とする．$X^\top X$の任意の一般化逆行列$(X^\top X)^-$に対して，$G:=(X^\top X)^-H$と定める．
    このとき，$G^\top X^\top XG$は正則であり，\[P_{X|H}=P_X-XG(G^\top X^\top XG)^{-1}G^\top X^\top.\]
    また，$\rank P_{X|H}=r-q$である．
\end{theorem}

\subsection{ブロック行列の逆}

\begin{notation}
    $A\in M_p(\R),B\in M_{pq}(\R),C\in M_{qp}(\R),D\in M_{q}(\R)$について，$M:=[A\;B;C\;D]\in M_{p+q}(\R)$とする．
    $D\in\GL_q(\R)$のとき$F:=A-BD^{-1}C$，$A\in\GL_p(\R)$のとき$G:=D-CA^{-1}B$とおく．
\end{notation}

\begin{proposition}
    \begin{enumerate}
        \item $D$が正則であるとき，$\abs{M}=\abs{F}\abs{D}$．特に，$D,M$が正則であることと，$D,F$が正則であることとは同値．
        \item $D,M$が正則であるとき，
        \[M^{-1}=\begin{bmatrix}F^{-1}&-F^{-1}BD^{-1}\\-D^{-1}CF^{-1}&D^{-1}+D^{-1}CF^{-1}BD^{-1}\end{bmatrix}.\]
        \item $A$が正則であるとき，$\abs{M}=\abs{A}\abs{G}$．特に，$A,M$が正則であることと，$A,G$が正則であることとは同値．
        \item $A,M$が正則であるとき，
        \[M^{-1}=\begin{bmatrix}A^{-1}+A^{-1}BG^{-1}CA^{-1}&-A^{-1}BG^{-1}\\-G^{-1}CA^{-1}&G^{-1}\end{bmatrix}.\]
    \end{enumerate}
\end{proposition}

\section{カイ２乗分布}

\begin{notation}
    Gamma分布$G(\al,\nu):\R_{>0}^2\to P(\R,\B_1)$の確率密度関数は
    \[g(x;\al,\nu)=\frac{1}{\Gamma(\nu)}\al^\nu x^{\nu-1}e^{-\al x}1_{x>0},\qquad\Gamma(\nu)=\int^\infty_0t^{\nu-1}e^{-t}dt.\]
    と表せる．
\end{notation}

\begin{definition}[noncentral chi-square distribution with $k$ degree of freedom and noncentrality parameter $\delta$]
    $(X_j)_{j\in[k]}$を$X_1\sim N(\nu,1),X_j\sim N(0,1)\;(j=2,\cdots,k)$を満たす独立な実確率変数列とする．
    このとき，二乗和で定まる確率変数$Y:=\sum^k_{j=1}X^2_j$が定める分布を\textbf{自由度$k$，非心率$\delta=\mu^2$の非心カイ2乗分布}といい，$\chi^2(k,\delta)$で表す．
\end{definition}
\begin{remark}
    $\delta=0$のとき，$\chi^2(k)=\chi^2(k,0)$をカイ2乗分布という．
    自由度$k=0$の非心カイ2乗分布は$x=0$上のデルタ測度とし，非心率は$\delta=0$とする．
\end{remark}

\begin{proposition}
    $\chi^2(k,\delta=\mu^2)$について，
    \begin{enumerate}
        \item 確率密度関数は$f(x):=e^{-\frac{\mu^2}{2}}\sum^\infty_{r=0}\frac{1}{r!}\paren{\frac{\mu^2}{2}}^rg\paren{x;\frac{1}{2},r+\frac{k}{2}}$．
        \item 特性関数は
        \[\varphi(u)=e^{-\frac{\mu^2}{2}}\sum^\infty_{r=0}\frac{1}{r!}\paren{\frac{\mu^2}{2}}^r(1-2iu)^{-r-\frac{k}{2}}=(1-2iu)^{-\frac{k}{2}}\exp\paren{\frac{\mu^2iu}{1-2iu}}\]
    \end{enumerate}
\end{proposition}

\begin{corollary}
    $X_j\sim N(\mu_j,1)\;(j\in[k])$を独立な実確率変数列とする．このとき，$\delta:=\sum^k_{j=1}\mu^2_j$とすると，$\sum^k_{j=1}X^2_j\sim\chi^2(k,\delta)$．
\end{corollary}

\begin{corollary}
    $(X_j)\;(j\in[k])$を，$\chi^2(k_j,\delta_j)$に従う独立同分布確率変数列とする．このとき，$\sum^k_{j=1}X_j\sim\chi^2\paren{\sum^k_{j=1}k_j,\sum^k_{j=1}\delta_j}$．
\end{corollary}

\section{フィッシャー・コクランの定理}

\begin{proposition}
    $A_i\in M_n(\R)\;(i\in[k])$は$\sum_{i=1}^kA_i=I_n$を満たすとする．
    このとき，次の４条件は同値．
    \begin{enumerate}
        \item $\forall_{i\in[k]}\;A_i^2=A_i$．
        \item $\forall_{i,j\in[k]}i\ne j\Rightarrow A_iA_j=O$．
        \item $\R^n=\bigoplus_{i=1}^k\Im A_i$と直和分解される．
        \item $\sum^k_{i=1}\rank A_i=n$．
    \end{enumerate}
\end{proposition}

\begin{notation}
    $\mu:=(\mu_1,\cdots,\mu_n)^\top\in\R^n$，$Y=(Y_1,\cdots,Y_n)^\top\sim N_n(\nu,I_n)$とする．
\end{notation}

\begin{theorem}[Fisher-Cochran]
    $A_i\in M_n(\R)\;(i\in[k])$は$\sum_{i=1}^kA_i=I_n$を満たす対称行列の列とする．
    $Q_i:=Y^\top AiY,\rank A_i=:n_i$とおく．このとき，次の５条件は同値．
    \begin{enumerate}
        \item $\exists_{\delta_i\ge0}\;Q_i\sim\chi^2(n_i,\delta_i)$かつ$Q_1,\cdots,Q_k$は独立．
        \item $\sum^k_{j=1}n_i=n$．
        \item $\forall_{i\in[k]}\;A^2_i=A_i$．
        \item $\forall_{i,j\in[k]}\;i\ne j\Rightarrow A_iA_j=O$．
        \item $\R^n=\bigoplus_{i\in[k]}\Im A_i$．
    \end{enumerate}
    さらにこのとき，$\delta_i=\mu^\top A_i\mu$と表わせ，特に$\sum^n_{j=1}\mu_j^2=\sum^k_{i=1}\delta_i$．
\end{theorem}

\begin{corollary}
    $A\in M_n(\R)$を対称行列とする．次の２条件は同値．
    \begin{enumerate}
        \item $\exists_{\delta\ge0}\;\exists_{k\in\Z_+}\;Q:=Y^\top AY\sim\chi^2(k,\delta)$．
        \item $A^2=A$．
    \end{enumerate}
    このとき，$k=\rank A,\delta=\mu^\top A\mu$が成り立つ．
\end{corollary}

\subsection{独立性}

\begin{corollary}
    $A_i\in M_n(\R)\;(i=1,2)$を対称行列とし，$Q_i:=Y^\top A_iY$は非心カイ2乗分布に従うとする．
    このとき，次の２条件は同値．
    \begin{enumerate}
        \item $Q_1\indep Q_2$．
        \item $A_1A_2=O$．
    \end{enumerate}
\end{corollary}

\begin{lemma}
    $A,B\in M_n(\R)$を対称な射影行列とする：$A^2=A,B^2=B$．
    差は非負値行列とする：$A-B\ge 0$．
    このとき，
    \begin{enumerate}
        \item $AB=BA=B$．
        \item $(A-B)^2=A-B$．
    \end{enumerate}
\end{lemma}

\begin{theorem}
    $A_i\in M_n(\R)\;(i=0,1,2)$を$A_0=A_1+A_2$を満たす対称行列とし，$Q_i:=Y^\top A_iY$は$Q_j\sim\chi^2(n_j,\delta_j)\;(j=0,1),Q_2\ge0\as$とする．
    このとき，$n_2:=n_0-n_1,\delta_2=\delta_0-\delta_1$について
    \begin{enumerate}
        \item $Q_2\sim\chi^2(n_2,\delta_2)$．
        \item $Q_1\indep Q_2$．
    \end{enumerate}
\end{theorem}

\section{$t$分布と$F$分布}

\subsection{$t$分布}

\begin{definition}[noncentral $t$-distribution]
    $Y\sim\chi^2(n),X\sim N(0,1)$は独立であるとする．このとき，$X:=\frac{Z}{\sqrt{\frac{Y}{n}}}$が定める分布を，\textbf{自由度$n$，非心率$\delta$の非心$t$分布}といい，$t(n,\delta)$で表す．
    特に$\delta=0$のとき$t(n):=t(n,0)$を\textbf{自由度$n$の$t$分布}という．
\end{definition}

\begin{proposition}[確率密度関数]
    
\end{proposition}

\subsection{$F$分布}

\begin{definition}[noncentral $F$-distribution]
    $Y_1\sim\chi^2(m,\delta),Y_2\sim\chi^2(n)$は独立であるとする．このとき，$X:=\frac{Y_1/m}{Y_2/m}$が定める分布を，\textbf{自由度$m,n$，非心率$\delta$の非心$F$分布}といい，$F(m,n,\delta)$で表す．
    特に$\delta=0$のとき$F(m,n):=F(m,n,0)$を\textbf{自由度$m,n$の$F$分布}という．
\end{definition}

\begin{proposition}[確率密度関数]
    
\end{proposition}

\section{ガウス・マルコフモデル}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $Y=X\beta+\ep$なる形で表せるモデルを線型回帰モデルといい，$X\in\R^p$の次元が$p\ge2$を満たすとき，重回帰という．
    この誤差項$\ep$について，$E[\ep]=0$かつ$\Var[\ep]=\sigma^2I_n$なる仮定(Gauss-Markov assumption)をおいた線型回帰モデルを\textbf{Gauss-Markovモデル}という．
\end{tcolorbox}

\begin{notation}[Gauss-Markov model]
    $n$個の実確率変数$Y_i\;(i\in[n])$が，定数関数$x_i,\beta:\Om\to\R^p$と，期待値$0$かつ互いに無相関で等分散を持つ$E[\ep_i]=0,E[\ep_i\ep_j]=\sigma^2\delta_{ij}$を満たす確率変数列$\ep_i:\Om\to\R$\footnote{独立ならば無相関である．}について
    \[Y_i=x_i^\top\beta+\ep_i\]
    なる関係を満たすとする．添字$i\in[n]$は観測の単位を表し，
    $x_i$を，$E[Y_i]$を説明する$p$個の要因を並べた変数，$\sigma^2$は観測の分散を表すパラメータである．
    \[Y:=\begin{bmatrix}Y_1\\\vdots\\Y_n\end{bmatrix},\quad\ep:=\begin{bmatrix}\ep_1\\\vdots\\\ep_n\end{bmatrix},\quad X:=\begin{bmatrix}x^\top_1\\\vdots\\x^\top_n\end{bmatrix}\]
    をそれぞれ$n$次元ベクトル値確率変数と$n\times p$次元定行列とすると，モデルは
    \[Y=X\beta+\ep,\quad E[\ep]=0,\quad\Var[\ep]=\sigma^2I_n.\]
    と表せる．すなわち，
    \[\P:=\Brace{\nu\in P(\R^n,\B_n)\mid\exists_{\beta\in\R^p} E[\nu]=X\beta,\exists_{\sigma\in(0,\infty)}\;\Var[\nu]=\sigma^2I_n}\]
    なるモデルである（正規分布よりも遥かに広いクラスである）．
\end{notation}

\subsection{正規方程式}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    結局，線型汎関数$\beta\mapsto c^\top\beta$が推定可能であるとき，正規方程式の解から最良線形不偏推定量が構成できる．
    これが$M$-推定量の雛形となる．
\end{tcolorbox}

\begin{definition}[sum of squared residuals, normal equation]
    未知パラメータ$\beta\in\R^p$の推定を考える．
    \[Q(\beta)=\SSR(\beta)=:=(Y-X\beta)^\top(Y-X\beta)=\sum^n_{i=1}(Y_i-x_i^\top\beta)^2\]
    について，$\wh{\beta}:=\argmin Q(\beta)$とすればよい．
    特にこれは，\textbf{正規方程式}
    \[\dd{Q}{\beta}=0\Leftrightarrow X^\top X\beta=X^\top Y\]
    の解である．
    この解は$L[X^\top X]=L[X^\top]$より，この解は常に存在し，$X^\top X$が正則であるとき一意的に存在する．
\end{definition}

\begin{theorem}
    $Y\in\R^n$とする．正規方程式の任意の解$\wh{\beta}$に対して，
    \begin{enumerate}
        \item $X\wh{\beta}=P_XY$．
        \item $\abs{Y-X\beta{\beta}}=\min_{\beta\in\R^p}\abs{Y-X\beta}$．
    \end{enumerate}
\end{theorem}

\subsection{最良線型不偏推定量}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    未知パラメータ$\beta,\sigma^2$の関数（主に線型汎関数）$g(\beta,\sigma^2)$の推定を考える．
    どの$(\beta,\sigma^2)\in\Pi\subset\R^p\times(0,\infty)$が真の値であろうと，良い結果がほしいとなると，これを不偏性という．
\end{tcolorbox}

\begin{definition}[unbiased estimator]
    統計量$\delta(Y):\Meas(\X,\R)\to\R$について，
    \[\forall_{(\beta,\sigma^2)\in\Pi}\quad E_{\beta,\sigma^2}[\delta(Y)]=g(\beta,\sigma^2)\]
    が成り立つとき，これを\textbf{不偏推定量}であるという．
\end{definition}

\begin{definition}[estimable]
    $c\in\R^p$について，$c\in L[X^\top]$のとき，線型汎関数$\R^p\ni\beta\mapsto c^\top\beta\in\R$を\textbf{推定可能}であるという．
\end{definition}

\begin{theorem}
    $a\in\R^n,c\in\R^p,\beta\in\R^p$について，
    \begin{enumerate}
        \item 次の２条件は同値．
        \begin{enumerate}[(a)]
            \item 確率変数$Y$の線型関数$a^\top Y$が$\beta$の線型関数$c^\top\beta$の不偏推定量である．
            \item $X^\top a=c$．
        \end{enumerate}
        \item 次の２条件は同値．
        \begin{enumerate}
            \item 線型関数$c^\top\beta$の線形不偏推定量が存在する．
            \item $c^\top\beta$は推定可能である．
        \end{enumerate}
    \end{enumerate}
\end{theorem}

\begin{theorem}
    線型関数$c^\top\beta$は推定可能であるとする．
    このとき，
    \begin{enumerate}
        \item $c^\top$と$c^\top\wh{\beta}$は，ある$b\in\R^p$を用いて$c=X^\top Xb,c^\top\wh{\beta}=b^\top X^\top Y$と表される．
        \item $c^\top\wh{\beta}$は正規方程式の解$\wh{\beta}$の取り方に依らず，$c^\top\beta$の線形不偏推定量である．
    \end{enumerate}
\end{theorem}

\begin{theorem}[BLUE: best linear unbiased estimator]
    $\beta$の線型関数$c^\top\beta$は推定可能であるとする．
    このとき，正規方程式の任意の解$\wh{\beta}$に対して，$c^\top\wh{\beta}$は$c^\top\wh{\beta}$の\textbf{最良線型不偏推定量}である：
    \[\forall_{(\beta,\sigma^2)\in\Pi}\quad\Var[c^\top\wh{\beta}]=\min\Brace{\Var[a^\top Y]\ge0\mid a^\top Y\text{は}c^\top\beta\text{の線形不偏推定量}}.\]
    ただし，$\Var$とは，真値が$(\beta,\sigma^2)$であるときの分散を表す．
\end{theorem}

\begin{theorem}[BLUEの特徴付け]
    次の３条件は同値である．
    \begin{enumerate}
        \item 線型統計量$a^\top Y$は線型関数$c^\top\beta$の最良線型不偏推定量である．
        \item $c^\top\beta$は推定可能で，任意の真値$(\beta,\sigma^2)\in\Pi$に対して，$c^\top\wh{\beta}=a^\top Y\as$
        \item $X^\top a=c$かつ$a\in L[X]$．
    \end{enumerate}
\end{theorem}

\subsection{分散共分散行列}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    
\end{tcolorbox}

\subsection{分散の推定}

\section{仮説検定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    ある部分集合$\Theta_0\subset\Theta:=\R^p$の部分集合について，
    問題
    \[H_0:\; H^\top\beta=d\quad\vs\quad H_1:\; H^\top\beta\ne d\]
    を検定することを考える．
    Gauss-Markovモデルに対して，さらに誤差項の分布の正規性$\ep\sim N_n(0,\sigma^2I_n)$を仮定する．
    この仮定は，誤差分布を楕円形分布に拡張することや，ロバスト推測論によって乗り越えられる．
\end{tcolorbox}

\subsection{理論}

\begin{notation}
    $R_0^2:=\min_{\beta\in\R^p}\abs{Y-X\beta}^2$とし，$r:=\rank X<n$を仮定する．
\end{notation}

\begin{theorem}
    任意の真値$(\beta,\sigma^2)\in\Pi$について，$\frac{R^2_0}{\sigma^2}\sim\chi^2(n-r)$．
\end{theorem}

\begin{theorem}
    次を仮定する．
    \begin{enumerate}[({A}1)]
        \item $H\in M_{p,q}(\R)\;(q<p)$は$L[H]\subset L[X^\top]$かつ$\rank H=q$を満たすとする．
        \item $d\in\R^q$について，$\Theta_0=\Brace{\beta\in\R^p\mid H^\top\beta=d}$と表せるとする．
    \end{enumerate}
    このとき，$R^2_1:=\Brace{\beta\in\Theta_0}\abs{Y-X\beta}^2$について，
    \begin{enumerate}
        \item 任意の真値$(\beta,\sigma^2)\in\Pi$について，$R^2_0$と$R^2_1-R^2_0$は独立であり，$\frac{R^2_0}{\sigma^2}\sim\chi^2(n-r)$，かつ，
        ある$\sigma\in\R_+$に対して，$\frac{R^2_1-R^2_0}{\sigma^2}\sim\chi^2(q,\delta)$．特に，
        \[\frac{(R^2_1-R^2_0)/q}{R^2_0/(n-r)}\sim F(q,n-r,\delta).\]
        \item 真値$\beta$が$\beta\in\Theta_0$を満たすならば，$\frac{R^2_1-R^2_0}{\sigma^2}\sim\chi^2(q)$であり，特に
        \[\frac{(R^2_1-R^2_0)/q}{R^2_0/(n-r)}\sim F(q,n-r).\]
    \end{enumerate}
\end{theorem}

\begin{proposition}
    
\end{proposition}

\subsection{応用}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    このように，$F$分布に基づく検定を$F$-検定という．
    このように，データのなすベクトルの2次形式による，データの変動の分解を基に検定を構成する手法を\textbf{分散分析}(ANOVA: ANalysis Of VAriance)と呼ぶ．
\end{tcolorbox}

\section{平均の検定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    パラメトリックモデル$N(\mu,\sigma^2)$を仮定し，ここからの無作為標本$Y_j\;(j\in[n])$から平均$\mu$に関する仮説$\mu=\mu_0$の検定は両側$t$-検定となる．
\end{tcolorbox}

\section{重回帰分析}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    重回帰分析においては，\textbf{切片項$\beta_0$を含むGauss-Markovモデル}が用いられる．
\end{tcolorbox}

\begin{discussion}
    \[y=\beta_0+\beta_1x_1+\cdots+\beta_px_p+\ep\]
    なる関係で説明したい．$n$個の観測$Y_i\;(i\in[n])$を説明したい場合，ベクトル表記で$Y=X\beta+\ep$とまとめられるが，$i\in[n]$が個体番号を表すとき，誤差項の独立性は成り立つであろう．
    こうして，Gauss-Markovモデルにたどり着く．
\end{discussion}

\section{一元配置}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=one-way ANOVA]
    
\end{tcolorbox}

\section{二元配置}

\chapter{統計的決定理論}

\begin{quotation}
    （意志）決定理論の立場から推定と検定の小標本理論を考察する．
    統計的な推定を行うことは，特定の損失関数を採用して，これについて統計的決定問題を解くことに等しい．

    統計的決定問題が設定されたとき，決定問題の下で起こる確率現象を解明するのが数理統計学の課題であって，特定の決定関数を無条件に是とするものではない．
\end{quotation}

\section{統計推論の枠組み}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    決定すべきは関数＝射である．
    その文脈は8-組で表現できる．
\end{tcolorbox}

\subsection{Waldの定義}

\begin{definition}[statistical decision problem, sample space, decision / action space, loss function, nonrandomized, randomized decision function (Abraham Wald)]
    次の8-組$(\X,\A,\P,\Theta,\D,\B,W,\Delta)$を\textbf{統計的決定問題}という．
    \begin{enumerate}
        \item $(\X,\A)$は可測空間で，\textbf{標本空間}という．
        \item $\Theta$はパラメータの空間で，$\P:=(P_\theta)_{\theta\in\Theta}$は$(\X,\A)$の確率分布の族．
        \item $(\D,\B)$は可測空間で，\textbf{決定空間}または\textbf{行動空間}という．
        \item 第二変数について可測な関数$W:\Theta\times\D\to\R_+$を\textbf{損失関数}という．
        \item $\Delta$は決定関数の族とする．
        \begin{enumerate}[(a)]
            \item 可測写像$\delta:\X\to\D$を\textbf{非確率的決定関数}という．
            \item 第一変数についてのcurrying $\X\to\Map(\B,[0,1])$は$(\D,\B)$上の確率測度を与え，第二変数についてのcurrying $\B\to\Map(\X,[0,1])$は$\X$上の$\A$-可測関数となる関数$\delta:\B\times\X\to[0,1]$を\textbf{確率的決定関数}という．
        \end{enumerate}
    \end{enumerate}
\end{definition}
\begin{remark}
    非確率決定関数$\delta:\X\to\D$が与えられたとき，任意の$B\in\B$に対して$\wt{\delta}(B|x):=1_{\Brace{\delta(x)\in B}}$で$\wt{\delta}:\B\times\X\to[0,1]$が定まるから，
    確率的決定関数の方がより一般的な設定となる．
\end{remark}

\begin{example}[平均値の推定量の決定]
    $(\X,\A):=(\R^n,\B_n)$，$\P_1:=\Brace{P=P_*^n\in\M(\R^n)\mid P_*は\R 上の確率分布で\int_\R x^2P_*(dx)<\infty}$，$\Theta:=\P_1$とする．
    決定空間は平均値の全体としたいから$(\D,\B):=(\R,\B_1)$．
    推定量$\Delta:=\Brace{T_1,T_2,T_3}$はそれぞれ$T_1:=X_1,T_2:=\sum^n_{j=1}\frac{X_j}{n},T_3:=X_n$という非確率的決定関数の族で，損失関数は$W(P,a):=(a-\mu_1(P))^2$とする．

    すると，損失関数$W$から定まる危険関数は
    \begin{align*}
        R(P,T_i)&=\int W(P,T_i(x_1,\cdots,x_n))P(dx_1,\cdots,dx_n)\quad(P\in\P_1)\\
        &=\Var_P[T_i]
    \end{align*}
    となる．こうして，$R(P,T_1)=R(P,T_3)=\Var_P[X_1],R(P,T_2)=\frac{\Var_P[X_1]}{n}$となり，$T_2$が危険関数$R$を最小にするとわかる．
\end{example}

\subsection{危険関数}

\begin{definition}[risk function, admissible]
    \textbf{危険関数}$R:\Theta\times\Delta\to\R$を次のように定める．
    \begin{enumerate}
        \item 確率的な決定関数$\delta:\B\times\X\to[0,1]$については，$R(\theta,\delta):=\int_\X\int_\D W(\theta,a)\delta(da|x)P_\theta(dx)$と定める．
        \item 非確率的な決定関数$\delta:\X\to\D$については，$R(\theta,\delta):=\int_\X W(\theta,\delta(x))P_\theta(dx)$と定める．
    \end{enumerate}
    ２つの決定関数$\delta_1,\delta_2\in\Delta$について，
    \begin{enumerate}
        \item $\forall_{\theta\in\Theta}\;R(\theta,\delta_1)\le R(\theta,\delta_2)$が成り立つとき，$\delta_1$と$\delta_2$は\textbf{同程度に良い}決定関数であるという．
        \item 同程度に良い決定関数がさらに$\exists_{\theta_0\in\Theta}\;R(\theta_0,\delta_1)<R(\theta_0,\delta_2)$を満たすとき，$\delta_1$は$\delta_2$より\textbf{一様に良い}決定関数であるという．
        \item 「一様に良い」という関係は順序を定める．$\Delta$に最大元が存在せず，極大元$\delta_*$のみが存在するとき，\textbf{許容的}であるという．
    \end{enumerate}
\end{definition}

\section{統計量の十分性と完備性}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    統計量とは可測関数$T:(\X,\A)\to(\cT,\B)$をいう．
    考えている確率分布族$\{P_\theta\}_{\theta\in\Theta}\subset P(\X,\A)$に応じて，$T$がその構造を保つかどうかが変わる．
    が，一般的に好ましい性質が定義できる．
\end{tcolorbox}

\subsection{十分統計量}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $\theta$の十分統計量$T$とは，$\theta$の推定に関する限り，データから得られる情報を漏らさず含んでいることを表している．
    すなわち，$T$で条件付けると，その条件づき確率がもはや$\theta$に依らなくなるような統計量$T:\X\to\T$をいう．
\end{tcolorbox}

\begin{notation}
    可測関数$T:(\X,\A)\to(\cT,\B)$を考える．確率分布族$\{P_\theta\}_{\theta\in\Theta}\subset P(\X,\A)$についての条件付き確率$P_\theta[A|T=t]$が，母数$\theta\in\Theta$に依存しないとき，$T$を得てしまえば，これはデータの持つ情報のすべてを持っていると言える．
\end{notation}

\begin{definition}[sufficient statistic]
    $T:\X\to\cT$が$\{P_\theta\}$に関して\textbf{十分}であるとは，次が成り立つことをいう：
    \begin{quote}
        任意の$A\in\A$に対して，$\B$-可測関数$q(A|-):\B\to\R$が存在して，
        \[\forall_{B\in\B}\;\forall_{\theta\in\Theta}\quad\int_Bq(A|t)P_\theta^\top(dt)=P_\theta[A\cap T^{-1}(B)].\]
    \end{quote}
\end{definition}

\begin{example}[order statistic]
    確率分布族
    \[\P:=\Brace{P\in P(\R^n)\mid\forall_{\sigma\in S_n}\;P^\sigma=P}\]
    を考えているとき，$x=(x_1,\cdots,x_n)$の各成分を小さい順に並べて出来る列$t(x)=(x_{(1)},\cdots,x_{(n)})$へ写す可測関数$T:\R^n\to\R^n;x\mapsto t(x)$を\textbf{順序統計量}という．
\end{example}

\subsection{Fisherの因子分解定理}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    十分統計量を構成する十分条件を探す．
\end{tcolorbox}

\subsection{Rao-Blackwellの定理}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    決定関数＝「推定量の候補」としては，十分統計量の関数のみを考えれば十分であることを定立する．
    あるいは，適当な推定量$\delta$に対して，$\delta_0:=E[\delta|T]$とすれば，これは$\delta$よりも同程度に良い決定関数となる．
\end{tcolorbox}

\begin{theorem}[Rao-Blackwell]
    $T$をモデル$\P=(P_\theta)_{\theta\in\Theta}$に対する十分統計量であるとする．
    このとき，非確率的決定関数$\delta$は$\forall_{\theta\in\Theta}\;E_\theta[\abs{\delta}]<\infty$を満たすとする．
    これに対して，$\delta_0:=E[\delta|T]$と定めると，これは同程度に良い決定関数である：
    \[\forall_{\theta\in\Theta}\quad R(\theta,\delta)\le R(\theta,\delta_0)\]
\end{theorem}


\subsection{完備性}

\begin{definition}[complete, boundedly complete]\mbox{}
    \begin{enumerate}
        \item 分布族$\P\subset P(\X,\A)$が\textbf{[有界的]完備}であるとは，
        任意の[有界]可測関数$f:\X\to\R$に関して$\forall_{\theta\in\Theta}\;E_\theta[f]=0\Rightarrow f=0\;\P\text{-}\as$が成り立つことという．
        ただし，$\P\text{-}\as$とは，任意の$P\in\P$について$P$-$\as$であることをいう．
        \item 可測関数$T:\X\to\cT$が\textbf{[有界的]完備}であるとは，
        $(\cT,\B)$上に引き起こされる分布族$(P^T_\theta)_{\theta\in\Theta}$が[有界的]完備であることをいう．
    \end{enumerate}
\end{definition}

\section{指数型分布族}

\section{統計的推定}

\subsection{不偏推定}

\begin{definition}[unbiased estimator, $U$-estimable]
    確率分布族$(P_\theta)$に対して，
    \begin{enumerate}
        \item $\forall_{\theta\in\Theta}\;E_\theta[\delta]=\theta$を満たす推定量$\delta$を\textbf{不偏推定量}という．
        \item パラメータの関数$g:\Theta\to\Theta$に対して不偏推定量が存在するとき，$g$を\textbf{$U$-推定可能}という．
    \end{enumerate}
\end{definition}

\subsection{正則な統計的実験}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    \begin{itemize}
        \item 確率空間$(\X,\A,P_\theta)$のエントロピー＝平均情報量とは，対数尤度の平均$-E[\log L(\theta|x)|\theta]$であり，系の乱雑さを表す．一様分布で，どの事象も起こり得て予測が難しいとき，値が大きくなる．
        \item 対数尤度の$\theta$に関する微分$V(x;\theta)$の平均は$E[V(x;\theta)|\theta]=0$を満たすが（$\theta$がいくら変わろうと確率分布である限り総重量は$1$なので），この分散に興味がある．
        もし至る所で変わるなら，分散は極めて大きくなるはずである．
        Fisher情報量とはこれであり，分布$P_\theta$自身が母数$\theta$に関してもつ情報の量を表す．
    \end{itemize}
\end{tcolorbox}

\begin{definition}[Fisher's finite information]
    $L_2(\X,\nu)$について，
    \[I(\theta):=4\int_\X\paren{\pp{}{\theta}p^{1/2}(x;\theta)}^2d\nu=4\Norm{\pp{}{\theta}p^{1/2}}_\nu^2\]
    をFisher情報，
    \[I(\theta)=4\int_\X\pp{}{\theta}p^{1/2}(x;\theta)\paren{\pp{}{\theta}p^{1/2}(x;\theta)}^\top d\nu\]
    をFisher情報行列という．
    ここで，関数$p^{1/2}(-;u)$が$u=\theta$で微分可能で導関数が$L_2(\nu)$級であることは必ずしも仮定しない．
    \[\exists_{\psi\in L^2(\X,\Meas(\Theta,\R^K))}\;\int_\X\Abs{g(x;\theta+h)-g(x;\theta)-(\psi(x;\theta),h)}^2d\nu=o(\abs{h}^2)\quad(h\to0)\]
    が満たされるとき，統計的実験$E$は$\theta\in\Theta$で\textbf{有限なFisher情報を持つ}という．
    $\pp{p}{\theta}(x;\theta)$はこの$L^2$-微分の意味で表している．
\end{definition}
\begin{remarks}
    対数尤度関数$\log L(\theta|x)$の$\theta$による微分をスコア関数という．
    \[V(x;\theta)=\pp{}{\theta}\log L(\theta|x)=\frac{1}{L}\pp{L}{\theta}.\]
    $\theta$を増やしたら尤度が上がる場合，スコア関数は正で，さらに尤度が低ければ低いほど$1/L$により拡大される．
    これはたしかに情報量の考え方である．
    実は$E[V(x;\theta)|\theta]=0$が成り立つ．
    そこで，スコア関数の2次のモーメント$I(\theta):=E[V(x;\theta)^2|\theta]$をFisher情報量というと，これはスコア関数の分散である：$I(\theta)=\Var[V(x;\theta)]$．
    なお，2つの定義の等価性
    \begin{align*}
        \paren{\pp{}{\theta}p^{1/2}(x;\theta)}^{2}&=\paren{\frac{1}{2}p^{-1/2}\pp{}{\theta}p(x;\theta)}^2\\
        &=\frac{1}{4}p^{-1}(x;\theta)\paren{\pp{p(x;\theta)}{\theta}}^2
    \end{align*}
    による．
\end{remarks}

\begin{definition}[regular statistical experiment]
    組$E:=(\X,\A,\{P_\theta\}_{\theta\in\Theta},\Theta\osub\R^K)$が，$P_\theta$が$\nu$-絶対連続であるとき，統計的実験という．
    $E$が$\Theta$上で\textbf{正則}であるとは，次の3条件を満たすことをいう：
    \begin{enumerate}
        \item Radon-Nikodym微分の族$p(x,-):\Theta\to[0,1]$は$\nu\dae x$に関して連続である．
        \item 任意の点$\theta\in\Theta$において有限なFisher情報が存在する．
        \item 関数$\psi(-,\theta)\in L^2(\nu)$は連続．
    \end{enumerate}
\end{definition}

\subsection{最適性・有効性}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    一般に不偏推定量は複数ある．そこで，設定した損失関数に応じて，「最適」な推定量を選ぶ必要がある．
    一般には二乗損失を考えるため，分散を最適性の指標とし，分散が最も小さい推定量を\textbf{有効(efficient)}であるという．
\end{tcolorbox}

\begin{notation}
    統計的決定理論の枠組みで不偏推定問題を考えると，次の通りになる．
    \begin{enumerate}
        \item 標本空間$(\X,\A)$上の確率分布族$(P_\theta)_{\theta\in\Theta}$を考える．
        \item 決定空間は，$\D\subset\R^p$を凸ボレル集合，$\B$はその上のボレル集合族とする．
        \item 損失関数$W$は凸とする．
        \item $\Delta:=\Delta_g$は$p$次元関数$g(\theta)$の不偏推定量を非確率論的決定関数$\delta:\X\to\D$と観て，その全体
    \end{enumerate}
    $\Delta_g\ne\emptyset$，すなわち，$g:\Theta\to\Theta^p$は$U$-推定可能とする．
\end{notation}

\begin{theorem}[Lehmann-Scheffe]
    $T$を分布族$\P$に対する完備十分統計量とする．
    任意の不偏推定量$\delta\in\Delta_g$に対して，$\delta_0:=E[\delta|T]$とすると，$\delta_0$は$g(\theta)$の最良の不偏推定量である：$\forall_{g'\in\Delta_g}\;\forall_{\theta\in\Theta}\;R(\theta,\delta_0)\le R(\theta,\delta')$．
\end{theorem}
\begin{remarks}
    一般には，$p=1$次元とし，損失関数は二乗損失$W(\theta,a)=[a-g(\theta)]^2$を考えることが多い．このとき，危険関数は$R(\theta,\delta)=\Var_\theta[\delta]$となる．
    したがって，分散を次の「最適性」の指標とする．
\end{remarks}

\begin{definition}[UMVUE: uniformly minumum variance unbiased estimator]
    $g(\theta)$の任意の不偏推定量$\delta'\in\Delta_g$に対して，$\forall_{\theta\in\Theta}\;\Var_\theta[\delta]\le\Var_\theta[\delta']$を満たす不偏推定量$\delta$を\textbf{一様最小分散不偏推定量}という．
\end{definition}

\subsection{Cramer-Raoの不等式}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    Fisher情報量は，スコア関数の２次のモーメントと定義される．また，KL情報量の二次の項としても登場する．
    これを用いて算出される，不偏推定量の「最適性」の情報理論的限界を，不偏推定量の分散行列の下界を与えることで示す定理を，Cramer-Raoの不等式という．
    したがって，Cramer-Raoの不等式の下界を達成する不偏推定量が存在するならば，それはUMV不偏推定量である．
\end{tcolorbox}

\begin{definition}
    パラメータ$\theta=(\theta_i)\in\R^q$に依存する確率密度関数$p_\theta(x)$に対して，
    \[I_{ij}(\theta):=\int_\X\pp{}{\theta_i}\log p_\theta(x)\cdot\pp{}{\theta_j}\log p_\theta(x)\cdot p_\theta(x)\mu(dx)\]
    として定まる行列$I(\theta)$を\textbf{Fisher情報行列}という．
    特に$q=1$の場合，Fisher情報量という．
\end{definition}

\begin{notation}
    パラメータの関数$g:\Theta\to\Theta^p$について，$J(\theta):=\pp{g}{\theta}$を$p\times q$-行列とする．
\end{notation}

\begin{theorem}[Cramer-Rao]
    $U\osub\R^q$で添字付けられたパラメトリックモデル$(P_\theta)_{\theta\in U}$と，偏微分可能な関数$g:\R^q\to\R^p$と，その$\theta\in U$における不偏推定量$\delta\in\L^2(\X^n;\R^p)$とについて，次を仮定する：
    \begin{enumerate}[({A}1)]\setcounter{enumi}{-1}
        \item 分布族$(P_\theta)_{\theta\in U}$はある$\sigma$-有限な参照測度$\mu\in P(\X)$に関して絶対連続とし，そのRadon-Nikodym微分を$p_\theta:\X\to[0,1]$で表す．
        \item $p_-(x):U\to[0,1]$は$P_\theta\dae\; x$に関して偏微分可能である．
        \item $p_\theta(x)>0$を満たす点$(x,\theta)$の上で，
        スコア関数の第$i$成分$\psi_i:\X\times U\nrightarrow\R$を
        $\psi_i(x,\theta):=\pp{\log p_\theta(x)}{\theta_i}=\frac{1}{p_\theta}\pp{p_\theta}{\theta_i}$と定めると\footnote{$\Brace{x\in\X\mid p_\theta(x)=0}$上で$\psi_i(-,\theta)$は定義されていないことに注意．}，二次の絶対積率が有限$\psi_i(-,\theta)\in\L^2(P_\theta)$：
        \[E_{P_\theta}[\abs{\psi_i}^2]=\int_\X\abs{\psi_i(x,\theta)}^2p_\theta(x)\mu(dx)<\infty\qquad(\forall_{i\in[q]}).\]
        \item 各$\theta\in U$におけるFisher情報行列$I=(I_{ij})$を，第$(i,j)$-成分を
        \[I_{ij}(\theta):=\Cov_{P_\theta}[\psi_i,\psi_j]=\int_\X\pp{\log p_\theta(x)}{\theta_i}\pp{\log p_\theta(x)}{\theta_j}\cdot p_\theta(x)\mu(dx)\qquad(\forall_{i,j\in[q]}).\]
        とすることによって定めると，正定値な対称行列となる．
        \item 不偏推定量$\delta$は$\psi:=(\psi_1,\cdots,\psi_q)^\top$が定める推定方程式の解であり：
        $E_{P_\theta}[\psi]=\int_\X\psi(x,\theta)p_\theta(x)\mu(dx)=0\in\R^q$，かつ，次の微分と積分の可換性が成り立つ：
        \[\int_\X\delta(x)\psi_i(x,\theta)p_\theta(x)\mu(dx)\paren{=\int_\X\delta(x)\pp{p_\theta}{\theta_i}\mu(dx)}=\pp{}{\theta_i}\int_\X\delta(x)p_\theta(x)\mu(dx)\qquad(i\in[q]).\]
    \end{enumerate}
    このとき，関数$g:\R^q\to\R^p$のJacobi行列を$J(\theta):=\pp{g}{\theta}\in M_{p,q}(\R)$とすると，次の行列不等式が成り立つ：
    \[\Var_\theta[\delta]\ge J(\theta)I(\theta)^{-1}J(\theta)^\top.\]
\end{theorem}
\begin{proof}
    任意に$\theta\in U$を取り，$I:=I(\theta),J:=J(\theta)$と略記する．
    \begin{description}
        \item[共分散への翻訳] 仮定(A4)より，
        \[J=\pp{g(\theta)}{\theta}=\pp{}{\theta}E_\theta[\delta]=E_\theta[\delta\psi^\top].\]
        これと$E_\theta[\psi]=0$より，
        \[\Cov_\theta[\psi,\delta]=E_\theta[\psi\delta^\top]=J^\top.\]
        $I=E_\theta[\psi\psi^\top]=\Var_\theta[\psi]$．
        \item[証明] すると，Cauchy-Schwartzの不等式同様，$\Var_\theta[\delta-JI^{-1}\psi]\ge O$であることから，$\Cov$の双線型性のみから従う．
        また，対称行列$S$に対して，$S^{-1}=(S^{-1})^\top=(S^\top)^{-1}$であることとFisher情報行列が対称であることに注意すると，
        \begin{align*}
            O&\le\Var_\theta[\delta-JI^{-1}\psi]=\Cov[\delta-JI^{-1}\psi,\delta-JI^{-1}\psi]\\
            &=\Var_\theta[\delta]-JI^{-1}\Cov_\theta[\psi,\delta]-\Cov_\theta[\delta,\psi]I^{-1} J^\top+JI^{-1}\Var[\psi]I^{-1}\psi^\top\\
            &=\Var_\theta[\delta]-JI^{-1}J^\top.
        \end{align*}
    \end{description}
\end{proof}
\begin{remarks}
    本質的にはCauchy-Schwartzの不等式である．
    $\R^q$の内積を$(-|-)$，$\L^2(P_\theta)$の内積を$\brac{-|-}$で表すと，
    $\norm{\brac{\psi|\delta}}^2\le(\norm{\psi}^2|\norm{\delta}^2)$．
\end{remarks}

\subsection{ベイズ推定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    ベイズ決定関数とミニマックス決定関数の関係を考える．
\end{tcolorbox}

\begin{notation}
    次の統計的決定問題を考える．
    \begin{enumerate}
        \item 標本空間$(\X,\A)$上の確率分布族$(P_\theta)_{\theta\in\Theta}$について，$\Theta$が可測であるとする．
        \item 損失関数$W:\Theta\times\D\to\R_+$は可測，危険関数$R(-,\delta):\Theta\to\R$を可測とする．
    \end{enumerate}
\end{notation}

\begin{definition}[prior distribution, Bayes risk, Bayes estimator, minimax decision function]\mbox{}
    \begin{enumerate}
        \item 先験的な確率分布$\pi\in P(\Theta)$を\textbf{事前分布}という．
        \item $R(\pi,\delta):=\int_\Theta R(\theta,\delta)\pi(d\theta)$を，$\pi$に関する$\delta$の\textbf{ベイズリスク}という．
        \item ベイズリスクを最小にする決定関数$\delta$を\textbf{ベイズ決定関数}という．これに対応するものが推定量であるとき，\textbf{ベイズ推定量}という．
        \item 一方で，リスク関数の上限$\sup_{\theta\in\Theta}R(\theta,\delta)$を最小にする決定関数を\textbf{ミニマックス決定関数}という．
    \end{enumerate}
\end{definition}

\subsection{非許容性}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $p$変量正規分布族$(N_p(\theta,\sigma^2I_p))_{\theta\in\R^p}$の位置母数の推定量$\o{x}=n^{-1}\sum^n_{j=1}x_j$はUMV不偏推定量であるが，$p\ge3$のとき非許容である．
\end{tcolorbox}

\section{統計的仮説検定}

\subsection{仮説検定の枠組み}

\begin{definition}[hypothesis testing, null hypothesis, alternative hypothesis, significance level, critical region]
    確率空間$(\X,P)$に対して，事前に与えられた設定$\alpha\in(0,1),H_0,H_1$から定められる部分集合$\X_1\subset\X$を検定という．
    \begin{enumerate}
        \item 仮説検定において，仮説とは，母数または確率分布に関する（メタ）条件をいう．
        \item 疑いの目が向けられており，検定される仮説$H_0$を\textbf{帰無仮説}といい，それに対立する仮説$H_1$を\textbf{対立仮説}という．
        \item 文脈としては，$H_0$を棄却し$H_1$の成立を示すことで社会的な意味を見出そうという文脈に当てはまるように$H_0,H_1$を選ぶ．
        \item 仮設$H_0$の下で計算した確率が「小さい」とはどういう意味かを形式化した指標を\textbf{有意水準}という．$\alpha=0.05,0.01$などが用いられる．
        \item $\X_1\subset\X$を$\alpha$と$H_0,H_1$の意義から事前に定めて\textbf{棄却域}といい，観測値$x$が$x\in\X_1$のとき棄却し，$x\notin\X_1$のとき採択する．
        $\X_1$は，$H_0$の下では非常に稀だが，$H_1$の下ではそうでもないような観測値の集合である．
    \end{enumerate}
\end{definition}

\begin{example}\label{exp-hypothesis-testing}
    $\alpha=0.05$とする．20回中事象$E$が13回起こったとする．帰無仮説$H_0$を$P(E)=1/2$とし，対立仮説$H_1$を$P(E)>1/2$とする．
    すると，棄却域は$x_0\in\N$を用いて$\X_1=\Brace{x\in[20]\mid x\ge x_0}$と定めるのが良いと考えられる．
    いま，
    \[\sum^{20}_{k=14}\begin{pmatrix}20\\k\end{pmatrix}\paren{\frac{1}{2}}^{20}\approx 0.0577\]
    より，$13<x_0$であって，観測値$13$は$\X_1$には入らないから，棄却できない．
    「確率$0.05$程度の事象が偶々起こった」と解釈する方が選択される．
\end{example}

\begin{definition}[type I error, type II error]
    $H_0$が正しいのに$H_0$を棄却してしまう誤りを\textbf{第一種の誤り}または\textbf{偽陽性}といい，$H_1$が正しいのに$H_0$を採択してしまう誤りを\textbf{第二種の誤り}または\textbf{偽陰性}という．
\end{definition}

\subsection{ランダム化検定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    検定の構成法の例をあげる．
    検定＝棄却域の良さは，第一種の誤りの確率を有意水準$\alpha$で制限した後に，その範囲内で第二種の誤りの確率を最小化することで得られる．
\end{tcolorbox}

\begin{example}
    例\ref{exp-hypothesis-testing}について，$x_0=15$とすると，
    \[\sum^{20}_{k=15}\begin{pmatrix}20\\k\end{pmatrix}\paren{\frac{1}{2}}^{20}\approx 0.0207\]
    より，棄却域のサイズは5\%を大きく切って2\%程度となってしまう．
    そこで，$14<x_0<15$にあたる棄却域を構成したいが，$x_0\in\Z$の値は変えられない．
    そこで，\textbf{棄却域をランダム化}する．
    \[\begin{pmatrix}20\\k\end{pmatrix}\paren{\frac{1}{2}}^{20}\varphi+\sum^{20}_{k=15}\begin{pmatrix}20\\k\end{pmatrix}\paren{\frac{1}{2}}^{20}=\alpha=0.05\]
    を満たす$\varphi\in(0,1)$を用いて，観測値$x=14$に対しては確率$\varphi$で$H_0$を棄却し，確率$1-\varphi$で$H_0$を採択することとする．
\end{example}

\subsection{形式化}

\begin{definition}[test / critical (function), size, level-$\alpha$-test, power function, uniformly most powerful (UMP) test]
    確率分布族$\P=(P_\theta)_{\theta\in\Theta}\subset\M(\X,A)$を考える．
    \begin{enumerate}
        \item 棄却域$\X_1\subset\X$の定めるパラメータ空間の分割$\Theta=\Theta_0+\Theta_1$を考え，帰無仮説を$H_0:\theta\in\Theta_0$で表し，対立仮説を$H_1:\theta\in\Theta_1$と表す．
        \item $\abs{\Theta_i}=1$の場合を単純仮説といい，そうでない場合を複合仮設という．\footnote{$\Theta$が１次元で，$H_0:\theta=\theta_0,H_1:\theta\ne\theta_0$の形のときを\textbf{両側検定}，$H_0:\theta\le\theta_0,H_1:\theta>\theta_0$の形のときを\textbf{片側検定}という．}
        \item 可測関数$\varphi:\X\to[0,1]$を\textbf{検定（関数）}という．$\exists_{A\in\A}\;\varphi=1_A$であるとき検定$\varphi$は非確率的である．\footnote{単純に棄却域とそうでない域に分割するため．そうでない場合は，ランダム化検定の方法論を取り入れたことになる．}
        \item $\sup_{\theta\in\Theta_0}E_\theta[\varphi]$を$\varphi$の\textbf{大きさ}という．大きさが$\alpha$以下の検定を\textbf{水準$\alpha$検定}という．水準$\alpha$検定の全体を$\Phi_\alpha\subset\Meas(\X,[0,1])$で表す．
        \item $\beta_\varphi:\Theta_1\to[0,1];\theta\mapsto E_\theta[\varphi]$を\textbf{検定出力関数}という．$1-\beta_\varphi(\theta)$は第二種の誤りの確率を表す．
        \item $\forall_{\varphi\in\Phi_\alpha}\;\forall_{\theta\in\Theta_1}\;\beta_{\varphi_0}(\theta)\ge\beta_\varphi(\theta)$を満たす検定$\varphi_0\in\Phi_\alpha$を\textbf{一様最強力検定}という．
    \end{enumerate}
\end{definition}
\begin{remarks}[統計的決定理論の枠組みでの解釈]
    決定空間は，採択する仮説の添字からなる空間$\D:=2=\{0，1\}$である($0$が受容，$1$が棄却)．
    損失関数は
    \[W(\theta,a)=\begin{cases}
        1_{\{1\}}(a),&\theta\in\Theta_0,\\
        1_{\{0\}}(a),&\theta\in\Theta_1.
    \end{cases}\]
    で，検定関数$\varphi$は確率的決定関数
    \[\delta_\varphi(dz|x)=\varphi(x)\ep_1(dz)+(1-\varphi(x))\ep_0(dz)\]
    に対応する．ただし，$\ep_a$はデルタ測度とした．したがって危険関数は
    \begin{align*}
        R(\theta,\delta_\varphi)&=\int_\X\int_\D W(\theta,z)\delta_\varphi(dz|x)\ep_0(dz)\\
        &=\begin{cases}
            E_\theta[\varphi],&\theta\in\Theta_0,\\
            1-E_\theta[\varphi],&\theta\in\Theta_1.
        \end{cases}
    \end{align*}
    となり，$\Phi_\alpha=\Brace{\varphi\in\Meas(\X,[0,1])\mid\sup_{\theta\in\Theta_0}R(\theta,\delta_\varphi)\le\alpha}$と表せる．
    決定関数の全体は$\Delta\subset\Phi_\alpha$となる．

    $\Delta$の中に入る一番自然な順序は一様順序で，これによる最大元が存在するときこれをUMPと呼ぶ．
\end{remarks}

\begin{definition}[nuisance parameter, goodness of fit test]\mbox{}
    \begin{enumerate}
        \item $\Theta$が２次元以上で，特定のパラメータにしか興味がないとき，分割$\Theta_1+\Theta_2$は商空間となる．この時の興味のない母数を\textbf{局外母数}または\textbf{撹乱母数}という．
        \item 帰無仮説を「確率変数が正規分布に従う」という命題にする場合，特に\textbf{分布の適合度検定}と呼ぶ．適合度検定や「確率分布が独立である」などの数学的な仮定が現実のデータと矛盾がないかチェックするための検定のクラスを\textbf{統計的モデルの診断}という．
    \end{enumerate}
\end{definition}

\subsection{Neyman-Pearsonの補題}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    サイズ$\al$の検定の中で，検出力$\beta$が最も大きい検定の存在を保証し，尤度比検定によって達成されることを主張する補題．
    「$\al$を決めておき、その中で検出力が最も大きい検定法を選択する」という方針をネイマン・ピアソンの基準という。
\end{tcolorbox}

\subsection{単調尤度比と複合仮説の検定}

\subsection{一般化されたNeyman-Pearsonの補題}

\subsection{不偏検定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    一様最強力検定は一般には存在しない．
    が，Neyman-Pearsonの理論はある指数分布族モデルに対して一様最強力検定の構成を可能にする．
\end{tcolorbox}

\subsection{両側$t$-検定}

\subsection{不変検定}

\section{区間推定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    パラメータ$\theta\in\Theta$に対して，区間$S(x)$を考えることを，区間推定という．
\end{tcolorbox}

\begin{definition}[confidence coefficient, confidence region]
    写像$S:\X\to P(\Theta)$は，$\al\in(0,1)$を所与の定数として，次の２条件をみたすとする：
    \begin{enumerate}
        \item $\forall_{\theta\in\Theta}\;\Brace{x\in\X\mid \theta\in S(x)}\in\A$．
        \item $\forall_{\theta\in\Theta}\;P_\theta[\theta\in S(x)]\ge1-\al$
    \end{enumerate}
    このとき，$1-\al$を\textbf{信頼係数}，$S$を\textbf{信頼域}という．
\end{definition}


\chapter{大標本理論}

\begin{quotation}
    非線形モデル・非ガウスモデルに対しては，統計量の分布が複雑なため，標準的な統計的決定理論を適用するのは困難であるが，
    これに替わって漸近的方法に基づく大標本理論が有力な解析手段となる．
    ここで，正規近似に基づく１次の漸近理論を扱う．

    大標本理論の一番最初の応用先として汎関数の漸近分布を求めることがあるので，統計的検定問題を考える．
    漸近論が準備できたことにより，尤度比検定はより理解できるようになった．

    これらの暗黙の枠組みは漸近決定理論であり，「統計的実験列の極限」というアイデアである．
\end{quotation}

\section{一致推定量}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    Ronald Fisher 1912-22が開発．
    最尤推定量は一致性を満たすが，この現象はより広いクラスについて成り立つ．
\end{tcolorbox}

\subsection{最尤推定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    統計推測においては，Bayes法と最尤法とが双璧をなす．
\end{tcolorbox}

\begin{problem}
    可測空間$(\X,\A)$上の分布族$(P_\theta)_{\theta\in\Theta}$について，独立な観測値$x_1,\cdots,x_n$から$\theta$を推定する問題を考える．
\end{problem}

\begin{definition}[MLE: maximum likelihood estimator]
    $(\X,\A)$上の$\sigma$-有限測度$\nu$に関して$P_\theta$が絶対連続であるとし，微分を$p(x,\theta):=dP_\theta/d\nu(x)$と表す．
    \begin{enumerate}
        \item $L_n(\theta):=\prod^n_{j=1}p(x_j,\theta)$によって定まる関数$\X^n\times\Theta\to\R$を\textbf{尤度関数}という．これは，ある$\theta\in\Theta$について，これが定める分布$P_\theta$から観測値$x_1,\cdots,x_n$が得られる条件付き確率に当たる．
        \item 各$(x_1,\cdots,x_n)\in\X^n$に対して，尤度関数を最大にする$\wt{\theta_n}(x_1,\cdots,x_n)\in\Theta$を対応させる関数$\wt{\theta_n}:\X^n\to\Theta$を\textbf{最尤推定量}という．\footnote{ここでは推定量は可測という仮定は置かない．}
    \end{enumerate}
\end{definition}
\begin{remark}[$M$-推定量としての最尤推定量]
    単調増加関数との合成を考えて，対数尤度関数$l_n:=\log\circ L_n$の最大化を考えるにあたって，パラメータの空間$\Theta$が可微分構造を持つとき，
    ランダムな方程式$\partial_\theta l_n(\theta)=0$の解としての最尤推定量を特徴付けることが出来る．
    これを\textbf{尤度方程式}という．
    このとき出現した対数尤度の微分を（標準化されていない）\textbf{スコア関数}$S(x,\theta):=\pp{}{\theta}l(\theta,x)$という．
    \begin{align*}
        L_n(\theta):=\prod^n_{j=1}p(x_j,\theta)\text{の最大化}&\Leftrightarrow l_n(\theta):=\log L_n(\theta)=\sum^n_{j=1}\log p(x_j,\theta)\text{の最大化}\\
        &\Leftrightarrow S_n(x,\theta):=\pp{}{\theta}l_n(\theta)=\sum^n_{j=1}\pp{}{\theta}\log p(x_j,\theta)\text{の零点を探す}
    \end{align*}
\end{remark}

\subsection{一致性}

\begin{remarks}[最尤推定量の一致性]\label{remarks-consistency-of-MLE}
    大数尤度関数$l_n(\theta):=\log L_n(\theta)=\sum_{j=1}^n\log p(x_j,\theta)$は，独立同分布に従う確率変数の和である．よって，大数の法則により，真値$\theta_0\in\Theta$について，
    \[\forall_{\theta\in\Theta}\quad\frac{1}{n}[l_n(\theta)-l_n(\theta_0)]\to\int\log\paren{\frac{p(x,\theta)}{p(x,\theta_0)}}P_{\theta_0}(dx)\quad P_{\theta_0}\text{-}\as\]
    が成り立つ．
    $P:\Theta\to\{P_\theta\}$が単射であるという識別可能性条件を仮定すると，相対エントロピーの非負性より，真値$\theta_0$はこの右辺を最大にする唯一の点となる．
    そこで，左辺を最大にする推定量が$\wt{\theta_n}$なのであったから，$n\to\infty$のとき，$\wt{\theta}_n\to\theta_0$が予想される．
    あとは，任意の$\theta\in\Theta$に対する一様性を確認すれば良い．
\end{remarks}

\begin{lemma}
    可測空間$(\Om,\F)$上の$\sigma$-有限測度$\nu$に関して絶対連続な確率測度$P,Q$について，その微分を$p(\om):=dP/d\nu(\om),q(\om):=dQ/d\nu(\om)$とおく．
    このとき，
    \[\int_\Om\log\paren{\frac{p(\om)}{q(\om)}}P(d\om)\le 0\]
    で，等号成立条件は$P=Q$のとき．
\end{lemma}

\begin{definition}[consistency]
    パラメータの推定量$\theta_n:\X^n\to\Theta$について，
    \begin{enumerate}
        \item 任意の$\theta\in\Theta$について，これが真値であったとき，$\wt{\theta_n}\to\theta\as\;(n\to\infty)$が成り立つとき，この推定量は\textbf{強一致性}を持つという．
        \item 推定量$\wt{\theta_n}$が可測で，任意の$\theta\in\Theta$について，これが真値であったとき，$\wt{\theta_n}\xrightarrow{p}\theta\;(n\to\infty)$が成り立つとき，すなわち，$\wt{\theta}_n-\theta=o_p(1)$が成り立つとき，この推定量は\textbf{弱一致性}を持つという．
    \end{enumerate}
\end{definition}

\begin{tbox}{red}{}
    たったこれだけの議論で，相対エントロピーなる確率測度間の距離概念と，実数の乗法群と加法群の間の準同型$\R_{>0}\to\R$としての対数関数とのいずれもが出てくるとは思わなかった．
    「尤度」なる意味論と「相対エントロピー」という意味論とが繋がることが一番の驚き．
\end{tbox}

\begin{lemma}[確率収束の消息を統計の言葉で表現]\mbox{}
    \begin{enumerate}
        \item $U_n$が$\theta$にある$r>0$について$r$-次平均収束するならば，弱一致推定量である．
        \item 特に，$U_n$が$\theta$の不偏推定量かつ$\Var[U_n]\to0$が成り立つならば，$\theta$の弱一致推定量である．
    \end{enumerate}
\end{lemma}

\begin{example}[一致推定量]\mbox{}
    \begin{enumerate}
        \item $U_n(X):=\o{X}=\frac{X_1+\cdots+X_n}{n}$は母集団平均の強一致推定量である（大数の強法則による）．
        \item 全く同様のことは，一般の標本モーメント$\wh{m}_r:=\frac{1}{n}\sum^n_{j=1}X_j^r$が強一致推定量であるということが言える．
        \item 一般の標本中心化モーメントも強一致推定量であるが，そのままでは不偏推定量にはならない．
        \item セミパラメトリックな線型回帰モデル$Y_j=\al+\beta X_j+\ep_j\;(j\in[n])$のOLS(ordinary least squares estimator)はいくらかの条件の下一致推定量である．
        \item 自己回帰モデル(AR:Autoregressive Model)$Y_j=\al+\beta Y_{j-1}+\ep_j$の最小二乗推定量
        \[\wh{\al}=\o{Y}_{1,n}-\wh{\beta}\o{Y}_{0,n-1},\quad\wh{\beta}=\beta+\frac{\sum_{r\in[n]}\ep_r(Y_{r-1}-\o{Y}_{0,n-1})}{\sum_{r\in[n]}(Y_{r-1}-\o{Y}_{0,n-1})^2}\]
        は一致推定量である．ただし，$\o{Y}_{i,j}$を$Y_i$から$Y_j$までのデータを使った標本平均とした．
    \end{enumerate}
\end{example}

\section{一様対数の法則}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    真値$\theta_0\in\Theta$はわからないため，あらゆる$\theta\in\Theta$について一様な一致性を持ってほしい．
    最尤推定量はこの性質を持つ．
\end{tcolorbox}

\begin{notation}\mbox{}
    \begin{enumerate}
        \item $(\X,\A,P)$の可算無限直積$(\o{\X},\o{\A},\o{P})$上に，$\pi_n:=(\pr_1,\cdots,\pr_n):\o{\X}\to\X^n$で$\X^n$上の可測関数を引き戻して議論する．
        \item $U:\X\times\Theta\to(-\infty,\infty]$に対して，
        \[\o{U_n}(x,\theta):=\bP_nU(x,\theta)=\frac{1}{n}\sum^n_{j=1}U(x_j,\theta)\]
        と表す．ただし，$x=(x_1,\cdots,x_n)\in\X^n$とした．
    \end{enumerate}
\end{notation}

\begin{lemma}
    $(\Theta,d)$を距離空間，$\Theta_1\subset\Theta$をコンパクト集合，$U:\X\times\Theta\to[-\infty,\infty]$を関数とし，以下の条件を仮定する：
    \begin{enumerate}[({A}1)]
        \item $\forall_{x\in\X}\;\theta\mapsto U(x,\theta)$は下半連続．
        \item $\forall_{\theta\in\Theta}\;r$が十分に小さければ，$x\mapsto\inf_{\theta'\in\Theta,d(\theta',\theta)<r}U(x,\theta')$は可測．
        \item ある可積分関数$M:\X\to\R$が存在して，$\forall_{(x,\theta)\in\X\times\Theta}\;U(x,\theta)\ge M(x)$．
    \end{enumerate}
    このとき，$\o{U}(\theta):=\int_\X U(x,\theta)P(dx)\in(-\infty,\infty]$に対して，
    \begin{enumerate}
        \item 
        \[\liminf_{n\to\infty}\inf_{\theta\in\Theta_1}\o{U_n}(\theta)\ge\inf_{\theta\in\Theta_1}\o{U}(\theta)\quad\as\]
        \item $\o{U}$は下半連続．
    \end{enumerate}
\end{lemma}

\begin{theorem}[一様大数の法則]
    $(\Theta,d)$を距離空間，$\Theta_1\subset\Theta$をコンパクト集合，$U:\X\times\Theta\to[-\infty,\infty]$を関数とし，以下の条件を仮定する：
    \begin{enumerate}[({A}1)]
        \item $\forall_{x\in\X}\;U(x,-):\Theta\to\R$は連続．
        \item $\forall_{\theta\in\Theta}\;U(-,\theta):\X\to\R$は可測．
        \item ある可積分関数$M\in\L^1(\X)$が存在して，$\forall_{(x,\theta)\in\X\times\Theta}\;\abs{U(x,\theta)}\le M(x)$．
    \end{enumerate}
    このとき，$\o{U}(\theta):=\int_\X U(x,\theta)P(dx)\in\R$は連続で，
    \[\lim_{n\to\infty}\sup_{\theta\in\Theta_1}\abs{\o{U_n}(\theta)-\o{U}(\theta)}=0\quad\as\]
\end{theorem}

\section{最小コントラスト推定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    最尤推定の枠組みを一般化して，まずは一様大数の法則を準備した．
    最小コントラスト推定なる一般的な枠組みを用意して，一様大数の法則と一致性との間の十分条件を示す．
    $U,U_n$に関わる仮定は，ここまで削ぎ落とすことが出来る．
    基本的に，最尤推定のときのように，尤度関数$\Phi_n$を変形して$\o{U}_n$を作る．
\end{tcolorbox}

\subsection{定義と例}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    最尤推定量は，真の分布との対数尤度の差，すなわちKL-分離度を最小化するものとして得られた．
    経験分布の真の分布とのKL-分離度は，$n\in\N$に対応して関数を与える．これを$n\to\infty$の極限で漸近的に最小にするクラスを最小コントラスト推定量という．
\end{tcolorbox}

\begin{definition}[contrast function, minimum contrast estimator]
    $(\Theta,d)$を距離空間，$(\o{\X},\o{\A},\o{P})$を確率空間とする．
    \begin{enumerate}
        \item \textbf{コントラスト関数}とは，可積分関数$\o{\Psi}:\o{\X}\times\Theta\to(-\infty,\infty]$であって，次を満たすものをいう：
        \[\forall_{\theta\in\Theta}\quad E_{\theta_0}[\o{\Psi}(x,\theta)]\ge E_{\theta_0}[\o{\Psi}(x,\theta_0)].\]
        \item これを最小にする写像$\wt{\theta}:\o{\X}\to\Theta$を\textbf{最小コントラスト推定量}という．
    \end{enumerate}
    以降，コントラスト関数の列$(\o{\Psi}_n)$を漸近的に最小にする列$(\wt{\theta}_n)$を考える．
\end{definition}

\begin{example}[最尤推定]
    最尤推定量は，第一義的には，
    $\o{\Psi}_n(\theta):=-\bP_n \log p(x,\theta)$を最小化した．
    
    が，一致性の議論の中で，真の分布との対数尤度の差$\abs{l_n(\theta)-l_n(\wt{\theta})}$の最小化を考えた．
    これは損失関数のようでより直感的である．
    すなわち，
    \[\o{U}_n(\theta)=\bP_n \paren{\frac{\log p(x,\theta_0)}{\log p(x,\theta)}}\]
    と取ることも出来る．
    この考え方は，$Z$-推定量では出来ない，最小コントラスト推定量独自の議論である．

    この大数の法則と定理とを併せた議論は完全に\ref{remarks-consistency-of-MLE}の議論の厳密化となっている．
\end{example}

\subsection{強一致性を持つための十分条件}

\begin{theorem}[最小コントラスト推定量が強一致性を持つための十分条件]
    関数列$\o{U}_n$，関数$\o{U}:\Theta\to(-\infty,\infty]$，空でない部分集合$\Theta'\subset\Theta$，写像列$\wh{\theta}_n:\o{\X}\to\Theta$は次の3条件を満たすとする．
    \begin{enumerate}[({C}1)]
        \item 一様性：$\forall_{\ep>0}\;\liminf_{n\to\infty}\inf_{\theta\in\Theta:d(\theta,\Theta')\ge\ep}\o{U}_n(\theta)\ge\inf_{\theta\in\Theta:d(\theta,\Theta')\ge\ep}\o{U}(\theta)\;\as$
        \item 識別可能性：$\forall_{\ep>0}\;\inf_{\theta\in\Theta:d(\theta,\Theta')\ge\ep}\o{U}(\theta)>\inf_{\theta\in\Theta'}\o{U}(\theta)$．
        \item 最小コントラスト推定量である：$\limsup_{n\to\infty}\Square{\o{U}_n(\wh{\theta}_n)-\inf_{\theta\in\Theta'}\o{U}_n(\theta)}\le0\;\as$
    \end{enumerate}
    このとき，一様大数の法則が成り立つ：$\limsup_{n\to\infty}\inf_{\theta\in\Theta'}\o{U}_n(\theta)\le\inf_{\theta\in\Theta'}\o{U}(\theta)\;\as$ならば，$\lim_{n\to\infty}d(\wh{\theta}_n,\Theta')=0\;\as$
\end{theorem}

\subsection{一致性を持つ推定量の例}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
続いて，この枠組みを用いてより一般的なクラスについて一致性を得よう．
この枠組みの強みは，最小コントラスト推定量が陽に（解析的に）計算できない場合でも，一致性を示すことが出来る点である．
\end{tcolorbox}

\begin{example}[多項分布]\label{exp-multinomial-model}
    $k$個の事象への分割$\Om=\sum^k_{i=1}E_i$を考え，$P_\theta[E_i]=:\theta^i$を$k$個のパラメータとする．
    この試行を$n$回繰り返し，$E_i$が観測された回数を$y_i$とすると，これは多項分布に従う確率変数である：$(y_1,\cdots,y_k)\sim M(n;\theta^1,\cdots,\theta^k)$．
    パラメータ空間を
    \[\Theta:=\Brace{(\theta^1,\cdots,\theta^k)\in[0,1]^k\mid\sum^k_{i=1}\theta^i=1}\]
    とする．コントラスト関数を
    \[\o{\Psi}_n(\theta):=-\sum^k_{i=1}\frac{y_i}{n}\log\theta^i\]
    と定めると，これに対応する最小コントラスト推定量とは最尤推定量に他ならない．
    これに対して，関数を次のように定めれば，定理が用いることが出来て，一致性がわかる：
    \begin{align*}
        \o{U}(\theta):=-\sum^k_{i=1}\theta^i_0[\log\theta^i-\log\theta^i_0]\\
        \o{U}_n(\theta):=-\sum^k_{i=1}\frac{y_i}{n}[\log\theta^i-\log\theta^i_0]
    \end{align*}

    今回は実は最尤推定量$\wh{\theta_n}=(y_1/n,\cdots,y_k/n)$は陽に計算できるが，一般に尤度方程式も，コントラスト関数の最小点も求めることが出来るとは限らない．
    この枠組みは，そのような場合でも通用するように出来ている．
\end{example}

\subsection{独立観測の構造を仮定した場合}

\subsection{可測な最小コントラスト推定量の存在}

\section{$M$-推定量}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    最尤推定量は対数尤度関数を最大化する推定量で，最小コントラスト推定量は真の分布との分離度を最小化する推定量であった．
    このように，損失関数または効用関数（２つの概念を併せて目的関数）の極値点として与えられる推定量は，
    モデルが微分可能であるとき，これらは導関数の零点として得られる推定量として一般化できる．
    これを$M$-推定量という．
    \footnote{この意味での推定量を$Z$-推定量とし，$M$-推定量は効用関数の最大化で得るものと狭義に解釈することもある．}

    Huberが頑健推定の動機で1964年に最尤推定を一般化し，"maximum likelihood-type"の頭文字から$M$-推定量のクラスを定めた．\footnote{\url{https://en.wikipedia.org/wiki/M-estimator}}
    実はこれは可微分構造を持つ正則なモデルに関して最適でもある．
    このセミパラメトリックな一般化を経験尤度法(Owen 1988)という．
\end{tcolorbox}

\subsection{定義と例}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $M$-推定量はロバスト推定の代表である．
\end{tcolorbox}

\begin{definition}[estimating function / objective function]
    $\Theta\subset\R^p$とする．予め設定した関数$\psi:\X\times\Theta\to\R^p$に対して，$x_1,\cdots,x_n$が定める経験分布についての平均を
    \[\o{\psi_n}(x,\theta):=\bP_n\psi_n(x,\theta)=\frac{1}{n}\sum^n_{j=1}\psi(x_j,\theta)\]
    とおき，推定方程式$\o{\psi_n}(\theta)=0$の解として構成される推定量$\wt{\theta_n}$を\textbf{$M$-推定量}(または\textbf{$Z$-推定量})という．
    $\psi$またはその標本平均$\o{\psi_n}$を\textbf{目的関数}または\textbf{推定関数}という．
\end{definition}
\begin{remark}
    狭義には，目的関数$\psi(x,\theta)$の標本平均$\bP\psi(x,\theta)$を最大化する推定量$\theta:=\argmax_{\theta\in\Theta}\bP\psi(x,\theta)$を$M$-推定量という．
\end{remark}

\begin{example}\mbox{}
    \begin{enumerate}
        \item 最尤推定量は，スコア関数を推定関数とする$M$-推定量であり，尤度方程式$S_n(x,\theta)=\partial_\theta l(\theta,x)=0$の零点として特徴付けられる．
        \item 最小コントラスト推定量は，$\partial_\theta\Psi_n(\theta)=0$なる推定関数に対応する$M$-推定量である．
        \item 非線形最小二乗法は，最小二乗$\rho(u)=u^2/2$を損失関数として，これを最小化する$M$-推定量である．このときの推定関数は，この損失関数の微分なので，$\psi(x)=x$である．なお，この$\psi$の代わりに，剪断するHuberの$\psi$（が定める損失関数）を用いたものを，頑健回帰という：
        \[\psi(u_i):=\begin{cases}
            u_i,&\abs{u_i}\le H,\\
            H,&u_i>H,\\
            -H,&u_i<H.
        \end{cases}\]
        このときの$H$を\textbf{調整定数}(tuning constant)という．
    \end{enumerate}
\end{example}

\subsection{影響関数}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    Huber and Ronchetti (2009)\cite{Huber} p.47．
    Hampel et al. (1986) p.85\cite{Hampel}
    $M$-推定量の影響関数は，推定関数$\psi$の定数倍になる．
    特に最尤推定量について言えば，スコア関数の定数倍となる．
    また，$\Var[\IF^2]$が漸近分散となる．
    最尤推定量について，これはFisher情報量である．
\end{tcolorbox}

\begin{theorem}
    \[\IF(x;P,\theta)=\]
\end{theorem}

\section{$M$-推定量の漸近正規性}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    一致性は大数の法則に当たるならば，その真値への収束の速度が同様に気になる．これが漸近正規性である．
    これを，$M$-推定量について議論する．
\end{tcolorbox}

\subsection{一致性を持つ$M$-推定量の漸近分布}

\begin{notation}
    モデルの可微分構造を仮定：$\Theta\subset\R^p$を可測集合とする．推定関数$\psi:\X\times\Theta\to\R^p$とし，$\psi(x,-)$は連続とする．
    $\psi_n(\theta):=\bP_n\psi(x,\theta)=\sum_{j=1}^n\psi(x_j,\theta)$と定める．
    $\o{\psi}_n(\theta):=\bE_n[\psi(\theta)]=\frac{1}{n}\psi_n(\theta)$，
    $\o{\psi}(\theta):=\int_\X\psi(x,\theta)P(dx)$と定める．
\end{notation}

\begin{theorem}[漸近正規性の十分条件]\label{thm-asymptotic-normality}
    $\theta_0\in\Theta^\circ$とし，$B\subset\Theta$を$\theta_0$の開近傍とする．
    次の５条件を仮定する．
    \begin{enumerate}[({E}1)]
        \item 推定関数$\psi:\X\times\Theta\to\R^p$は$B$上$C^1$級．
        \item $\psi(-,\theta)$は可測．
        \item $\psi(x,\theta_0)\in\L^2(P;\R^p)$で，$P\psi(x,\theta_0)=0$．
        \item $\exists_{M\in\L^1(\X,\R)}\;\forall_{x\in\X,\theta\in B}\;\abs{\partial_\theta\psi(x,\theta)}\leq M(x)$．
        さらに，$\Gamma:=-P\partial_\theta\psi(x,\theta_0)$は正則．
        \item 確率変数列$\wt{\theta}_n:\X^n\to\Theta$が$\wh{\theta}_n\xrightarrow{p}$かつ$\psi_n(\wh{\theta}_n)=o_p(\sqrt{n})$を満足する．
    \end{enumerate}
    このとき，
    \[\sqrt{n}(\wh{\theta}_n-\theta_0)-\Gamma^{-1}\frac{1}{\sqrt{n}}\psi_n(\theta_0)=o_p(1)\quad(n\to\infty).\]
    特に，$\Sigma:=\Gamma^{-1}\Phi(\Gamma^{\top})^{-1}$，$\Phi:=\int_\X\phi\phi^{\top}(x,\theta_0)P(dx)$について，
    \[\sqrt{n}(\wh{\theta}_n-\theta_0)\xrightarrow{d}N_p(0,\Sigma)\quad(n\to\infty).\]
\end{theorem}

\subsection{収束レート}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $\Theta$上の関数$\theta\mapsto P(\Psi_\theta-\Psi_{\theta_0})$と$\theta\mapsto E^*[\abs{\bG_n(\Psi_\theta-\Psi_{\theta_0})}]$の$\theta_0$近くのHolder指数
    によって，$d(\wh{\theta}_n,\theta_0)$の収束レートが定まる．
\end{tcolorbox}

\begin{notation}
    コントラスト関数$\Psi_\theta(x)$について，目的関数$\theta\mapsto\bP_n\Psi_\theta(x)$を（近似的に）最大化する推定量列を$(\wh{\theta}_n)$とする．
\end{notation}

\begin{theorem}[rate of convergence]
    ある$C\in\R,\al>\beta\in\R$と任意の$n\in\N$と十分小さい任意の$\delta>0$について，次が成り立つとする：
    \[\sup_{d(\theta,\theta_0)<\delta}P(\Psi_\theta-\Psi_{\theta_0})\le-C\delta^\al,\qquad E^*\Square{\sup_{d(\theta,\theta_0)<\delta}\abs{\bG_n(\Psi_\theta-\Psi_{\theta_0})}}\le C\delta^\beta.\]
    \[\bP_n\Psi_{\wh{\theta}_n}\ge\bP_n\Psi_{\theta_0}-O_P\paren{n^{\frac{\al}{2\beta-2\al}}},\qquad\wh{\theta}_n\xrightarrow{P^*}\theta_0\]
    このとき，$n^{\frac{1}{2\al-2\beta}}d(\wh{\theta}_n,\theta_0)=O^*_P(1)$．
\end{theorem}

\subsection{最尤推定量での例}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $\psi(x,\theta):=\partial_\theta l(x;\theta)$を推定関数とし，この零点を最尤推定量という．
\end{tcolorbox}

\begin{theorem}[最尤推定量の漸近正規性の十分条件]\label{thm-ASN-of-MLE}
    
\end{theorem}

\subsection{モーメント法での例}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    良い条件の下でモーメント推定量は$\sqrt{n}$-一致性と漸近正規性を持つが，最尤推定量のものより漸近分散は大きい．
    これはデルタ法の結果である．
\end{tcolorbox}

\begin{notation}
    $\{P_\theta\}_{\theta\in\Theta}\subset P(\X)$を分布族，$g:\X\to\R^p$を所与の関数とする．
    $\psi(x,\theta):=g(x)-\int_\X g(y)P_\theta(dy)$とすると，真値$\theta_0\in\Theta$について$E[\psi(x,\theta_0)]=0$である．
    このとき，次の識別可能条件がなりたつとする
    \[\theta_1\ne\theta_2\Rightarrow\int_\X g(y)P_{\theta_1}(dy)\ne\int_\X g(y)P_{\theta_2}(dy).\]
    このとき，$\psi(x,\theta)$の$P_{\theta_0}$-平均$E_{\theta_0}[\psi(x,\theta)]$の零点は$\theta_0$に限る．
    たとえば$g(x)=x^n$と取ると，この条件を満たす．
\end{notation}

\begin{definition}[method of moments]
    この$\psi$を推定関数とした$M$-推定量$\wh{\theta}^\dagger_n$，すなわち次の$\theta$についての方程式の解を\textbf{$g$に関するモーメント推定量}という：
    \[\o{\psi}_n(x,\theta):=\bP_n[\psi(x,\theta)]=\frac{1}{n}\sum_{j=1}^n\psi(x_j,\theta)=0\quad\Leftrightarrow\quad \bP_ng(x)=P_\theta g(x)\]
    あるいは，あるノルム$\norm{-}$について，$\norm{\o{\psi}_n(x,\theta)}$を最小にする$\wh{\theta}$を探す(これを近似的モーメント推定量(approximate moment estimator)という)．
    このとき，推定関数$\psi$を\textbf{モーメント関数}という．
\end{definition}

\begin{theorem}
    次の条件を仮定する．
    \begin{enumerate}[({M}1)]
        \item $e(\theta):\theta\mapsto P_\theta f\in\R^k$は$\Theta\osub\R^k$上の単射で，$\theta_0$上で連続微分可能で，非特異なJacobi行列$e'_{\theta_0}\in\GL_k(\R)$を持つとする．
        \item $P_{\theta_0}\norm{f}^2<\infty$．
    \end{enumerate}
    このとき，モーメント推定量$\wh{\theta}_n$は任意に$1$に近い確率で存在し，次を満たす：
    \[\sqrt{n}(\wh{\theta}_n-\theta_0)\xrightarrow{\theta_0}N\paren{0,e'^{-1}_{\theta_0}P_{\theta_0}ff^\top(e'^{-1}_{\theta_0})^\top}.\]
\end{theorem}

\subsection{一般化モーメント法}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    Lars HansenはPearson 1894によるモーメント法を拡張してファイナンスに応用し，2013年のノーベル経済学賞を受賞した．
    モーメントに限らず，真の分布を特徴付ける別の母数$\psi$で計算しやすいものがあったら，これに変換することで
    $M$-推定量を構成する．
    最尤法は一般化モーメント法の特殊な場合とみなせる．
\end{tcolorbox}

\section{LeCamのワンステップ更新による漸近有効推定量の構成}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=一致推定量さえ見つかれば，漸近分散は逆Fisher情報行列まで改善できる標準的算譜がある]
    $M$-推定量は推定関数の零点として定まるが，実際に方程式を解くことは容易でない場合も多い．
    また推定方程式の解が一致性を持つためには，推定関数が$\Theta$の大域的性質を満たす必要がる．

    一方で，有効でないが，一致性をもつ推定量は容易に見つかることがしばしばある．
    このとき，それを用いて，最尤推定量と同じ漸近分散を持つ推定量を構成できる．

    推定量の漸近有効性は，数値解析のようなNewton-Rhapson反復法を採用すれば良いことは初等的に思いつくが，
    実はこの手続きは１回で$Z$-推定量(最尤推定量)と同等にまで改善される．\footnote{\url{https://www.ma.imperial.ac.uk/~das01/MyWeb/M3S3/Handouts/OneStep.pdf}}
\end{tcolorbox}

\begin{definition}[one-step estimator]
    $\Theta$-値確率変数列$(\wh{\theta}_n^0)$から，新たな推定量の系列$(\wh{\theta}_n)$を
    \[\wh{\theta}_n:=\wh{\theta}^0_n-(\partial_\theta\psi_n(\wh{\theta}^0_n))^{-1}\psi_n(\wh{\theta}^0_n)\]
    で定めると，これは
    \[\X^*_n:=\Brace{x\in\X^n\mid\wh{\theta}^0_n\text{において}\psi_n\text{は微分可能で}\partial_\theta\psi_n(\wh{\theta}^0_n)\text{は正則で}\wh{\theta}_n\in\Theta}\]
    上で定まるから，これを$\X$上に可測に延長する．延長の仕方は任意で良い．
    こうして得た$(\wh{\theta}_n)$を\textbf{$(\wh{\theta}_n^0)$を初期推定量とするワンステップ推定量}という．
\end{definition}
\begin{remarks}
    実は，推定関数の標本平均$\o{\psi}_n$が，ある正則行列$\dot{\psi}_0\in\R^p$について次を満たすとき，
    \[\sup_{\sqrt{n}\norm{\theta-\theta_0}<M}\Norm{\sqrt{n}(\o{\psi}_n(\theta)-\o{\psi}_n(\theta_0))-\dot{\psi}_0\sqrt{n}(\theta-\theta_0)}\xrightarrow{p}0.\]
    係数$(\partial_\theta\psi_n(\wh{\theta}^0_n))^{-1}$は任意の推定量で良い．
    このとき，任意のランダムな行列$\dot{\psi}_{n,0}$はある$\dot{\psi}_0$に収束するならば，
    \[\wh{\theta_n}:=\wh{\theta}^0_n-\dot{\psi}_{n,0}^{-1}\o{\psi_n}(\wh{\theta}^0_n)\]
    で定まる推定量も\textbf{ワンステップ推定量}といい，次の定理を満たす．
\end{remarks}

\begin{theorem}[ワンステップ推定量が漸近正規であるための十分条件]
    $\theta_0\in\Theta^\circ$，$B\subset\Theta$を$\theta_0$の開近傍とする．
    条件(E1)~(E4)を仮定し，次の条件を仮定する：
    \begin{enumerate}[({E}1)]\setcounter{enumi}{5}
        \item 初期推定量の列$\wh{\theta}^0_n:\X^n\to\Theta$が$\sqrt{n}$-一致性を持つ：$\wh{\theta}^0_n-\theta_0=O_p(1/\sqrt{n})$．\footnote{条件$\wh{\theta}^0_n-\theta_0=O_p(1/\sqrt{n})$は，弱一致性$\wh{\theta}^0_n-\theta_0=o_p(1)$を含意することに注意．これに加えて，$n^{-1/2}$倍の範囲で真値$\theta_0$を捕まえていることは要求する．}
    \end{enumerate}
    このとき，ワンステップ推定量$\wh{\theta}_n$に対して，定理\ref{thm-asymptotic-normality}と同様の結果が成り立つ：
    \[\sqrt{n}(\wh{\theta}_n-\theta_0)=-\Gamma^{-1}\frac{1}{\sqrt{n}}\psi_n(\theta_0)+o_P(1).\]
    特に，$\sqrt{n}(\wh{\theta}_n-\theta_0)\xrightarrow{d}N_p(0,\Sigma)$．
\end{theorem}

\section{Cramerの一致推定量の構成}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    一致推定量の漸近有効性を改善する算譜を得たので，次は一致推定量の構成算譜を考える．

    これにより，$\Theta$全域で$L^2$-微分可能で非退化なFisher情報行列を持つ識別可能なパラメトリックモデル$(P_\theta)_{\theta\in\Theta}$には正則で漸近最適な$\theta$の統計量が存在することが示せる．
\end{tcolorbox}

\section{頑健推定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    1964年にHuberによって提唱された．
\end{tcolorbox}

\subsection{影響関数}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    影響関数は，有界線型作用素$T:P(\X)\to\Theta$の特別な微分とみなせる．
\end{tcolorbox}

\begin{notation}
    推定関数$\psi:\X\times\Theta\to\R^p\;(\Theta\osub\R^p)$に対して，汎関数$T:P(\X)\to\Theta$は推定方程式$\forall_{P\in\P}\;\int_\X\psi(x,T(P))P(dx)=0$を満足すると仮定する．
    すると経験分布$\bP_n$について，$\wt{\theta_n}:=T(\bP_n)$は推定関数$\psi$に関する$M$-推定量である．
    このようにして，経験分布の汎関数として得られる推定量のクラスを考える．

    推定関数は$\theta$について可微分とし，
    \[\Gamma(P):=-\int_\X\partial_\theta\psi(y,T(P))P(dy)\quad\in M_p(\R)\]
    とする．
\end{notation}
\begin{remark}[汎関数$T:\P\to\R$の描像は？]
    $\P$の極点の凸包上での値は$M$-推定量を与え，極点の凸包は$\P$上弱稠密である．
    したがって$T(P)$の分布は一意に定まる(?)
\end{remark}

\begin{definition}\mbox{}
    \begin{enumerate}
        \item 確率測度$P\in P(\X)$に対して，$x\in\X$が定めるデルタ測度との$\ep\in[0,1]$が定める凸結合を$P^x_\ep:=(1-\ep)P+\ep\delta_x\in P(\X)\;(x\in\X,\ep\in[0,1])$と定める．
        \item $T$について，$P\in P(\X)$における$\ep\in[0,1]$に関する微分係数$\IF(x;T,P):=\dd{}{\ep}T(P^x_\ep)|_{\ep=0}(=\Gamma(P)^{-1}\psi(x,T(P)))$を関数$\X\to\Theta$とみたとき，これを\textbf{$T$の$P$における影響関数}という．
        これは，各点$x\in\X$に観測を得た場合に，$M$-推定量を与える写像$T$がどれほど影響を受けるかを表す指標とみなせる．
    \end{enumerate}
\end{definition}

\begin{lemma}[$M$-推定量の影響関数]
    $T$が前述条件を満たす$M$推定量を与える汎関数であり，$\forall_{(x,\theta)\in\X\times\Theta}\;\abs{\partial_\theta\psi}$は可積分であるとする．
    \[\IF(x;T,P)=\Gamma(P)^{-1}\psi(x,T(P)),\qquad\Gamma(P):=-\int_\X\partial_\theta\psi(y,T(P))P(dy)\]
    特に，$p=1$のとき，推定関数$\psi(x,T(P))\in\R^p$の定数倍である．
\end{lemma}
\begin{proof}
    推定方程式より
    \[0=\int_\X\psi(x,T(P+\ep\delta_x))(P+\ep\delta_x)(dx)=\int_\X\psi(x,T(P+\ep\delta_x))P(dx)+\ep\psi(x,T(P+\ep\delta_x))\]
    が成り立つ．この両辺を$\ep$で微分して$\ep=0$を考えると，関数$\forall_{(x,\theta)\in\X\times\Theta}\;\abs{\partial_\theta\psi}$は可積分であるとすると，微分と積分とは交換可能であるから，
    \[0=\paren{\int_\X\partial_\theta\psi(x,T(P))P(dx)}\cdot\left.\dd{}{\ep}T(P+\ep\delta_x)\right|_{\ep=0}+\psi(x,T(P)).\]
\end{proof}

\begin{example}
    正規母集団$N(\theta,1)$の位置母数$\theta$に対する最尤推定量$\wt{\theta_n}$の影響関数は$\IF(x;T,N(\theta,1))=x-\theta$となる．したがって，$x\in\R$の絶対値が大きいほど，最尤推定量への影響が大きいため，ここに異常値が来ると，大きく推論に影響する危険がある．
\end{example}

\subsection{バイアスロバスト推定量}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    頑健統計では，外れ値による影響がなるべく小さい$M$-推定量を構成することを考える．
    $M$-推定量の影響関数には推定関数$\psi$が明示的に出現するため，頑健性の考察がしやすい．
\end{tcolorbox}

\begin{definition}[bias robust estimation]
    $(P_\theta)_{\theta\in\Theta}$をモデルとする．
    ある定数$M\in\R$が存在して，$\Gamma_\theta:=\Gamma(P_\theta)$について$\abs{\Gamma^{-1}_\theta\psi(x,\theta)}\le M$を満たす$\psi$が定める$M$-推定量を\textbf{$B$-ロバスト推定量}という．
\end{definition}

\begin{example}[最尤推定]
    最尤推定量は，漸近分散が最小という意味では最適であるが，一般に影響関数は有界にはならない．
    特に，$Y$方向の誤差について頑健であるが，$X$方向の誤差については有界ではない．
    ここを補おうとするのが，\textbf{有界影響推定}またはGM推定である．
\end{example}

\begin{example}[]
    OLSによって係数を推定した回帰モデルは，崩壊点(breakdown point) $1/n$を持つ非頑健なモデルである．
    そこで，外れ値＝推定値に高い影響力を持つ影響点を検出する手法「回帰診断」があり，外れ値に対するウェイトを制御することで頑健なパラメータ推定を行うのが頑健回帰となる．
\end{example}

\subsection{Fisherの一致性}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    Fisherの一致性を仮定すると，最適な不偏ロバスト推定量を構成できる．
\end{tcolorbox}

\begin{definition}[Fisher consistency]
    写像$T:P(\X)\to\Theta$が，モデル$(P_\theta)$について，$T(P_\theta)=\theta$を満たすとき，\textbf{Fisher一致性を満たす}という．
\end{definition}

\section{頑健回帰推定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    Andersen \textit{Modern Methods for Robust Regression} (2008)
    
\end{tcolorbox}

\section{尤度比検定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    初等統計で扱われるカイ$2$乗検定の根拠は，この漸近理論によって明らかになる．
\end{tcolorbox}

\subsection{検定から見た漸近論}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    「密度推定は無理でも，汎関数の分布ならわかる」という技法である．
\end{tcolorbox}

棄却域を定めるに当たって，
決定理論から観て妥当な設計をするためには，
検定統計量の帰無仮説の下での分布を知る必要がある．
これを解析的に行なえない場合やモデルのmisspecificationが疑われる場合は漸近分布によって近似をすることとなる．

統計量が$t$-分布に従うと仮定するとき，$\chi^2$-分布に従うと仮定するときの検定法を$t$-検定，$\chi^2$-検定というのである．
パラメトリックモデル$N(\mu,\sigma^2)$を仮定したとき，$S_n$を標本標準偏差として$\frac{\sqrt{n}(\o{X_n}-\mu)}{S_n}$は$\tau$-分布に従い，
多変数の場合は二次形式$Z^\top\Sigma^{-1}Z$は$\chi^2$-分布に従うことが判明している．
また，Pearsonの適合度検定や一部の尤度比検定は，検定統計量が漸近的に$\chi^2$-分布に従う．
だがこれらはtoy exampleである．平均や二次形式を検定するために，よりよい検定統計量を構成できる．
これらは大抵分布がわからないから，大標本理論＝漸近理論に頼ることとなる．

Neyman-Pearsonの理論はある指数分布族モデルに対して一様最強力検定の構成を可能にする．
Rao-Blackwellの理論は不偏推定量のクラスの中で分散が最小のものが存在することを明らかにする．
一方でこのような理論が使えないときは，漸近最適理論が呼ばれる．
次に競われるのは検定出力関数(power function)の近似や，漸近分散となる．
例えば，滑らかなパラメトリックモデルに対しては最尤推定量が漸近最適である．
\begin{enumerate}
    \item 漸近的一致性を持つ．
    \item 真値への収束レートが最速である：$n^{-1/2}$．
    \item 極限分布の分散が最小である．実際，Cramer-Raoを漸近的に満たす．
\end{enumerate}

\subsection{尤度比検定}

\begin{notation}
    $\Theta\subset\R^p,\theta_0\in\Theta^\circ$とする．$(\X,\A)$上の$\sigma$-有限な確率分布族$\{P_\theta\}_{\theta\in\Theta}\subset P(X)$に対する検定
    \[H_0:\theta=\theta_0\quad\vs\quad H_2:\theta\ne\theta_0\]
    を考える．$x=(x_1,\cdots,x_n)\in\X^n$を無作為標本とする．
\end{notation}

\begin{definition}
    尤度関数$L_n(\theta):=\prod^n_{j=1}p(x_j,\theta)$の比
    \[\Lambda_n:=\frac{L_n(\theta)}{\sup_{\theta\in\Theta}L_n(\theta)}\in[0,1]\]
    の値を考え，$\Lambda_n:\X^n\to[0,1]$の値が小さい時に帰無仮説を棄却する検定を\textbf{尤度比検定}という．
\end{definition}
\begin{remark}
    $\wh{\theta}_n$が$\theta$の最尤推定量であるとき，定義上これが尤度関数を最大にする点であるから，$\Lambda_n=\frac{L_n(\theta_0)}{L_n(\wh{\theta}_n)}$となる．
\end{remark}

\subsection{棄却域の定め方}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    では，棄却域はどう定めるのが「良い」か？
    $\Lambda_n:\X^n\to[0,1]$の分布は極めて複雑であるため，近似を用いる．
    ここで漸近論が登場し，漸近正規性が，ミニマックスの意味で「妥当」であることが言える．
    正しくは，漸近決定理論の枠組みで捉えるのが良い．
\end{tcolorbox}

\begin{definition}\mbox{}
    \begin{enumerate}
        \item $\theta$に関する，規準化されたスコア関数を$Z_n(\theta):=\frac{1}{\sqrt{n}}\sum^n_{j=1}\partial_\theta'\log p(x_j,\theta)$とする．
        \item $Z_n:=Z_n(\theta_0)$とする．
    \end{enumerate}
\end{definition}

\begin{theorem}
    $\wh{\theta}_n\xrightarrow{P_{\theta_0}}\theta_0$を満たす最尤推定量が存在し，
    漸近正規性を持つための十分条件[F1]~[F4]を満たすとする\ref{thm-ASN-of-MLE}．
    このとき，$P_{\theta_0}$の下で
    \[-2\log\Lambda_n\xrightarrow{d}\chi^2(p)\quad(n\to\infty)\]
\end{theorem}

\subsection{複合仮説の検定}

\subsection{Rao検定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    複合仮説の検定に関しては，尤度比検定に変わって，ラオ検定（Lagrange未定乗数法検定）を用いることが出来る．
    これは$H_0$の下で尤度比検定と同じ漸近分布$\chi^2(r)$を持つ．
\end{tcolorbox}

\subsection{Wald検定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    Wald検定も尤度比検定と漸近同等であって，特に$H_0$の下で漸近的に分布$\chi^2(r)$に従う．
\end{tcolorbox}

\section{多項分布の検定}

\begin{notation}[モデルの設定]
    \[\Delta:=\Brace{p=(p_1,\cdots,p_k)\in\R_+^k\mid\sum^k_{i=1}p_i=1}\]
    とするとこれは多項分布の全体とみなせる．開集合$\Theta\subset\R^l$から$\Int(\Delta)$への単射な$C^1$級写像$p:\mono\Theta\to\Int(\Delta)$で定まるパラメトリックモデル$\Theta$を考える\ref{exp-multinomial-model}．
    さらに$U\osub\R^m\;(1\le m<l\le k-1)$上の$C^1$級部分モデル$\theta:U\mono\Theta$を考える．
\end{notation}

\begin{problem}
    仮説検定問題
    \[H_0:\theta\in\theta(U)\quad\vs\quad H_1:\theta\notin\theta(U)\]
    を考える．スコア関数と漸近同等な$\theta,u$の推定量$\wh{\theta}_n,\wh{u}_n$がモデル$\Theta,U$のそれぞれに対して得られているとする．
    このとき，検定統計量$Q_n,Q_n^*$を
    \begin{align*}
        Q_n&=\sum^k_{i=1}\frac{[np_i(\wh{\theta}_n)-np_i\circ\theta(\wh{u}_n)]^2}{np_i\circ\theta(\wh{u}_n)}\\
        Q_n^*&=\sum^k_{i=1}\frac{[np_i(\wh{\theta}_n)-np_i\circ\theta(\wh{u}_n)]^2}{np_i(\wh{\theta}_n)}
    \end{align*}
    とし，$Q_n,Q_n^*$の値が大きいときに$H_0$を棄却することを考える．
\end{problem}

\begin{theorem}
    $H_0$の下で，$Q_n,Q_n^*$の漸近分布はカイ2乗分布$\chi^2(l-m)$である．
\end{theorem}
\begin{remarks}
    証明抽出により，尤度比検定と漸近同等であることがわかる．
\end{remarks}

\begin{example}[goodness-of-fit test]
    各事象$E_i$が起こる確率$p_i$は，与えられた値$p^*_i$に等しい，という単純仮説の検定を\textbf{適合度検定}という．
    このとき，漸近分布は$\chi^2(k-1)$である．
\end{example}

\section{接触構造}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $P_n$が帰無仮説，$Q_n$が対立仮説の法則という意味論がある．
    接触しているとは\textbf{漸近的絶対連続}であることをいい，統計量の極限において，測度変換を与える基礎になる．
\end{tcolorbox}

\subsection{尤度比と測度変換}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=測度変換の公式が成り立つためには，絶対連続性が必要である．]
    2つの確率変数があったときに，一方のもう一方に対する絶対連続部分のRadon-Nikodym微分を\textbf{尤度比確率変数}という．これは零集合上の差を除いて一意に定まる．
    「尤度比」という名前は実用上の重要性が先行して考察されている対象かと思いやすいが，実際は本質的に数学的に自然に重要な対象である．

    絶対連続性$Q\ll P\Leftrightarrow P(A)=0\Rightarrow Q(A)=0$は，$P\dae$性質はそのまま$Q\dae$に成り立つことを意味し，$\supp q\subset\supp p$だから$P$のグラフが上から覆いかぶさっている形になる．
    $q/p\; dP$なる形式は$Q^a$を表す．
    実はこの$\Om$全域での積分が$1$であるとき，$Q^\perp=0$である．
    実際，
    \[\int fdQ\ge\int f\frac{dQ}{dP}dP\quad(f\ge0)\]
    が一般に成り立つが，等号が成り立って測度変換公式として使えるための必要十分条件が$Q\ll P$である．

    Lebesgue測度は参照測度として優秀で，人類はみなこの測度に引き戻して計算を行う．
\end{tcolorbox}

\begin{motivation}[尤度比とは，数学的には絶対連続な確率測度組のRadon-Nikodym微分のことに他ならない]\mbox{}
    \begin{enumerate}
        \item （参照測度$\nu$の下の）任意の確率測度$P,Q$について，絶対連続部分と直交／特異部分とへのLebesgue分解$Q=Q^a+Q^\perp$が存在して，$Q^a\ll P$かつ$Q^\perp\perp P$かつ
        \[\forall_{A\in\F}\quad Q^a(A)=\int_A\frac{q}{p}dP,\qquad \frac{dQ}{dP}=\frac{q}{p}\;P\dae\]
        が成り立つ．
        \item こうして，Radon-Nikodym微分という確率変数$\frac{dQ}{dP}:\Om\to\R_+$を確率$P$の下で考えるというテーマが定まる．これを\textbf{尤度比}という．
        \item 尤度比の平均値が$1$であることと，$Q^\perp=0$であることと，絶対連続$Q\ll P$であることとは同値．
    \end{enumerate}
\end{motivation}

\begin{lemma}
    $P,Q\ll\mu$を確率測度とし，対応する密度を$p,q$とする．
    \begin{enumerate}
        \item 絶対連続部分$Q^a$の参照測度$\mu$が介在しない表示$Q^a(A)=\int_A\frac{q}{p}dP$が成り立つ．
        \item 尤度比の$P$-平均値が$1$であること$\int\frac{q}{p}dP=1$と，$Q^\perp=0\Leftrightarrow Q[p=0]=0$であることと，絶対連続$Q\ll P$であることとは同値．
        \item 一般に$\int fdQ\ge\int f\frac{dQ}{dP}dP$が成り立ち，等号が成立するのは$Q\ll P$のとき．
    \end{enumerate}
\end{lemma}

\subsection{接触性}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $X$の$Q$についての法則を，$P$の言葉で考えたいとき，$q/p$を積分核とすれば良いことがわかった．この
    測度変換の問題の漸近版を考える．
    いつ，極限実験$P$を用いて，対象の実験列$(Q_n)$の極限分布が計算可能になるか？
\end{tcolorbox}

\begin{notation}
    台となる測度空間$(\Om^n,\A^n)$を共通とする統計的実験の列$(P_n),(Q_n)$を考える．
    そしてそこでの統計量の列$X_n:\Om^n\to\R^k$との振る舞いを考察する．
\end{notation}

\begin{definition}[contiguous]
    モデル$(Q_n)$は\textbf{$(P_n)$下に接触している}$Q_n\triangleleft P_n$とは，任意の可測集合の列$(A_n),A_n\in\A^n$に対して，$P_n(A_n)\to0\Rightarrow Q_n(A_n)\to0$が成り立つことをいう．
\end{definition}

\begin{remark}
    $\frac{dQ_n}{dP_n},\frac{dP_n}{dQ_n}$はそれぞれ$P_n,Q_n$の下で一様に緊密であるから，Prohorovの定理から，弱コンパクトである．
    すなわち，任意の部分列は，弱収束する部分列をもつ．
    以降，この意味で$U$に収束するとき，$\frac{dP_n}{dQ_n}\xrightarrow[Q_n]{d}^*U$と表す．
\end{remark}

\begin{lemma}[接触性の特徴付け(LeCam 1)]
    次の4条件は同値．
    \begin{enumerate}
        \item $Q_n\triangleleft P_n$．
        \item $\frac{dP_n}{dQ_n}\xrightarrow[Q_n]{d}^*U$ならば，$P[U>0]=1$である．
        \item $\frac{dQ_n}{dP_n}\xrightarrow[P_n]{d}^*V$ならば，$E[V]=1$である．
        \item 任意の統計量$T_n:\Om_n\to\R^k$について，$T_n\xrightarrow{P_n}0$ならば$T_n\xrightarrow{Q_n}0$である．
    \end{enumerate}
\end{lemma}
\begin{remarks}
    漸近的でない場合の消息でいうと，
    \[Q_n\paren{\frac{dP_n}{dQ_n}=0}=0\quad\Leftrightarrow E_P\Square{\frac{dQ}{dP}}=1\]
    は$Q_n\ll P_n$に同値だが，$Q_n\triangleleft P_n$と同値になるためには，上のような表現となる．
\end{remarks}

\begin{example}[漸近的対数正規性]
    \[\frac{dP_n}{dQ_n}\xrightarrow[Q_n]{d}e^{N(\mu,\sigma^2)}\]
    が成り立つとき，$Q_n\triangleleft P_n$で，さらに互いに接触していることは$\mu=-\frac{1}{2}\sigma^2$に同値．
\end{example}

\begin{theorem}[漸近的測度変換(LeCam 3)]
    $X_n:\Om^n\to\R^k$を確率変数列とする．
    \begin{enumerate}
        \item $Q_n\triangleleft P_n$，
        \item $\paren{X_n,\frac{dQ_n}{dP_n}}\xrightarrow[P_n]{d}(X,V)$
    \end{enumerate}
    ならば，$L(B):=E[V,B]=E[1_BV]$は確率測度を定め，これについて$X_n\xrightarrow[Q_n]{d}L$．
\end{theorem}

\begin{lemma}[接触性の十分条件]
    $P_n,Q_n$はある$\sigma$-有限測度$\nu_n$に対して絶対連続であるとし，Radon-Nikodym微分を$p_n,q_n$とおく．
    \[\Lambda_n:=\begin{cases}
        \log(q_n/p_n),&p_n,q_n>0,\\
        0,&p_n=q_n=0,\\
        \infty,&p_n=0,q_n>0,\\
        -\infty,&p_n>0,q_n=0.
    \end{cases}\]
    ある確率変数$\Lambda$が存在して$P_n$の下で$\Lambda_n\xrightarrow{d}\Lambda$かつ$E[e^\Lambda]=1$であるならば，$(Q_n)$は$(P_n)$に接触している．
\end{lemma}

\section{局所漸近正規性}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
パラメータのスケーリングの違いを除いて，局所漸近正規なモデルはあるガウスモデルに収束する．
\end{tcolorbox}

\begin{discussion}
    ある点$\theta_0\in\R^k$に注目して，その周りの
    局所パラメータを$h:=\sqrt{n}(\theta-\theta_0)$とすると，元の統計的実験$(\X^n,\A^n,(P_\theta^n)_{\theta\in\R^k})$は，
    $(\X^n,\A^n,(P^n_{\theta_0+h/\sqrt{n}})_{h\in\R^k})$と変換される．
    するとこの統計的実験は，元の統計的実験$\theta\mapsto P_\theta$が滑らかであるとき，
    $(\X^n,\A^n,(N(h,I_{\theta_0}^{-1}))_{h\in\R^k})$に漸近する！
\end{discussion}

\subsection{尤度の拡張}

\begin{discussion}
    $P_\theta\ll\mu$とし，$p_\theta:=\dd{P_\theta}{\mu}$と表す．
    ここでは$\theta,h\in\R$のような記法をするが，$k$次元としても$\dot{l}(x)$が$k$ベクトルになり，掛け算が内積または二次形式になるのみである．
    $l_\theta(x):=\log p_\theta(x)\in C^2(\X)$と仮定すると(この仮定は強すぎるが)，Taylorの定理より，
    \[\log P_{\theta+h}(x)=l_\theta(x)+\dot{l}_\theta(h)+\frac{1}{2}\ddot{l}_\theta(x)h^2+o_x(h^2).\quad h\in\R^k,h\to0\]
    \[\log\frac{p_{\theta+h}}{p_\theta}(x)=h\dot{l}_\theta(x)+\frac{1}{2}h^2\ddot{l}_\theta(x)+o_x(h^2).\quad h\in\R^k,h\to0\]
    ここで尤度比が現れる．特に$x$に観測値$X_1,\cdots,X_n$を代入し，$h$を$\frac{h}{\sqrt{n}}$として$n\to\infty$の極限を考えると，
    \[\log\prod_{i=1}^n\frac{p_{\theta+h/\sqrt{n}}}{p_\theta}(X_i)=\frac{h}{\sqrt{n}}\sum^n_{i=1}\dot{l}_\theta(X_i)+\frac{1}{2}\frac{h^2}{n}\sum^n_{i=1}\ddot{l}_\theta(X_i)+\sum_{i=1}^no_x\paren{\frac{h^2}{n}}\quad(n\to\infty)\]
    ここで，$E_\theta[\dot{l}_\theta]=0,-E_\theta[\ddot{l}_\theta]=E_\theta[\dot{l}^2_\theta]=I_\theta$だから，
    \begin{enumerate}
        \item $\Delta_{n,\theta}:=\frac{1}{\sqrt{n}}\sum^n_{i=1}\dot{l}_\theta(X_i)\in\R$とおくと$\Delta_{n,\theta}\xrightarrow{d}N(0,I_\theta)$（中心極限定理）．
        \item 大数の法則より$\frac{1}{n}\sum^n_{i=1}\ddot{l}_\theta(X_i)\xrightarrow{d}-I_\theta$．
        \item 3項目はおそらく確率収束（仮定をうまく設定すれば良い）．
    \end{enumerate}
    よって，
    \[\log\prod_{i=1}^n\frac{p_{\theta+h/\sqrt{n}}}{p_\theta}(X_i)=h\Delta_{n,\theta}-\frac{1}{2}I_\theta h^2+o_{p_\theta}(1)\]
    が成り立つ．
    実は，必要な過程は，$L^2$-微分の言葉で簡潔に書ける．
\end{discussion}

\begin{theorem}[局所漸近正規性の十分条件]
    モデル$(p_\theta)_{\theta\in\Theta\osub\R^k}$は，
    密度関数の平方根$\theta\mapsto\sqrt{p_\theta}$が$L^2$-微分可能であるとする：
    \[\exists_{\dot{l}\in\L^2(\X)}\quad\int\paren{\sqrt{p_{\theta+h}}-\sqrt{p_\theta}-\frac{1}{2}h^\top \dot{l}_\theta\sqrt{p_\theta}}^2d\mu=o(\norm{h}^2)\quad(h\to0).\]
    なお，通常の意味で微分可能であった場合は，
    \[\frac{1}{2}\dot{l}_\theta\sqrt{p_\theta}=\pp{\sqrt{p_\theta}(x)}{\theta}\quad\Leftrightarrow\quad \dot{l}_\theta=\pp{}{\theta}\log p_\theta(x)\]
    が必要であることに注意．このとき，
    \begin{enumerate}
        \item $E_\theta[\dot{l}]=0\in\R^k$かつ$I_\theta:=E_\theta[\dot{l}_\theta\dot{l}_\theta^\top]\in M_k(\R)$が定まる．
        \item 任意の収束列$\{h_n\}\subset\Theta,h_n\to h$について，$\Delta_{n,\theta}:=\frac{1}{\sqrt{n}}\sum^n_{i=1}\dot{l}_\theta(X_i)\in\R^k$とすると，
        \[\log\prod_{i=1}^n\frac{p_{\theta+h_n/\sqrt{n}}}{p_\theta}(X_i)=h^\top\Delta_{n,\theta}-h^\top\frac{1}{2}I_\theta h+o_{p_\theta}(1)\]
    \end{enumerate}
\end{theorem}

\begin{definition}[LAN: local asymptotic normal]
    モデル$(P_{n,\theta}|\theta\in\Theta)_{n\in\N}$が$\theta_0\in\Theta$において\textbf{局所漸近正規}であるとは，
    \begin{enumerate}
        \item 行列$r_n,I_{\theta_0}\in M_k(\R)$，
        \item $P_{\theta_0}$の下で$N(0,I_{\theta_0})$に分布収束する確率変数$\Delta_{n,\theta}:\Om\to\R^k$，
    \end{enumerate}
    が存在して，任意の収束列$\{h_n\}\subset\R^k,h_n\to h$について，
    \[\log\frac{dP_{n,\theta+r_n^{-1}h_n}}{dP_{n,\theta}}=h^\top\Delta_{n,\theta}-\frac{1}{2}h^\top I_\theta h+o_{P_{n,\theta}}(1)\]
    と展開できることをいう．
\end{definition}
\begin{example}
    $L^2$-微分可能なモデル$(P_\theta)_{\theta\in\Theta}$が定める実験列$(P^n_\theta)$は，$r_n=\sqrt{n}I$を行列として局所漸近正規である．
\end{example}

\subsection{正規実験への収束}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    上の「局所漸近正規性」の意味を，統計的実験列の収束概念以前の状態で考えたい．

\end{tcolorbox}

\begin{discussion}
    統計的実験が収束するとき，元の実験で弱収束する統計量は，極限実験での弱収束先に法則同等である．
\end{discussion}

\begin{definition}
    ランダム化された統計量$T=T(X,U)$とは，$U\sim U([0,1])$にも依存する統計量をいう．
\end{definition}

\begin{theorem}[正規実験へ収束するための十分条件]
    モデル$(p_\theta)_{\theta\in\Theta\osub\R^k}$は，
    密度関数の平方根$\theta\mapsto\sqrt{p_\theta}$が$L^2$-微分可能であり，Fisher情報行列$I_\theta:=E_\theta[\dot{l}_\theta\dot{l}_\theta^\top]\in\GL_k(\R)$が可逆であるとする．
    実験$(P_{\theta+h/\sqrt{n}})_{h\in\R^k}$における統計量$T_n:\X^n\to\R^k$が，
    任意の$P_{\theta+h/\sqrt{n}}\;(h\in\R^k)$について弱収束するとき，
    実験$(N(h,I^{-1}_{\theta}))_{h\in\R^k}$上のランダム化された統計量$T:=T(X,U):\X\times[0,1]\to\R^k$が存在して，任意の$h$について同じ収束先へ分布収束する．
\end{theorem}

\section{漸近決定理論}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $h_0\in H$上の尤度比過程は，周りの$h$と$h_0$との「分離度」を表す．実際対数尤度比の$P_h$についての平均が$h,h_0$の間のKL-分離度である．
    そこで統計的実験が収束するとは，任意の尤度比過程がバージョンの違いを除いて収束することとすると，
    統計モデルがある種の"Riemann多様体"として，各点において局所構造が等しいことと思える．

    極限的実験を定めると，ここでの効率限界が，元の実験での下界を与える．
\end{tcolorbox}

\subsection{実験列の収束の定義}

\begin{definition}[likelihood ratio process, limit experiment]
    統計的実験$(\X,\A,(P_h)_{h\in H})$について，
    \begin{enumerate}
    \item $h_0\in H$上の\textbf{尤度比過程}とは，$\paren{\frac{dP_{h}}{dP_{h_0}}(X)}_{h\in H}$，または参照測度が存在するとき，$\paren{\frac{p_h}{p_{h_0}}(X)}_{h\in H}$をいう．
    これは，観測という確率変数$X$に，モデルの密度関数族が定める積写像$(p_h/p_{h_0})_{h\in H}$を合成して得たものである．
    \item 実験列$((\X_n,\A_n,P_{n,h};h\in H))_{n\in\N}$が\textbf{極限実験}$(\X,\A,P_h;h\in H)$に収束するとは，それらの任意の$h_0\in H$上の尤度比過程の任意の有限次元周辺分布が，$h_0$が真のパラメータであるという過程の下で法則収束することをいう：
    \[\forall_{h_0\in H}\;\forall_{I\subset H}\;\abs{I}<\infty\Rightarrow\paren{\frac{dP_{n,h}}{dP_{n,h_0}}(X_n)}_{h\in I}\xrightarrow{h_0}\paren{\frac{dP_h}{dP_{h_0}}(X)}_{h\in I}.\]
    \end{enumerate}
\end{definition}

\subsection{漸近表現定理}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    法則収束する統計量の漸近的な振る舞いは，極限実験における統計量の考察に帰着する．
    実験内の最適統計量の考察と，実験の収束とは，完全に独立に議論できるのである！

    実は，さらに仮定をおくと，最尤推定量は極限実験での最尤推定量に収束し，尤度比統計量の列は尤度比統計量に収束することなども言える．
\end{tcolorbox}

\begin{notation}
    実験$\E_n=(P_{n,h}|h\in H)_{n\in\N}$上の統計量$T_n:\X^n\to H$を考える．
    任意の分布$P_h$の下で，$T_n\xrightarrow{P_h} L_h\in P(H)$に法則収束するとする．
    このとき，$T_n$の漸近的な振る舞いは極限法則$\{L_h\}_{h\in H}$に支配される．
    ここで，極限法則$\{L_h\}_{h\in H}$が，極限実験における（少し変換した）統計量の法則に対応する．
\end{notation}

\begin{definition}[dominated, randomized statistic]\mbox{}
    \begin{enumerate}
        \item 実験$\E=(P_h|h\in H)$が\textbf{支配されている}とは，ある$\sigma$-有限測度$\mu$が存在して$\forall_{h\in H}\;P_h\ll\mu$が成り立つことをいう．
        \item $(\X,\A,P_h|h\in H)$上の\textbf{ランダム化された統計量}$T$とは，可測写像$T:\X\times[0,1]\to\R^k$をいう．ただし，$[0,1]$上には一様分布を与える．
    \end{enumerate}
\end{definition}

\begin{theorem}[法則収束統計量の漸近表現定理]
    実験$\E_n=(P_{n,h}|h\in H)_{n\in\N}$は，$\mu$に支配された実験$\E=(P_h|h\in H)$に収束するとする．
    $\E_n$の統計量$T_n$が，任意の$h\in H$について法則収束するならば，$\E$のランダム化された統計量$T$が存在して，$\forall_{h\in H}\;T_n\xrightarrow{h}T$．
\end{theorem}

\subsection{漸近正規性}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $h\in H$にて局所漸近正規なモデルは，正規分布モデル$N(Jh,J)$へ収束する実験列を定める．
\end{tcolorbox}

\begin{theorem}[局所漸近正規ならば，正規実験に収束する]
    $\E_n=(P_{n,h}|h\in H)$を実験列で，パラメータ空間$H\subset\R^d$は$0\in H$を満たすとする．
    $h=0$下で$N(0,J)\;(J\in M_d(\R))$に収束する確率変数列$\Delta_n:\X^n\to\R^d$を用いて
    \[\log\frac{dP_{n,h}}{dP_{n,0}}=h^\top\Delta_n-\frac{1}{2}h^\top Jh+o_{P_{n,0}}(1)\]
    と展開できるとき，実験列$\E_n$は$(N(Jh,J))_{h\in H}$に収束する．
\end{theorem}

\subsection{一様分布}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    一様分布のモデル$(U([0,\theta]))_{\theta\in\R_+}$は$L^2$-微分可能ではなく，正規な実験に収束しない．
    実は，指数実験に収束する．
\end{tcolorbox}

\subsection{Pareto分布}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    Pareto分布の極限実験は，正規実験と指数実験の組み合わせとなる．
\end{tcolorbox}

\subsection{漸近混合正規性}

\section{尤度比確率場の局所漸近構造}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    統計推測の最適性の問題は，尤度解析に帰着される．
\end{tcolorbox}

\subsection{漸近決定理論}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    漸近決定理論の方法は，その普遍性ゆえに，確率過程の統計推測論などの新しい分野と合流し発展を続けている．
    統計量の漸近有効性を議論するためには，統計的実験の列に対する漸近決定理論の立場で一般論を展開する方が好ましい．
    これは，小標本理論における十分性・不偏性・不変性などの精緻な理論を，非線形な場合にも拡張する企てである．
\end{tcolorbox}

\begin{history}
    漸近論への数学的アプローチで，初めて理論と言えるものはWald 1943とLeCam1960, 1972, 1979である．
    そこで，統計的実験とは，多様体$\{P_\theta\}_{\theta\in\Theta}\subset P(\R^n)$とされた．
    いまでは無限次元の場合（セミ／ノンパラメトリック）も考えられている．
    実験を繰り返すにつれて，実験は局所的に簡単なモデルで近似可能になっていく，この現象をGaussian shiftという．
    よって，問題は元の実験の研究から，Gaussian shiftの近似へと移り変わる（極限定理）．
    これは本質的には実験の滑らかさに起因する．
\end{history}

\begin{definition}[experiment, deficiency]
    $\Theta\ne\emptyset$に関する実験とは，3つ組$E=(\Om,\A,\P)$をいう．
    パラメータ空間$\Theta$に関する実験全体の集合を$\E(\Theta)$で表す．
    $\delta(E,F):=\inf\Brace{\ep>0\mid E\overset{\ep}{\subset} F}\;(E,F\in\E(T))$に対して，$\Delta(E,F):=\max\Brace{\delta(E,F),\delta(F,E)}$を\textbf{欠損}という．
    これは$\E(T)$上に擬距離を定める．
    \begin{align*}
        \Delta(E,F)&=\sup\Brace{}
    \end{align*}
\end{definition}

\subsection{統計的実験}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    漸近正規性を定義する枠組みを整備する．
    Frechet微分のような考え方をするが，$L^2$ノルムにより条件が弱められている\cite{Ibragimov HasMinskii}．
\end{tcolorbox}

\begin{definition}
    可測空間とその上の確率分布族$(P_\theta)_{\theta\in\Theta}$の組$(\X,\A,\{P_\theta\}_{\theta\in\Theta})$を\textbf{統計的実験}という．
    大標本理論では，直積による列$(\X^n,\A^n,\{P_\theta^n\})_{n\in\N}$を考える．
\end{definition}

\begin{notation}[正則性の仮定]
    $\Theta\osub\R^p$とし，$P_\theta$の真値$P_{\theta_0}$に対するLebesgue分解における，絶対連続部分のRadon-Nikodym微分を$dP_\theta/dP_{\theta_0}$，特異部分を$\sigma_{\theta/\theta_0}$で表すと，
    \[P_\theta[A]=\int_A\frac{dP_\theta}{dP_{\theta_0}}dP_{\theta_0}+\sigma_{\theta/\theta_0}(A)\quad(A\in\A)\]
    関数$D_u:\X\to\R\;(u\in\R^p)$を
    \[D_u(x):=\sqrt{\frac{dP_{\theta_0+u}}{dP_{\theta_0}}(x)}-1\]
    で定める．
\end{notation}

\begin{enumerate}[({G}1)]
    \item 原点の近傍の$u\in\R^p$(十分小さな変位)に対して$D_u\in L^2(P_{\theta_0})$であって，$u\mapsto D_u$は$u=0$において$L^2(P_{\theta_0})$の意味で微分可能である：
    \[\exists_{\varphi=\varphi_{\theta_0}\in L^2(P_{\theta_0};\R^p)}\quad\int_\X(D_u(x)-u'\varphi(x))^2P_{\theta_0}(dx)=o(\abs{u}^2)\quad(\abs{u}\to0).\]
    \item $\sigma_{\theta_0+u/\theta_0}(\X)=o(\abs{u}^2)\;(\abs{u}\to0)$．
\end{enumerate}

\begin{lemma}[$L^2$-微分の表示]
    G1,G2を仮定し，$P_\theta$は確率密度関数族$p(x,\theta)$を持ち，対応$\theta\mapsto p(x,\theta)$は$\theta_0$において微分可能とする．
    このとき，
    \[\varphi(x)=\frac{1}{2p(x,\theta_0)}\pp{p}{\theta}(x,\theta_0).\]
    また，Fisher乗法行列に当たる作用素$\Theta\to\R$を
    \[I(\theta_0):=4\int_\X\varphi(x)\varphi(x)^\top P_{\theta_0}(dx)\]
    とおく．
\end{lemma}

\begin{lemma}
    
\end{lemma}

\subsection{局所漸近正規性}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    定理の$Z_n(u;\theta_0)$の展開が成り立つとき，すなわち，G1,G2が成り立つとき，統計的実験$(\X^n,\A^n,\{P^n_\theta\}_{\theta\in\Theta})$は$\theta_0$において\textbf{局所漸近正規}であるという．

    対応$\R^p\ni u\mapsto Z_n(u;\theta_0)\in C(\R^p)$を尤度比確率場という．
    Ibragimov HasMinskiiは，この関数の$C(\R^p)$上での弱収束を示した：$Z_n(-;\theta_0)\xrightarrow{d}Z(-;\theta_0)$．
\end{tcolorbox}

\begin{notation}[normalized likelihood ratio]
    正規化された尤度比を
    \[Z_n(u;\theta_0):=\prod^n_{j=1}\frac{dP_{\theta_0+n^{-1/2}u}}{dP_{\theta_0}}(x_j),\quad(u\in\R^p,x\in\X^n)\]
    とおくと，$Z_n$は十分大きな$n\in\N$に対して定義されている．
\end{notation}

\begin{theorem}[LeCam]
    正規な統計的実験の列$E_i$について，
    $I(\theta_0)\;(\theta_0\in\Theta)$は非退化とする．
    [G1],[G2]を仮定し$\varphi$を$D_u$の$L^2$-微分とする．
    このとき，ある$p$次元と$1$次元確率変数$\Delta_n(\theta_0),\rho_n(u,\theta_0)$が存在して，
    正規化された尤度比は次の表現を持つ：
    \[Z_n(u;\theta_0):=\exp\paren{u^\top\Delta_n(\theta_0)-\frac{1}{2}u^\top I(\theta_0)u+\rho_n(u,\theta_0)}.\]
    また，
    \[L\Square{\Delta_n(\theta_0)|P^n_{\theta_0}}\to N_p(0,I(\theta_0)),\quad\rho_n(u,\theta_0)\xrightarrow{P^n_{\theta_0}}0.\]
\end{theorem}
\begin{proof}
    \[\Delta_n(\theta_0):=\frac{2}{\sqrt{n}}\sum^n_{j=1}\varphi(x_j),\quad\rho_n(u,\theta):=\log Z_n(u;\theta_0)-u^\top\Delta_n(\theta_0)+\frac{1}{2}u^\top I(\theta_0)u.\]
    とすれば良い．
\end{proof}
\begin{remarks}
    この定理が保証する尤度比の性質は，推定量の性質を調べるに当たって強力な道具となる．
    特に，尤度比の対数$\log Z_n$は$N\paren{-\frac{1}{2}u^\top I(\theta_0)u,u^\top I(\theta_0)u}$に分布収束する．
\end{remarks}

\begin{definition}[LAN: Locally Asymptotically Normal (LeCam 1960)]
    定理の$Z_n(u;\theta_0)$の展開が成り立つとき，すなわち，G1,G2が成り立つとき，統計的実験$(\X^n,\A^n,\{P^n_\theta\}_{\theta\in\Theta})$は$\theta_0$において\textbf{局所漸近正規}であるという．
\end{definition}
\begin{remarks}
    LANのとき，$P^n_{\theta_0+n^{-1/2}u}$は$P^n_{\theta_0}$に接触している．
\end{remarks}

\subsection{局所漸近正規な例}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    統計モデルの列が正規モデルで近似可能であるという性質が，局所漸近正規性である．
    正則なパラメトリックモデルの独立同分布モデルは局所漸近正規で，これについてのLeCamの定理は中心極限定理に他ならない．
\end{tcolorbox}

\begin{theorem}[Hajekの十分条件]
    $\Theta\subset\R^1$で添字付けられた実験$E_i$の分布族$f(x,\theta)=\dd{P_\theta}{\nu}$が次の3条件を満たすならば，族$P^n_\theta$はLAN条件を$\theta=t$について満たす．
    \begin{enumerate}
        \item 任意の$x\in\X$について$f(x,-):\Theta\to[0,1]$は$\theta=t$の近傍$U\osub\Theta$で絶対連続である．
        \item $\nu\dae x\in\X$について，微分係数$\pp{f(x,\theta)}{\theta}$が$\theta\in U$上存在する．
        \item $I(\theta)\;(\theta=t)$は連続で半正定値である．
    \end{enumerate}
\end{theorem}

\subsection{漸近有効性}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    supによって，パラメータの1点でのみ良いパフォーマンスを示す病的な推定量（超有効推定量）が競争から排除されている．これが漸近ミニマックスリスク関数である．
\end{tcolorbox}

\begin{theorem}[Hajekの不等式]
    統計的実験は$\theta_0$にて局所漸近正規であるとする．$\theta$の任意の推定量系列$T_n$について，
    \[\forall_{\gamma,\delta>0}\quad\liminf_{n\to\infty}\sup_{\theta:\abs{\theta-\theta_0}<\delta}E_\theta\Square{\abs{\sqrt{n}(T_n-\theta)}^\gamma}\ge E[\abs{I(\theta_0)^{-1}\Delta(\theta_0)}^\gamma].\]
\end{theorem}

\section{漸近有効性}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    滑らかなパラメトリックモデルの漸近下界は$N(0,I^{-1}_\theta)$が与える．
\end{tcolorbox}

\begin{notation}
    $\sqrt{n}(T_n-\psi(\theta))$は全ての$\theta\in\Theta$について分布収束すると仮定して(この仮定を正則性という)，その効率性を比較する．
\end{notation}

\subsection{実験の下界}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    漸近有効性の議論は，正規な位置母数モデル(Gaussian shiftモデル)での有効推定量の問題に帰着する．
\end{tcolorbox}

\begin{definition}[regular]
    統計量の列$(T_n:\X^n\to\Theta)$が$\theta$において\textbf{正則}または\textbf{漸近的法則同等}であるとは，ある極限分布$L_\theta$が存在して，
    \[\forall_{h\in\Theta}\;\sqrt{n}\paren{T_n-\psi\paren{\theta+\frac{h}{\sqrt{n}}}}\xrightarrow[\theta+h/\sqrt{n}]{d}L_\theta\]
    が成り立つことをいう．
    極限分布$L_\theta$が$h$に依ることを許すとき，\textbf{前正則}であるとする．
    前正則な統計量が正則であるとは，局所一様に取れる消息を表す．
\end{definition}

\begin{theorem}\mbox{}
    \begin{enumerate}[({E}1)]
        \item 実験$(P_\theta)_{\theta\in\Theta}$は点$\theta\in\Theta\osub\R^k$で
        $\theta\mapsto dP^{1/2}$が
        $L^2$-微分可能で，可逆なFisher行列$I_\theta\in\GL_k(\R)$を持つとする．
        \item $\psi:\Theta\to\R$は$\theta$で微分可能であるとする．
        \item 実験$(P_{\theta+h/\sqrt{n}})_{h\in\R^k}$の統計量$T_n$は前正則であるとする．
    \end{enumerate}
    このとき，ランダム化された統計量$T$が実験$(N(h,I_\theta^{-1}))_{h\in\R^k}$上に存在して，$T-\dot{\psi}_\theta h\sim L_{\theta,h}$．
\end{theorem}

\subsection{Gaussモデルの平均の推定}

\subsection{畳み込み定理}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    正則な統計量の中で，最適なものは解析的に定まる．
\end{tcolorbox}

\begin{theorem}\mbox{}
    \begin{enumerate}[({E}1)]
        \item 実験$(P_\theta)_{\theta\in\Theta}$は点$\theta\in\Theta\osub\R^k$で
        $\theta\mapsto dP^{1/2}$が
        $L^2$-微分可能で，可逆なFisher行列$I_\theta\in\GL_k(\R)$を持つとする．
        \item $\psi:\Theta\to\R$は$\theta$で微分可能であるとする．
        \item 実験$(P^n_\theta)_{\theta\in\Theta}$の統計量$T_n$は正則で極限分布$L_\theta$を持つとする．
    \end{enumerate}
    このとき，ある確率測度$M_\theta\in P(\X)$が存在して，
    $L_\theta=N(0,\dot{\psi}_{\theta}I^{-1}_\theta\dot{\psi}_{\theta}^\top)*M_\theta$
    と表せる．
\end{theorem}

\subsection{局所漸近minimax定理}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    全く違うアプローチで，最適な統計量が正規分布$N(0,\dot{\psi}_{\theta}I^{-1}_\theta\dot{\psi}_{\theta}^\top)$に漸近的に従うことを示す方法を考える．
\end{tcolorbox}

\subsection{下界を達成する推定量}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    漸近分布$N(0,\dot{\psi}_{\theta}I^{-1}_\theta\dot{\psi}_{\theta}^\top)$を持つ推定量が最適とわかったから，このようなものの必要十分条件を探す．
    これは実は漸近線型性である．線形性が正規性に対応するのである！
\end{tcolorbox}

\begin{lemma}[漸近線型性:最適性の特徴づけ]\mbox{}
    \begin{enumerate}[({E}1)]
        \item 実験$(P_\theta)_{\theta\in\Theta}$は点$\theta\in\Theta\osub\R^k$で
        $\theta\mapsto dP^{1/2}$が
        $L^2$-微分可能で，可逆なFisher行列$I_\theta\in\GL_k(\R)$を持つとする．
        \item $\psi:\Theta\to\R$は$\theta$で微分可能であるとする．
    \end{enumerate}
    このとき，実験$(P^n_\theta)_{\theta\in\R^k}$の統計量列$(T_n)$について，次の2条件は同値．
    \begin{enumerate}
        \item 漸近線型である：$\sqrt{n}(T_n-\psi(\theta))=\frac{1}{\sqrt{n}}\sum^n_{i=1}\dot{\psi}_\theta I_\theta^{-1}\dot{l}_\theta^\top(X_i)+o_P(1)$．
        \item $T_n$は$\psi(\theta)$に対する正則な統計量の中で最適である：$\forall_{h\in\Theta}\;\sqrt{n}\paren{T_n-\psi\paren{\theta+\frac{h}{\sqrt{n}}}}\xrightarrow[\theta+h/\sqrt{n}]{d}N(0,\dot{\psi}_{\theta}I^{-1}_\theta\dot{\psi}_{\theta}^\top)$．
    \end{enumerate}
\end{lemma}
\begin{remarks}
    $\Delta_{n,\theta}:=\frac{1}{\sqrt{n}}\sum\dot{l}_\theta(X_i)$の部分が$N(0,I_\theta)$に収束する．
\end{remarks}

\begin{example}
    適切な正則条件の下で，最尤推定量は漸近線型である．
    したがってデルタ法によって，任意の汎関数$\psi(\theta)$に対して，plug-in推定量は漸近線型である．
\end{example}

\section{射影}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    統計量の漸近分布を求める方法の1つに，既に漸近分布が知れている確率変数と漸近同等であることを示す方法がある．
    このとき，Slutskyの補題の簡単な系から，極限法則が一致することが従う．

    すると，何で近似するかという問題が生じるが，射影は$L^2$-近似であることを考えると，この言葉を使うのが良かろう．
\end{tcolorbox}

\subsection{Hajekの射影}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    Hajekの射影は，ある確率変数$X_1,\cdots,X_n$と，それぞれの一変数関数$g_i$との合成$g_1(X_1),\cdots,g_n(X_n)$の形で$T$を最もよく近似する元である．
\end{tcolorbox}

\begin{notation}
    \[S:=\Brace{\sum^n_{i=1}g_i(X_i)\in L^2(\Om)\;\middle|\;\forall_{i\in[n]}\;g_i(X_i)\in L^2(\Om)}\]
    とおく．
\end{notation}

\begin{lemma}[Hajek projection]
    $X_1,\cdots,X_n$は独立とする．
    このとき，任意の$T\in L^2(\Om)$の$S$への射影は
    \[\pr_S(T)=\sum_{i=1}^nE[T|X_i]-(n-1)E[T]\]
    と表せる．
\end{lemma}

\begin{remarks}
    特に$X_1,\cdots,X_n$が同分布に従うとし，$T=T(X_1,\cdots,X_n):\Om\to\X^n\to\R$を対称な可測関数とする．
    このとき，条件付き期待値は
    \[E[T|X_i=x]=E[T(x,X_2,\cdots,X_n)]\]
    と簡略化されるから，
    \[S':=\Brace{\sum^n_{i=1}g(X_i)\in\L(\Om)\;\middle|\;g\in\L(\X)}\]
    上への射影と一致する：$\pr_S=\pr_{S'}$．
\end{remarks}

\subsection{Hoeffdingの分解}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    一変数関数$g_i:\X\to\R$を用いるのではなく，より変数を多くして，$X_1,\cdots,X_n$の間の複雑な依存関係も表現できるようにすることで，近似の精度を上げることを考える．
    これは，初めは$(H_{i})_{i\in[n]}$の線型結合で表せる元の空間だったものを，どんどん互いに直交する部分空間を追加することで大きくしていく算譜である．
\end{tcolorbox}

\begin{notation}
    任意の$A\in P([n])$に対して，
    \[H_A:=\Brace{g_A(X_i:i\in A)\in L^2(\Om)\mid g_A\in\L(\X^{\abs{A}}),\;\forall_{B\in P([n])}\;\abs{B}<\abs{A}\Rightarrow E[g_A(X_i:i\in A)|X_j:j\in B]=0}\]
    と定める．
\end{notation}

\begin{lemma}
    各$\{H_A\}_{A\in P(n)}\subset L^2(\Om)$は組ごとに直交している．
\end{lemma}

\begin{theorem}
    $X_1,\cdots,X_n$を独立な確率変数，$T\in L^2(\Om)$とする．
    \begin{enumerate}
        \item $\pr_{H_A}(T)=\sum_{B\in P(A)}(-1)^{\abs{A}-\abs{B}}E[T|X_i:i\in B]$．
        \item $\forall_{B\in P(A)}\;T\perp H_B$ならば，$E[T|X_i:i\in A]=0$．
        \item $\bigoplus_{B\in P(A)}H_B$は，$(X_i)_{i\in A}$の関数で2乗可積分であるものの全体を含む部分空間となる．
    \end{enumerate}
\end{theorem}

\begin{remarks}
    特に$X_1,\cdots,X_n$が同分布に従うとし，$T=T(X_1,\cdots,X_n):\Om\to\X^n\to\R$を対称な可測関数とする．
    このとき，Hoeffding分解は
    \[T=\sum^n_{r=0}\sum_{\abs{A}=r}g_r(X_i:i\in A),\qquad g_r(x_1,\cdots,x_r)=\sum_{B\in P[r]}(-1)^{r-\abs{B}}E[T(x_i\in B,X_i\notin B)]\]
    となり，各項$\sum_{\abs{A}=r}g_r(X_i:i\in A)$は退化した核を持った$r$次の$U$-統計量で，全ての項は互いに直交だから，分散は容易に計算できる：
    \[\Var[T]=\sum_{r=1}^n \begin{pmatrix}n\\r\end{pmatrix}E[g^2(X_1,\cdots,X_r)].\]
\end{remarks}



\section{$U$-統計量}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $U$-統計量の理論はHoeffding 1948による．
    $U$-統計量は最小分散不偏推定量の計算中に自然に現れる．

    $r=1$のとき，核$h$を用いた標本平均の一般化となっている．
    すなわち，$U_{1,n}=\bP h(X)$．
    これをさらに$r>1$とすると，
    基本的にはresampling法の一種で，標本から大きさ$r$の再抽出をして平均を取る．
    これは標本分割に繋がるのではないか？
\end{tcolorbox}

\begin{notation}
    $I_n=[n],(\X,\A)$を可測空間，$X_j\;(j\in I_n)$を独立同分布に従う$\X$-値確率変数，$h:\X^r\to\R$を対称な可測関数とする．
    部分集合$A:=\{i_1,\cdots,i_r\}\subset I_n$に対して，$X_A=(X_{i_1},\cdots,X_{i_r})$と略記すると，対称性から$h(X_A):=h(X_{i_1},\cdots,X_{i_r})$と表して良い．
    すなわち，関数$h(X_{-},\cdots,X_{-})$は$I_n^r\to\R$として定めたが，本質的には$[I_n]^r\to\R$である．
\end{notation}

\subsection{対称な関数}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    対称な関数の取り扱いと表示は仰々しいが，要は対称関数$h:\X^r\to\R$は集合$[\X]^r$上の関数$f:[\X]^r\to\R$と一対一対応する．
    そして$[\X]^r$なる対象は，$\X$の点から$r$個の抽出として自然に出現する．
\end{tcolorbox}

\begin{definition}
    $E[\abs{h(X_A)}]<\infty$とする．部分集合$B\subset A$に対して，
    \[h(X_A)_B:=\sum_{C:C\subset B}(-1)^{\abs{B}-\abs{C}}E[h(X_A)|X_C]\]
    とする．$E[h(X_A)|X_{\emptyset}]:=E[h(X_A)]$とした．
\end{definition}

\begin{lemma}[反転公式]
    $h(X_A)=\sum_{B:B\subset A}h(X_A)_B$．
\end{lemma}

\subsection{定義と例}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    標本$n$個のうち$r$個を選んで，その$r$個の部分集合$A,\abs{A}=r$に対応する値$h(X_A)$の標本平均として得られる統計量のクラスを，$U$-統計量という．

    $\theta:=E[h(X_1,\cdots,X_r)]$の推定を考えると，当然$h$自身は$\theta$の不偏推定量であるが，$h$を核とする$U$-統計量$U_{r,n}$は再び不偏推定量で，より小さい漸近分散を持つ(また漸近正規である)．
    実際，$U_{r,n}$は$h$の射影(条件付き期待値)となっており，射影はノルム減少的であるから分散を減らすのである．
\end{tcolorbox}

\begin{definition}[U-statistic with kernel $h$]
    対称関数$h:\X^r\to\R$を\textbf{核}とする$r$次の\textbf{$U$-統計量}とは，
    \[U_{r,n}:=\begin{pmatrix}n\\r\end{pmatrix}^{-1}\sum_{A\in[I_n]^r}h(X_A)=\frac{1}{\begin{pmatrix}n\\r\end{pmatrix}}\sum_{\sigma:[r]\mono[n]}h(X_{\sigma(1)},\cdots,X_{\sigma(r)})\]
    をいう．
\end{definition}
\begin{remarks}[標本平均の拡張]
    単に1つ1つの観測点$X_i$に等しく荷重平均を取る$\bP$を一般化して，

    標本$n$個からの$r$個のすべての選び方についての測度$\o{\bP}\subset P([I_n]^r)$
    について，ある対応$h:[\X]^r\to\R$に関しての値の平均$U_{r,n}=\o{\bP}h(X,\cdots,X)$をいう．
\end{remarks}

\begin{definition}
    $U$-統計量$U_{r,n}$が\textbf{退化次元$i-1$を持つ}とは，$2\le i\le r$に関して，
    \[E[h(X_1,\cdots,X_r)\mid X_1,\cdots,X_{i-1}]=0,\quad\land\quad E[h(X_1,\cdots,X_r)|X_1,\cdots,X_i]\ne0\]
    を満たすことをいう．また，$E[h(X_1,\cdots,X_r)|X_1]\ne0$であるとき，\textbf{非退化}であるという．
\end{definition}

\begin{example}[rank correlation coefficient]
    明らかに標本平均，不偏分散は，それぞれ$f=\id_\X,f(x_1,x_2)=\frac{(x_1-x_2)^2}{2}$に関する$U$-統計量である．
    歪度も$k_{3,n}(x)=\sum\frac{(x_i-\o{x}_n)^3n}{(n-1)(n-2)}$が定める$U$-統計量で，とくに3次の$k$-統計量である．

    また，2次元の無作為標本$(X_j,Y_j)\;(j=1,\cdots,n)$に関して，順位の間の相関係数として
    \[\tau:=\frac{4}{n(n-1)}\sum_{i<j}1_{\Brace{(X_i-X_j)(Y_i-Y_j)>0}}-1\]
    を\textbf{Kendallの順位相関係数}という．
    また，
    \begin{align*}
        K&:=\Abs{\Brace{\{i,j\}\in[[n]]^2\mid (x_i>x_j\land y_i>y_j)\lor(x_i<x_j\land y_i<y_j)}},\\L&:=\Abs{\Brace{\{i,j\}\in[[n]]^2\mid\lnot(x_i>x_j\land y_i>y_j)\land\lnot(x_i<x_j\land y_i<y_j)}}
    \end{align*}
    として$\tau=\begin{pmatrix}n\\2\end{pmatrix}^{-1}(K-L)$とも表せる．
\end{example}

\begin{example}[K-statistic]
    キュムラントの最小分散不偏推定量をFisherの$K$-統計量と言い，John Tukeyはこれから一般化$K$-統計量(polykay)を導いたが，
    これは斉次多項式が定める$U$-統計量に他ならない．
\end{example}

\begin{example}[V-statistic]
    対称な関数$h$に関して，
    \[V_n:=\frac{1}{n^r}\sum_{(i_1,\cdots,i_r)\in I_n^r}h(X_{i_1},\cdots,X_{i_r})\]
    を$V$-統計量という．$U$-統計量は大きさ$r$の部分標本の抽出について平均を取ったが，$V$-統計量は大きさ$r$の部分標本の，抽出の順序も区別し，また重複も許す．$[I_n]^r$と$I_n^r$の違いである．
    $V$-統計量はHoeffdingの一年前である1947年にRichard von Misesが導入した．\footnote{von Mises, R. (1947). "On the asymptotic distribution of differentiable statistical functions". Annals of Mathematical Statistics. 18 (2): 309–348. doi:10.1214/aoms/1177730385. JSTOR 2235734.}
    $v_1>0$のとき，$U$-統計量と漸近同等である．

    $h(x,y)=\frac{(x-y)^2}{2}$が定める2次の$V$-統計量は
    \[V_{2,n}=\frac{1}{n}\sum^n_{i=1}(x_i-\o{x})^2\]
    で，これは分散の最尤推定量である．しかし同じ核に対する$U$-統計量は
    \[U_{2,n}=s=\frac{1}{n-1}\sum^n_{i=1}(x_i-\o{x})^2\]
    となる．漸近同等ではあるが，不偏推定量となるのは後者である！
\end{example}

\subsection{漸近正規性}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    構成の仕方から明らかに，大数の法則から$U_n$は母数$\theta:=E[h(X_1,\cdots,X_r)]$の不偏推定量となっている．
    （また，これが逆マルチンゲールになっていることからも従う）．
    そこで，$\sqrt{n}(U_n-\theta)$の漸近分布を考える．

    そのための道具がHoeffdingの分解で，$U$-統計量は，より退化度の高い$U$-統計量の和として表せる．
    これは確率変数の分解に似ている．
\end{tcolorbox}

\begin{notation}
    $E[\abs{h(X_1,\cdots,X_r)}]<\infty$とする．
    \[h_k(x_1,\cdots,x_k):=E[h(x_1,\cdots,x_k,X_{k+1},\cdots,X_r)]\quad l\le r\]
    とし，$h_0:=E[h(X_1,\cdots,X_r)]$とする．
\end{notation}

\begin{lemma}
    $\forall_{C\subset A}\;E[h(X_A)|X_C]=h_{\abs{C}}(X_C)$．
    ただし，$h_0(X_{\emptyset})=h_0$とする．
\end{lemma}

\begin{definition}
    $\wh{h_k}:=h_k-\theta\;(k=0,1,\cdots,r)$に関して
    \[g_k(x_{[k]}):=\sum_{C:C\subset[k]}(-1)^{k-\abs{C}}\wh{h}_{\abs{C}}(x_C)\]
    と定める．このとき，$\wh{h}_r(X_A)_B=g_{\abs{B}}(X_B)$．
\end{definition}

\begin{lemma}[Hoeffdingの分解]
    \[U_n-\theta=\sum^r_{k=1}\begin{pmatrix}n\\k\end{pmatrix}^{-1}\begin{pmatrix}r\\k\end{pmatrix}S_k(n)\]
    ただし，$S_k(n):=\sum_{B:B\subset I_n,\abs{B}=k}g_k(X_B)$とした．
\end{lemma}

\begin{lemma}[漸近分布の平均]
    $h\in L^1(\X^r)$とする．$B\subset I_n,\abs{B}=k>0,b\in B$について，
    \[E[g_k(X_B)|X_{I_n\setminus\{b\}}]=0.\]
\end{lemma}

\begin{lemma}[漸近分布の分散]
    $h\in L^2(\X^r)$とし，$v_k:=E[g_k(X_1,\cdots,X_k)^2]$とおく．
    $B,C\subset I_n$について，
    \[E[g_{\abs{B}}(X_B)g_{\abs{C}}(X_C)]=\delta_{B,C}v_{\abs{B}}.\]
\end{lemma}

\begin{theorem}[中心極限定理]
    $h\in L^2(\X^r),v_1>0$とする．このとき，
    \[\sqrt{n}(U_n-\theta)\xrightarrow{d}N(0,r^2v_1).\quad(n\to\infty)\]
\end{theorem}

\begin{tbox}{red}{}
    こうして，統計量を汎関数$\P\to\R$と捉え，この漸近分布を考える（あわよくば中心極限定理を目指す）発想が出てくる．
    これがHoeffdingがvon Misesから受け継いだものである．

    そう，標本平均とは$U$-統計量の一例に過ぎないのであった．
    そして独立確率変数列の和からマルチンゲールの概念が生まれたのならば，$U$-統計量がマルチンゲールになるのは当然である．

    標本平均も，これに核$h$を重ねたものも，線型汎関数であることは変わらないから，漸近分布は正規である．
\end{tbox}

\subsection{2標本$U$-統計量}

\subsection{退化$U$-統計量}

\begin{definition}[degenerated]
    $U$-統計量の列$(U_n)$(の核$(h_n)$)が\textbf{退化している}とは，漸近分散が$r^2v_1=0$であることをいう．
\end{definition}

\section{情報量規準}

\subsection{枠組みとKL距離}

\begin{notation}
    $\sigma$-有限測度$\nu$に支配されている確率分布族$\P=\{P_\theta\}_{\theta\in\Theta}\subset P(\X,\A)$を考える．
    真の分布$Q$の確率密度関数も存在して$q(x):=dQ/d\nu(x)$と表され，$\P$に属しているかは不明とする．
    $n$個の無作為標本$x=(x_1,\cdots,x_n)$に基づく未知パラメータ$\theta$の推定量を$\wh{\theta}_n(x)$とする．
    新たな観測に対する予測分布として，plug-in分布$P_{\wh{\theta}_n}$で$Q$を近似するのは自然である．
\end{notation}

\begin{definition}[statistical manifold]
    $\sigma$-有限測度$\nu$に支配されている確率分布族$\P=\{P_\theta\}_{\theta\in\Theta}$が\textbf{正則統計モデル}であるとは，
    \begin{enumerate}
        \item パラメータ空間$\Theta$はあるEuclid空間$\R^p$の開集合と同相．
        \item $\theta\mapsto P_\theta$は微分可能．
        \item 非特異なFisher情報行列を持つ．
    \end{enumerate}
    このとき，$\P$は$p$-次元多様体をなす．
\end{definition}

\begin{definition}[divergence]
    $(M,(\xi_x))$を多様体とする．関数$D:M\times M\to\R_+$が\textbf{分離度}であるとは，
    \begin{enumerate}
        \item 非退化性：$D[P,Q]=0\Leftrightarrow P=Q$．
        \item 十分近い2点$\xi,\xi+d\xi$について，$D[\xi,\xi+d\xi]=\frac{1}{2}g_{ij}(\xi)d\xi_id\xi_j$が定める行列$G(\xi)=(g_{ij}(\xi))$は正定値対称である．
        \item 点$P\in M$の$r$-近傍$N(P,r):=\Brace{Q\in M\mid D[P,Q]<r}$は$r$について単調に増大する．
    \end{enumerate}
\end{definition}
\begin{remarks}
    対称性も三角不等式も成り立たず，距離の概念とは違い，むしろ距離の二乗のような，分散のような概念である．
    そして，微小な2点間の「Riemann距離」を与えるが，非対称性は受け継がれ，これが双対性という方向を持った新たな構造を授ける．
\end{remarks}

\begin{definition}[Kullback-Leibler information / divergence / relative entropy]
    $P,Q\in\P$について，
    \[I[Q,P]:=\int_\X q(x)\log\frac{q(x)}{p(x)}\nu(dx)\]
    によって定まる関数$I:\P\times\P\to\R_+$を\textbf{Kullback-Leibler情報量}という．
\end{definition}

\begin{example}[正規分布族のKL分離度]
    $M_n(\R)$の中の半正定値行列のなす部分多様体$\GL_n(\R)_+$は$n(n+1)/2$次元である．
    この多様体に
    \[D[P,Q]=\Tr(PQ^{-1})-\log\abs{PQ^{-1}}-n\]
    なるダイバージェンスが定まる．これは，$P,Q$を分散共分散行列とする平均$0$の正規分布の間のKLダイバージェンスになっている．
\end{example}

\subsection{バイアス補正}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    真の分布と，そのモデルがはじき出す予測分布との分離度を，そのモデルのmisspecificationの度合いを測る尺度に用いるという発想である．
    すると，「\textbf{モデル$f$の平均対数尤度が大きいほど良いモデル}」ということになる．
    しかし，分離度を経験分布から求めても，そのバイアスの推定の仕事が残っている．
    これはバイアスの漸近値(の一致推定量plug-in)を用いるということである．
    \textbf{このバイアスは，同じデータ$X$を，パラメータの推定と，推定されたモデルの平均対数尤度の推定とに二度用いたことによって生じる}．
    こうして得られる種々の分離度の推定量を，情報量規準という．
\end{tcolorbox}

\begin{discussion}[KL情報量の最小化と平均対数尤度の最大化は同値]
    真の分布$Q$と予測分布$P_{\wh{\theta}_n}$の分離度は
    \[I[Q,P_{\wh{\theta}_n}]=\int_\X q(z)\log q(z)\nu(dz)-\int_\X q(z)\log p(z,\wh{\theta}_n)\nu(dz).\]
    ここで，モデルの選択に依るのは第二項の\textbf{平均対数尤度}
    \[h(Q,p,\wh{\theta}_n(x)):=\int_\X\log p(z,\wh{\theta}_n(x))Q(dz)\]
    である．
\end{discussion}
\begin{remarks}
    $Q$の経験分布$P_n$について，$h(P_n,p,\wh{\theta}_n(x))$で推定することになるが，これが最大になるときの$\wh{\theta}_n$を最尤推定量というのであった．
    これが，情報理論的な観点から再発見される最尤法の意味の一つである．
\end{remarks}

\begin{discussion}[バイアスの推定]
    第二項$h$を経験分布から求めることになるが，そのときのバイアスはどうなるか．
    \[b_n:=\int_{\X^n}\Square{h(P_n,p,\wh{\theta}_n(x))-h(Q,p,\wh{\theta}_n(x))}Q^n(dx).\]
    このとき，次が成り立つ．
\end{discussion}

\begin{theorem}
    ある$\theta_*\in\Theta$と$\varphi_Q\in L^2(\X,Q;\R^k)$が存在して，
    $\int_\X\varphi_Q(x)Q(dx)=0$かつ
    \[R:=\sqrt{n}(\wh{\theta}_n-\theta_*)-\zeta_n\xrightarrow{Q_n}0,\quad\zeta_n:=n^{-1/2}\sum_{j=1}^n\varphi_Q(x_j)\]
    が成り立つと仮定する．\footnote{最小コントラスト推定量はこの仮定を満たす．}

    スコアを$s(x,\theta)=\partial_\theta\log p(x,\theta)$，
    $C:=\Cov_Q[s(-,\theta_*)^\top,\varphi_Q]$とすると，
    $nb_n\xrightarrow{n\to\infty}\Tr C$．
\end{theorem}

\begin{definition}[information criterion]
    尤度関数にペナルティ項$p$を付けた
    \[IC=-2\Square{\sum^n_{j=1}\log p(x_j,\wh{\theta}_n(x))-p(n)}\]
    を，推定量$\wh{\theta}_n$に対する\textbf{情報量規準}という．
\end{definition}

\begin{example}\mbox{}
    \begin{enumerate}
        \item バイアス修正を考えるならば，$p(n)=\Tr C$と取るとよいが，これは$Q$に依存するので，その一致推定量を取るとよい．
        \item 推定量$\wh{\theta}_n$が$\Psi$が定める最小コントラスト推定量であるときの形は，小西貞則-北川源四郎による\textbf{一般化情報量規準}(GIC: Generalized Information Criterion)の特殊な場合である．
        これは一般の統計的汎関数$T:\P\to\R^m$に関する漸近バイアスの研究による．
        特に最尤推定量であるとき，補正項は$p(n)=\Tr(J_Q^{-1}I_Q)$となり(Huber 1976, 竹内1976)，この一致推定量で置き換えた情報量規準を\textbf{竹内の情報量規準}(TIC)という．
        $I$はFisher情報量行列で，$J$はHesse行列．
        \item $\wh{\theta}_n$が最尤推定量である上で特にモデルが真の分布$Q$を含むとき，
        $I=J$が成り立ち，漸近バイアスはパラメータベクトル$\theta\in\Theta$の次元となる．
        すなわち，パラメータ数を$k$として$p(n)=k$となる．この場合を\textbf{赤池情報量規準}(AIC)という．
    \end{enumerate}
\end{example}

\begin{history}[「バイアス補正法」として拡張の歴史を辿った]
    モデルの評価基準を「その予測能力としよう」として初めて提案したのは赤池(1973,74)．
    $-2$の係数はこのときの名残である．
    モデルが真の分布を含まない場合にも拡張したのが竹内啓(76)である．
    小西・北川(96)は，バイアス補正の方法を一般の統計量に拡張した．
    石黒(97)はブートストラップ法によってバイアス補正を行うことを提案し，拡張情報量規準EIC(Extended IC)と呼ばれる．
\end{history}

\begin{remark}
    AICの発明のときから取っている基本的な考え方として次の3点がある．
    \begin{enumerate}
        \item モデルの良さはその予測能力でみる．
        \item 予測は分布に対して行う．
        \item 分布の近さは(二乗誤差などではなく)KL情報量で測る．
    \end{enumerate}
\end{remark}

\begin{remarks}
    本懐はバイアスの補正であるとするなら，ロバスト統計と考えていることの方向性は同じではないか？
\end{remarks}

\section{密度推定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    ここからはノンパラメトリック解析の手法を概観する．
\end{tcolorbox}

\begin{notation}
    $(X_j)$を密度関数$f$の定める独立同分布に従う実確率変数列とし，ここから$f$の推定を考える．
\end{notation}

\subsection{カーネル密度推定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    枠組みとしては非常に自然であり，核$K$の選択はたしかに正規性の仮定が自然だろう．
\end{tcolorbox}

\begin{definition}
    積分核の中でも統計分野で\textbf{カーネル}というと，次の2条件を満たすものをいう：
    \begin{enumerate}
        \item $\int_\R K(u)du=1$．
        \item 偶関数：$K(-u)=K(u)\;\as$．
    \end{enumerate}
    なお，時系列解析では\textbf{窓関数}という．
\end{definition}

\begin{definition}[KDE: Kernel density estimator, Parzen-Rosenblatt window]
    ある可測関数$K:\R\to\R$とバンド幅$h>0$について，$K_h(x):=\frac{1}{h}K(x/h)$として平滑化し，
    \[\wh{f}(x):=\frac{1}{nh}\sum^n_{j=1}K\paren{\frac{x-X_j}{h}}=\frac{1}{n}\sum^n_{j=1}K_h(x-x_i)\]
    として定まる推定量$\wh{f}$を\textbf{核型推定量}または\textbf{パルツェン窓}という．
\end{definition}
\begin{example}
    標準正規分布の確率密度関数であるガウス関数
    \[K(x)=\frac{1}{2\pi}e^{-x^2/2}\]
    をカーネル関数として採用することが多い．
\end{example}
\begin{remarks}
    バンド幅は平滑化のためのパラメータである．
    いずれにしろ可測関数$K_h:\R\to\R$を用いて，$x-x_j$の重み$K_h(x)$付き標本平均を$f(x)$の推定値とするのである．

    これは，$K_h(x)$なる波形を各観測点$x_j$において合成波を取ったものと見れる．実際，熱核を各$x_j$において熱の総量を求める操作に等しい．
    なお，ヒストグラムもカーネル推定の例と見れる．
    カーネル推定は合成波の手法で，基本波形を求めようとする逆問題と思える．
\end{remarks}

\subsection{誤差評価}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    誤差の評価法は様々だが，ここではMISEを規準とする．
    これは$L_2$危険関数とも呼ばれる．
\end{tcolorbox}

\begin{definition}[mean integrated square error]
    $\MISE(\wh{f}):=\int_\R E[(\wh{f}(x)-f(x))^2]dx$を\textbf{積分2乗誤差}という．
\end{definition}

\begin{definition}[super kernel]
    関数$K:\R\to\R$が$s\in\N_{\ge2}$について次の2条件を満たすとき，\textbf{クラス$s$の核}であるという：
    \begin{enumerate}
        \item $x^sK(x)\in L^1(\R)$すなわち$\int_\R\abs{x}^s\abs{K(x)}dx<\infty$．
        \item $\forall_{1\le i<s}\;\int_\R x^iK(x)dx=0$．
    \end{enumerate}
    $\forall_{s\in\N_{\ge2}}\;\int_\R x^sK(x)dx=0$を満たすとき，\textbf{超核}であるという．
\end{definition}

\begin{theorem}
    $f\in C^s(\R)$かつ$f^{(s)}\in L^2(\R)$とする．
    クラス$s$の核$K\in L^2(\R)$について，ある定数$C\in\R$が存在して，
    \[\forall_{n\in\N}\;\forall_{h>0}\quad\MISE(\wh{f})\le C\paren{\frac{1}{nh}+h^{2s}}.\]
\end{theorem}
\begin{remarks}
    誤差を最小にする$h$のレートは$n^{-1/(2s+1)}$で，そのときの積分2乗誤差のオーダーは$O(n^{-2s/(2s+1)})$となる．
    これは必ずパラメトリックモデルの$n^{-1}$よりも遅くなる．
\end{remarks}



\chapter{漸近展開とその応用}

\begin{quotation}
    中心極限定理は，正規近似に基づく，分布の一次の漸近理論であった．
    Edgeworthによる展開はその近似をより精密にしたものである．
    標本数がそれほど大きくない実際的な状況では，分布のより精密な近似を基礎として統計量を構成することが必要になる．

    今日，漸近展開法も確率過程にまでその領域を広げ，新しい確率統計学が発展しつつあるが，このような新領域を理解する上でも，独立確率変数列における現象を理解することが重要である．

    キュムラント関数をよく使う．その関係で，テンソルがよく出る．
    また，最尤推定量の漸近展開に出てくる係数の一部は接続係数にほかならない．
\end{quotation}

\section{漸近展開}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    平均$0$，分散共分散行列$\Sigma>O$の$d$次元確率変数列$\{Z_j\}_{j\in\N}$は独立同分布に従うとする．
    中心極限定理によると，
    \[S_n=\frac{1}{\sqrt{n}}\sum^n_{j=1}Z_j\xrightarrow{d}N_d(0,\Sigma)\quad(n\to\infty)\]
    であるが，この正規近似はあまり精度は良くないため，標本数$n$が小さい時にはより良い近似が必要になる．

    数学的には，これは$X$の特性関数$\varphi_X(u)=E[e^{iu\cdot X}]\;(u\in\R^d)$の漸近展開理論に等価である．
\end{tcolorbox}

\begin{definition}
    確率変数$X:\X\to\R^d$に対して，
    \[\chi_r(u;X):=i^r\kappa_r[u\cdot X]=(\partial_\ep)^r_0\log\varphi_{u\cdot X}(\ep)\]
    を\textbf{キュムラント関数}という．ただし，$(\partial_\ep)^r_0$とは，$\ep$で$r$回偏微分をして$\ep=0$を代入したものをいう．
    これはキュムラント母関数の$r$次項係数（を$u$の関数と観たもの）である．
\end{definition}

\section{平滑化補題}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    分布の差と特性関数の差の関係を与える補題を準備する．
\end{tcolorbox}

\section{特性関数の展開}

\section{漸近展開の正当性の証明}

\section{漸近展開の変換}

\section{最尤推定量の漸近展開}

\section{漸近展開と情報幾何}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    
\end{tcolorbox}

\section{ブートストラップ法}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    ブートストラップ法では，データから作った分布$\wh{P}_n$から，さらに標本を取る．そこから，リサンプリング法とも言われる．
    パラメトリックモデルではプラグイン推定量$P_{\wh{\theta}_n}$，ノンパラメトリックモデルでは経験分布などを取る．
\end{tcolorbox}

\begin{notation}
    $(X_j)$を確率分布$P$に従う独立な実確率変数列とし，$R_n(\mathbf{X}_n,P):\X\to\R$を確率変数とする．
    $R_n(\mathbf{X}_n,P)$の分布関数を
    \[H_n(x;P):=\int_{\R^n}1_{R_n(\mathbf{x},P)\le x}P_n(d\mathbf{x})\]
    と定め，$H_n(c_\al;P)=\al\in(0,1)$を満たす点$c_\al$を求める問題を考える．
\end{notation}
\begin{example}
    $P$が未知で，$\theta=\theta(P)$の信頼区間を構成したいとき，推定量$\wt{\theta_n}(\mathbf{X}_n)$から定まる確率変数$R_n(\mathbf{X}_n,P):=\sqrt{n}(\wt{\theta_n}(\mathbf{X}_n)-\theta(P))$の分位点$c_\al$を求めたい．
\end{example}

\begin{definition}
    $\wh{P}_n$を$P$の推定量とする．
    \begin{enumerate}
        \item 真の分布関数$H_n(x;P)$に対する近似$H_n(x;\wh{P}_n):=\int_{\R^n}1_{R_n(\mathbf{x},\wh{P}_n)\le x}(\wh{P}_n)^n(d\mathbf{x})$を，\textbf{ブートストラップ分布}という．
        \item 推定分布$\wh{P}_n$に従う大きさ$n$の無作為標本$\bX^*=(X^*_1,\cdots,X^*_n)$を\textbf{ブートストラップ標本}という．
    \end{enumerate}
\end{definition}
\begin{remark}[computer-intensive]
    推定分布$\wt{P}_n$が複雑であるとき，
    積分$H_n(x;\wh{P}_n)$はモンテカルロ法で近似する．
    これが，ブートストラップ法が計算機集約的と言われる所以である．
\end{remark}
\begin{remark}[studentization]
    $R_n(\bX_n,P)$をステューデント化された確率変数で置き換えることで，ブートストラップ分布の近似精度の改善が可能である．
\end{remark}

\begin{thebibliography}{99}
    \bibitem{吉田}
    吉田朋広『数理統計学』(朝倉書店，2006)
    \bibitem{竹村}
    竹村彰道『現代数理統計学』（学術図書，2020）
    \bibitem{久保川}
    久保川達也『現代数理統計学の基礎』（共立出版，2017）
    \bibitem{西山陽一}
    西山陽一『マルチンゲール理論による統計解析』（近代科学社，2011）
    \bibitem{Bhattacharya}
    Rabi Bhattacharya - Course in Mathematical Statistics and Large Sample Theory
    \bibitem{Ibragimov HasMinskii}
    Ibragimov and Has'minskii - Statistical Estimation
    \bibitem{van der Vaart}
    van der Vaart - Asymptotic Statistics
    \bibitem{Helmut Strasser}
    Helmut Strasser "Mathematical Theory of Statistics"
    \bibitem{Hampel}
    Hampel - Robust Statistics - 2005
    \bibitem{Huber}
    Huber and Ronchetti - Robust Statistics - 2009
    \bibitem{稲垣宣生}
    稲垣宣生『数理統計学』
    \bibitem{Lehmann}
    E. L. Lehmann - Nonparametrics: Statistical Methods Based on Randks
\end{thebibliography}

\end{document}