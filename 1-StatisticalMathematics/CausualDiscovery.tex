\documentclass[uplatex,dvipdfmx]{jsreport}
\title{統計的因果推論}
\author{司馬博文}
\date{\today}
\pagestyle{headings} \setcounter{secnumdepth}{4}
\input{/home/hirofumi/StatisticalNature/preamble_no_fonts.tex}
%\input{/Users/hirofumi.shiba48/StatisticalNature/preamble_no_fonts.tex}
\usepackage[math]{anttor}
\begin{document}
\tableofcontents

\chapter{因果推論}

\begin{quotation}
    統計的因果推論の研究は次の２つに分類できる．
    \begin{enumerate}
        \item 因果グラフを所与として，因果関係を定式化する実証分野．Rubinによる因果モデルRCMとPearlによる構造的因果モデルSCMとがある．
        \item 因果グラフを未知として，因果グラフの構成算譜を定式化する理論分野．これを\textbf{統計的因果探索}という．
    \end{enumerate}
    これは数理科学の拡張の過渡期における重要な瞬間であり，物理学の意味から「実験」を「計算」に拡張して汎神化する過程である．
    この過程を経た後に，学問のあり方は大きく変化する．このフェーズでは数学も変化が必要である．
    私が憧れたのはこの未来感であり，未来を見せた啓蒙に対する数学の責任であり，
    属人化される知である．

    まさに私の目前で，相関関係を超えた因果関係の数理構造が定式化されつつある．
    この営みに乗らないはずがない．事事無礙の法界を写すことが数理の営みであるのならば，
    因果推論は莫大な霊性源とならないはずがない．
\end{quotation}

\section{歴史}

\begin{history}[調査観察データに対する因果推論の潮流]\mbox{}
    \begin{enumerate}
        \item Donald Rubinの因果モデルは手法としては「欠測データ解析の方法論」として結実した．EMアルゴリズムと多重代入法など．
        その後のMCMC法を用いたベイズ統計学の発展にも寄与し，統計学のパラダイムに大きな変化をもたらした．
        \item もう一つの流れが，モデル仮定をなるべくおかずに推定することを目指すセミパラメトリック推定法である．Huberの$M$-推定，Liang \& Zegerの一般化推定方程式など．
        医学データなど，経時的多変量データや，共変量が多い場合の解析に強い．
        \item 共変量と結果変数の回帰関係を仮定せずに，共変量の影響を削除する傾向スコアが頻繁に利用されるようになったが，これもRubinのセミパラメトリック解析の一種である．
        \item さらに，Pearl流の手法としては「構造方程式モデル」と「ベイジアンネットワーク」とに代表される．Wrightのパス解析のノンパラメトリックモデルへの拡張と言える．\footnote{\href{https://www.jstage.jst.go.jp/article/jjb/32/2/32_119/_article/-char/ja/}{構造的因果モデルについて．黒木 学, 小林 史明}}
    \end{enumerate}
\end{history}

\begin{history}[セミパラメトリックモデル]
    医学や経済学での調査観察研究では共変量を非常に多く考える必要があるため，線型の回帰モデルではデータの説明力が低く，またカーネル法などのノンパラメトリック法ではいわゆる「次元の呪い」の問題が完全には解決できない．
    CS的に後者を解決する突破口ももちろんある．
\end{history}

\section{枠組み}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    因果推論の枠組みは，推定したい目的量を，理想化された介入のことばによって定義することを可能にする．
    （この形式的成果は，きっと因果推論の他にも使えるが，概念上因果と結びつきが深い）．
    現実には出来ない設定に対して，同一視可能な設定を作り上げる手法を考える，と捉えると，営みは圏論的である．
    Pearlのgraphical criteriaは極めてpromisedだと思う．
    潜在反応と構造方程式はその例である．
\end{tcolorbox}

\begin{notation}[outcomes that would have been observed under some intervention]\mbox{}
    \begin{enumerate}
        \item $A$をどのような処置を行ったかを表す独立変数とする．
        \item $Y^a$によって，$A=a$とした場合の独立変数$Y$の実現値とする．
        \item $T$-次元の場合，多重指数記法$\o{A}_T:=(A_1,\cdots,A_T)$を採用すると，$Y^{\o{a}_T}$で表す．
        \item $G:\X\to\A$を確率的な介入(stochastic intervention)とする．これは，共変量$X:\Om\to\X$の値に対して，$A=1$を分配する確率$g(x)$を表す確率密度関数である．$Y^G$で，この処置を行った場合の反応を表す．
    \end{enumerate}
\end{notation}

\subsection{汎関数推定}

\begin{notation}
    真の分布$\bP$に対して，$\bP\in\P$を満たす統計モデル$\P$を立てる．
    これ自体を推定するのではなく，パラメータなど，汎関数$\psi(\bP)$の推定を考える．
\end{notation}

\begin{example}
    汎関数の推定量$\wh{\psi}$を立てるのに，次のような手法が考えられる．
    \begin{enumerate}
        \item パラメトリックベイズ，MLE（最尤推定法）
        \item ノンパラメトリックな最尤推定法，plug-in．
        \item ノンパラメトリック影響関数に基づいた手法．double machine learningやtargeted leaerningともいう．
    \end{enumerate}
\end{example}

\begin{remark}
    統計学的な数理的課題と，因果論的な話題はいつだって区別するべきである．
    「どうやって」推定するか，と，「何を」推定するべきか．
    反事実標本上の確率分布を$\bP^*$としたとき，
    $\psi^*(\bP^*)=\psi(\bP)$と標本分布$\bP$に翻訳するところまでが因果推論における形式論で，
    その後起こることは純粋な汎関数推定の問題であり，
    数理的な状況に対処する段階とを峻別するべきである．
\end{remark}

\begin{example}[因果推論以外の文脈で生じる汎関数推定]\mbox{}
    \begin{enumerate}
        \item integrated squared density：$\psi=\int p(x)^2dx$．
        \item entropy：$\psi=-\int p(x)\log p(x)dx$．
        \item support size：$\psi=\Sigma_x1_{\Brace{p(x)>0}}$．
        \item mutual information：$\psi=\int\int p(x,y)\log\frac{p(x,y)}{p(x)p(y)}dxdy$．
    \end{enumerate}
\end{example}

\subsection{潜在反応モデル}

\begin{axiom}[ATE]
    次を仮定する．
    \begin{enumerate}
        \item Positivity：$\exists_{\delta>0}\;\bP[\bP[A=a|X]\ge\delta]=1$．
        \item Consistency：$A=a\Rightarrow Y=Y^a$．
        \item Ignorability：$A\indep Y^a|X$．
    \end{enumerate}
\end{axiom}

このとき，
\[E[Y|X,A=a]=E[Y^a|X,A=a]=E[Y^a|X]\]
より，$E[Y^a]$という仮想上の量を，$E[E[Y|X,A=a]]$という求めることが可能な量によって推定出来る．

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
    title=]
    反事実，潜在反応などの用語が出来た．
    $E[y_1]-E[y_0]$をはじめに定義したのはRubin 1974で，
    Rosenbaum \& Rubin 1983は「平均処置効果」と言い直した．

    このモデルの上に立脚した統計的因果推論には，Rubin流のRCMs(Rubin's Causal Models)とPearl流の構造的因果モデル(SCMs)との２つがある．
    Judea Pearlはベイジアンネットワークの理論を体系的に整備した人工知能研究者である．グラフィカルモデルがよく用いられ，実際Judea Pearlの論文はすべてグラフ用語でも記述されている（この表記の重要性は強調されるが，必ずしも理論的に不可分ではない）．
\end{tcolorbox}

\begin{axiom}[counterfactual model / approach]
    ある行為をした場合による事実と，反事実の乖離を\textbf{因果関係}という．
    この定立を\textbf{反事実モデル}という．
\end{axiom}

\begin{definition}[potential outcome, Fundamental Problem of Causal Inference, causal effect, average treatment effect]\mbox{}
    \begin{enumerate}
        \item 独立変数の値域の濃度と同数の，仮想的な従属変数を\textbf{潜在的な結果変数}という．
        「仮想的な」というのは，独立変数は１つの値にしか実現しないので，１つを除いて欠測することが約束されていることをいう．
        このことを\textbf{因果推論の根本問題}(Holland 1986)という．
        \item 潜在的な結果変数の差$y_1-y_0$を\textbf{因果効果}という．
        一方で，潜在的な結果変数は１つを除いて欠測することが約束されているため，畢竟\textbf{平均処置効果}$E[y_1-y_0]=E[y_1]-E[y_0]$を考えることとなる．
    \end{enumerate}
\end{definition}
\begin{remark}
    もし群別が無作為ならば，独立変数と（潜在的な）従属変数はすべて独立であるから，欠測を無視して，観測された各群の平均値の差で平均処置効果を不偏推定出来る．
\end{remark}

\begin{definition}[avarage treatment effecti on the treated, avarage treatment effect on the untreated, quantile treatment effect]\mbox{}
    \begin{enumerate}
        \item $TET:=E[y_1-y_0|z=1]$を\textbf{処置群での平均介入効果}という．政策評価など，こちらのほうが重要視される場面もある．
        \item $TEU:=E[y_1-y_0|z=0]$を\textbf{対照群での因果効果}という．
        \item $Q_\al(a)$を，変数$a$の$100\times(1-\al)$パーセント分位点とする．$Q_\al(y_1)-Q_\al(y_0)$を\textbf{分位点での因果効果}という．所得の分布など，期待値でない特性値に関心がある場面もある．
    \end{enumerate}
\end{definition}

\begin{definition}[intermediate variable, moderator variable, covariate / control variable / confounding factor]
    独立変数，従属変数の他に，
    \begin{enumerate}
        \item 独立変数の関数で，従属変数がその関数であるような変数を，\textbf{中間変数}または媒介変数という．これを介して起こる因果効果を間接効果という．\footnote{間接効果と直接効果を併せたものを総合効果といい，社会科学では多くこれを取り扱うことになる．}
        \item 独立変数がこの関数となっているような変数を，\textbf{調整変数}という．この変数が定める同値類で，従属変数と独立変数との関係が大きく変わるような変数である．
        \item 従属変数と独立変数が，いずれもこの関数であるような変数を\textbf{共変量}や統制変数という．医学分野では交絡要因ともいう．
    \end{enumerate}
\end{definition}
\begin{remark}
    これらの峻別，特に中間変数を共変量と誤認することは因果効果を過小評価することに繋がる．
\end{remark}

\subsection{区間推定の営み}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    空想上の量$\psi^*$は点推定可能でないときの方が良い．そのときでも，$\psi^*$のboundsが求まるときは多い．
    そこで，代わりにboundsを推定することを考えることが出来る．Manskiなど．
\end{tcolorbox}

\section{研究の種類}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=すべてが実験になる]
    独立変数への研究者による介入があるかどうかで，実験研究と観察研究を分けることが出来る．
    独立変数の群別への割り当てが無作為であるかどうかで，さらに細かく分類することが出来る．

    因果効果は処置の有無が産む差と定義したのが潜在反応モデルであるが，
    因果推論の根本問題どころか，処置を施すこと自体が不可能な場合が多い．
\end{tcolorbox}

\subsection{実験研究}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    \textbf{実験研究}または統制実験という．
\end{tcolorbox}

\begin{definition}[experimental / treatment group, control group]
    無作為割当について，処置を行った群を\textbf{実験群}または\textbf{処置群}，行っていない群を\textbf{対照群}という．
\end{definition}

\subsection{調査観察研究}

\begin{example}\mbox{}
    \begin{enumerate}
        \item ランダム化比較試験などは，無作為割り当てを伴う．計量経済学では社会実験ともいう．EBMにおいてはメタアナリシス＝システマティックレビューの次に根拠が高いとされる．
        \item メタアナリシス＝システマティックレビューなど，１次研究の結果を統合するプロセスを\textbf{Research synthesis}という．
        \item 一般に\textbf{相関研究}(correlational study)や\textbf{観察研究}(observational study)は，無作為割当を伴わない研究を言う．計量経済学では，独立変数の値が共変量の値によって確率的に決定され，いわゆる独立変数に\textbf{内生性}がある場合の研究を言う．
        \item 政策評価では，自然実験なる概念もある(EBPM)．これは2021にノーベル経済学賞の主題であった．似た概念を，EBMではコホート研究という．\footnote{特定の要因に曝露した集団と曝露していない集団を一定期間追跡し、研究対象となる疾病の発生率を比較することで、要因と疾病発生の関連を調べる観察研究の一種である。要因対照研究（factor-control study）とも呼ばれる。}
        \item EBMでさらに一段階下の観察研究が，後ろ向きコホート研究とも呼ばれる，症例対照研究である．\footnote{疾病に罹患した集団を対象に、曝露要因を観察調査する。次に、その対照として罹患していない集団についても同様に、特定の要因への曝露状況を調査する。}
    \end{enumerate}
\end{example}

\section{RCMs：Rubin因果モデル}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    因果推論の根本問題を，欠測データの枠組みで捉える．
    このとき用いることが出来る情報は，共変量情報である．
    このときに行う解析法は，パラメトリックだと非線形関係を取りこぼし，ノンパラメトリックでは共変量がそもそも多いために次元の呪いに陥る．
    そこで，特に関心のある部分はパラメトリックに仮定し，共変量が影響する関心のない部分はモデリングを避ける．これをセミパラメトリックモデルという．
\end{tcolorbox}

\section{SCMs：構造的因果モデル}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
    title=]
    数値への意味論の付与の仕方はただ一通りのみ許す定立を反事実モデルという．
    すなわち，因果効果の測定は，相関係数とは何の関係もなく\footnote{相関関係と因果関係の乖離を一般に疑似相関という．}，（平均）因果効果と呼ばれる数量によって評価する．

    次の問題として，構造的因果モデル$M$に数学的対象を付与する関手をあてがう．
    この方式は創造的行為であり，線型非線形を飛び交う議論になる．

    構造的因果モデルの定義がやけに数学基礎論的に提示されたことが希望をくすぐる．
    これはこれから種々の数学手法を導入し，最終的には計算機の上に実装することが究極の祈りではなかろうか？

    参考：\footnote{\href{https://www.jstage.jst.go.jp/article/jjb/32/2/32_119/_article/-char/ja/}{構造的因果モデルについて．黒木 学, 小林 史明}}
\end{tcolorbox}

\subsection{構造方程式モデルと因果グラフ}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    因果関係は2上の豊穣圏として定式化できる．
    この射をデータ生成過程と見て，確率分布の間の変換として定式化する
    数理モデルを，構造方程式モデルという．
\end{tcolorbox}

\begin{notation}[数学基礎論的記号設定]\mbox{}
    \begin{enumerate}
        \item 大文字は確率変数，小文字はアルファベットとする．ここに目的言語とメタ言語の構造がある．
        \item これを用いて，代入$=$を$A=a$と表し，介入という意味論を持つ．または$\Do(x=c)$とも表す．モデルに対して$M_{x=c}$という記法も，目的言語に於ける代入を意味する．
        \item 右上には個体名／添字を書く．反変ベクトルであるためである．
    \end{enumerate}
\end{notation}

\begin{axiom}\mbox{}
    \begin{enumerate}
        \item (mean exchangability) $\forall_{a}\;E[Y_a|A=1]=E[Y_a|A_0]$．２つに分けた集団が仮に逆であっても平均を変えない性質をいう．
        \item (consistency) $\forall_{a}\;E[Y_a|A=a]=E[Y|A=a]$．
    \end{enumerate}
\end{axiom}
\begin{example}[confounding, randomization]\mbox{}
    \begin{enumerate}
        \item 次のDAGが成立している場合（\textbf{交絡要因の存在}），平均交換律は成り立たない．
        \item 十分に大きな集団で無作為割賦を行うと，平均交換律を満たす．\footnote{ランダム化したにもかかわらず偶然でExchangeabilityが成立しなかった場合には、共変量の調整によって後述するConditional Exchangeabilityを目指していきます。\cite{芝孝一郎}}\footnote{現実の世界では、割付された介入に従わない人が少なからずいます。例えば薬の効果を調べようとしてランダム化を行ったときに、薬を割付られたのにめんどくさがって薬をきちんと飲まない人がいるかもしれません。このような状況を割付へのnon-complianceと呼びます。\cite{芝孝一郎}}
    \end{enumerate}
\end{example}

\begin{definition}[SEM: Structural Equation Model]
    4-組$M=(v,u,f,p(u))$を\textbf{構造方程式モデル}という．特にデータ生成過程$f$をモデルに含む点が特徴的であり，
    $f$が$p(u)$から，内生変数の確率分布$p(v)$を引き起こす点が特徴である．\footnote{    * このデータ生成過程を記述しないのが多くの統計学や機械学習のモデルとなる。\cite{清水昌平}}
    \begin{itemize}
        \item $v\in\R^p$を内生変数(endogenous variable)という．
        \item $u\in\R^q$を外生変数(exogenous variable)という．
        \item 関数$f:\R^q\to\R^p$をデータ生成過程という．
        \item $p(u):\R^q\to[0,1]$は外生変数の確率分布を与える．
    \end{itemize}
\end{definition}

\begin{definition}[causal graph / path diagram / causal Bayesian network / DAG:directed acyclic graph]\mbox{}
    \begin{enumerate}
        \item データ生成過程のモデルを作る上での仮定を表現する図である．\footnote{They can also be viewed as a blueprint of the algorithm by which Nature assigns values to the variables in the domain of interest.\url{https://en.wikipedia.org/wiki/Causal_graph}}
    \end{enumerate}
\end{definition}

\subsection{構造的因果モデル}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    確率論に言語$\Do$を足したもの，と考えられる．
    こうして数学基礎論的にまとめているのは，伊藤清に加えた新たな純粋数学をどう作るかの気概を感じる．
    SCMはJudea Pearl (1995, 2009a)が導入．
    $f,p(u)$によりデータ生成過程がモデルに入っていることが特徴である．
\end{tcolorbox}

\begin{definition}[Structural Causal Models]
    構造方程式モデル$M=((x,y),e_y,f,p(e_y))$に対して，介入$M_{x=c}$は次を構造方程式とするモデルとなる：
    \begin{align*}
        x&=c,&y&=f_y(x,e_y).
    \end{align*}
    \begin{enumerate}
        \item 集団において，$x$は$y$の原因になるとは，次が成り立つことをいう：$\exists_{c,d}\;p(y|\Do(x=c))\ne p(y|\Do(x=d))$．
        \item $E(y|\Do(x=d))-E(y|\Do(x=c))$を平均因果効果と呼ぶ．
    \end{enumerate}
\end{definition}

\subsection{グラフィカルモデリング}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    宮川雅巳氏の著書が草分け．
\end{tcolorbox}

\begin{discussion}[工学分野が主流]
    例えば社会科学分野であまり使われていない理由は，中間変数間の部分的な独立性や変数の親子関係などの仮定が明確である場合が少ないからである．むしろそれらの確定が目的のひとつだったりする．
\end{discussion}

\section{因果効果の推定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
無作為割当が出来ない場合は，共変量を利用することで，無作為割当の状態を推定し，因果効果の推定値を出すことになる．
\end{tcolorbox}

\begin{definition}[marginal structural model]\mbox{}
    \begin{enumerate}
        \item 共変量$x$の影響を除去した，潜在的な結果変数の周辺期待値を考えることを，\textbf{周辺構造モデル}という．
        \item 共変量の値に依存しない量を得るために，共変量の分布について平均を取ることを，\textbf{共変量についての周辺化}または\textbf{共変量調整}という．
    \end{enumerate}
\end{definition}

\subsection{強く無視できる割当条件}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    RCTのように比較が出来るための，共変量$x$が満たすべき条件である．
\end{tcolorbox}

\begin{axiom}[strongly ignorable treatment assignment (Rosenbaum \& Rubin 1983), conditional effect, marginal effect]
    割当は，共変量のみに依存し，結果変数には依存しないとする仮定を\textbf{強く無視できる割当}という．
    これは，船内的な結果変数$(y_1,\cdots,y_n)$が，割当変数$z$と，共変量$x$の値についての条件付き確率について独立であること$(v_1,v_0)\indep z|x$と同値．
    また，これは欠測がRAMであることの十分条件である．
    このとき，共変量の値を所与とした結果変数の条件付き期待値の差$E[y_1-y_0|x]$を\textbf{条件付き効果}，共変量の影響を除去したあとの因果効果を\textbf{周辺効果}という．
\end{axiom}

\begin{lemma}[mean independence]
    共変量が同じ対象者については，処置群と対照群で潜在的な結果変数の期待値は同じである：
\end{lemma}

\subsection{マッチング（均衡化）}

処置群と対照群で，共変量の値が等しい対象者の組を考える．
当然，共変量の次元が多いと難しくなる（\textbf{次元問題}）．
また，共変量の分布が大きくずれると，その部分についてはマッチング不可能（\textbf{サポート問題}）．

\begin{example}\mbox{}
    \begin{enumerate}
        \item 共変量ベクトルが一致することを要請することを，完全マッチングという．
        \item Mahalanobis距離によるマッチング(Rubin, 1980)．これは質的変数が共変量となることに弱い．
        \item 最近傍マッチング，caliperマッチング．
    \end{enumerate}
\end{example}

\subsection{サブグループ解析（恒常化・限定）}

条件付き効果を推定する．

\subsection{層別解析}

ある程度粗い同値類を作って解析する．

\subsection{回帰分析}

各群ごとに回帰関数$E[y_i|z=i,x]$を推定し，その差の標本平均を取る．

\begin{example}\mbox{}
    \begin{enumerate}
        \item 共分散分析モデル
        \item パス解析モデル
        \item Cox回帰
        \item 多群の構造方程式モデル
    \end{enumerate}
    いずれも，回帰関数を誤って設定すると大きなバイアスを生じる．パラメトリックな手法の限界である．
\end{example}

\begin{problem}\mbox{}
    \begin{enumerate}
        \item 結果変数と共変量のモデリングが必要である．
        \item 直接因果効果の推定値は得られない．
    \end{enumerate}
\end{problem}

この問題を解決するために，傾向スコア解析法など，ノンパラメトリック・セミパラメトリックな手法が要請される．
特に，潜在的な結果変数と説明変数との間はパラメトリックな仮定を置くが，共変量$x$とその他の変数の間には回帰関係を仮定せずに解析するとなると，これはセミパラメトリックモデルとなる．

\begin{example}[局所多項式回帰 Stone 1977：ノンパラメトリック]
    バンド幅をどう指定するか，次元の呪いなどの問題点が残る．
    突破口があるならCSであるが，社会科学分野で解析手法として用いられることはまだあまりない．
\end{example}

\section{因果探索の基本問題}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    経済学や社会科学や疫学や生活など，controlled experimentが不可能な場面は多く，non-experimentalなデータから因果効果を推定する必要は各学問で肝要である．
\end{tcolorbox}

\subsection{因果探索の基本問題}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
    title=]
        違う因果関係が同じ相関関係を定め得るが，観測変数の分布には相違が現れる．
        これを足掛かりにして因果探索が行われる．
\end{tcolorbox}

\begin{model}[因果探索の基本問題]
    次の３つの構造的因果モデル$M=((x,y),(z,e_x,e_y),f,p)$を考える．ただし，$x,y\in\R^n$．
    \begin{enumerate}[(A)]
        \item $\begin{cases}
            x=f_x(z,e_x)\\
            y=f_y(x,z,e_y)\\
            p(z,e_x,e_y)=p(z)p(e_x)p(e_y)
        \end{cases}$すなわち，$x\to y$．
        \item $\begin{cases}
            x=f_x(y,z,e_x)\\
            y=f_y(z,e_y)\\
            p(z,e_x,e_y)=p(z)p(e_x)p(e_y)
        \end{cases}$すなわち，$x\leftarrow y$．
        \item $\begin{cases}
            x=f_x(z,e_x)\\
            y=f_y(z,e_y)\\
            p(z,e_x,e_y)=p(z)p(e_x)p(e_y)
        \end{cases}$すなわち，$x\quad y$．
    \end{enumerate}
    外生変数$z,e_x,e_y$を独立とした．これは，$z$以外に未観測共通要因が無いことと同値．
    また，自律性(autonomy)を仮定する，すなわち，$u$介入を行っても，$f,p$に影響しないことを仮定する．
    また，因果関係が一方向である（$f_x$が$y$を引いたり，再帰的な構造がない）ことを仮定した．これをacyclicという．

    この時，データ行列
    \[X=\begin{pmatrix}x^1&\cdots&x^n\\y^1&\cdots&y^n\end{pmatrix}\]
    が与えられた時，これを生成したモデル$M$を決定する問題を，因果探索の基本問題という．
    変数を$x,y$以外に追加した場合も，この問題に還元される．
\end{model}
\begin{example}\mbox{}
    \begin{enumerate}
        \item $x$はチョコレートの消費量，$y$は一国のノーベル賞受賞者数，$z$はGDPと見れる．
        \item $x$は薬を飲むかどうか，$y$は病気に罹患しているか，$z$は病気の重症度とみれる．
    \end{enumerate}
\end{example}

\subsection{３つの手法}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $f,p$について種々の仮定を置くことが考えられるが，数学的には，因果グラフが識別可能な仮定のクラスが重要となる．
    そのためには，$p$の非Gauss性が肝要になることが\cite{清水昌平}の発見である．
\end{tcolorbox}

\subsubsection{non-parametric approach}

何の仮定も置かないと，因果グラフは識別可能でない．
理論的な限界点を明らかにする理論的な価値がある．

\subsubsection{parametric-approach}

実質科学からの事前知識や洞察を反映して，$f$と$p$に仮定をおいて
３つのモデルを比較する営みである．
特に数理的には，推定に必要な観測数が小さくなるなど，解析が容易になるなどが起こる．
特に，数理のモデル進化の定番として，最初は
$f$の線形性と$p$のGauss性を仮定することが多い．
が，この場合も，観測変数の分布がいずれも同様なGauss分布となるので，
因果探索の基本問題について，因果グラフは識別可能でない．
これはGauss分布が２次元多様体をなすことに起因する．

\begin{remark}
    因果グラフの推測には，例え非線形な系に対しても線形性を仮定した方がうまくいくという報告が多い．
    その後にノンパラメトリックな方法で因果効果の大きさを定量化するという流れが考えられる．
\end{remark}

\subsubsection{semi-parametric approach}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $p$の非Gauss性に注目すると，未観測の交絡要因$z$が存在しようと（存在さえ未知だろうと），因果推論が可能になる．
    We have recently described how \textit{non-Gaussianity} in the data can be exploited for estimating causal effects.
    In this paper we show that, with non-Gaussian data, causal inference is possible even in the presence of hidden variables (unobserved confounders),
    even when the existence of such variables is unknown a priori.
    Thus, we provide  a comprehensive and complete framework for the estimation of causal effects between the observed variables in the linear, non-Gaussian domain. \cite{清水08}
\end{tcolorbox}

\begin{definition}[LiNGAM: Linear Non-Gaussian Model]
    $f$は線型関数，$p$は非Gaussな連続分布と仮定する手法をいう．
\end{definition}

\begin{theorem}
    LiNGAMの仮定を置いた場合，３つのモデルの因果グラフは識別可能である．\cite{清水08}
\end{theorem}

\chapter{欠測データの扱い}

\begin{quotation}
    偏りのあるデータからの統計的な因果推論を考える．
    全てがデータになる。全てがアルゴリズムになる。そして、全てが実験になる。世界全体がデータと実験の塊になった世界。

    \cite{星野}の話題は３つ．
    \begin{enumerate}
        \item 実験が行われていない研究で得られるデータからの統計的な因果推論．
        \item 偏った抽出による標本・データを用いることで生じるバイアス（これを\textbf{選択バイアス}と呼ぶ）の統計的調整．労働経済学者のHeckmanが草分け．実は，人工知能分野での「領域適応(domain adaptation)」や「共変量シフト」とも関連が深い．
        インターネット調査などでは，レスポンスをする人自体に偏りがあるので，マーケティング分野で積極的に利用されるようになってきた．
        \item 複数の情報源から得られたデータを統計的に融合させて行うデータ融合．特に大きなプラットフォームを持つ主体が，消費者のカテゴリーを超えた購買行動やクロスメディアコミュニケーションを分析するために興隆した．
    \end{enumerate}
    \textbf{背景情報＝共変量}を積極的に利用することで，欠測データの部分を予測できる可能性がある．
    (2),(3)の話題は，統計的因果推論と同型の問題構造を有しているので，(1)に集中すれば良い．

    現実の構造を見抜いて，それに対応する数理構造を創る．これは想像と創造である．

    さらに，セミパラメトリックモデルで置かれた仮定になるべく依存せずに関心のある量の推測を精度良く行う，ロバスト解析が重要になる．
\end{quotation}

\section{欠測の分類}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    欠測メカニズムのモデル$p(m|\phi)$をモデルに盛り込む．
\end{tcolorbox}

\begin{definition}[monotone missingness]
    欠測パターンについて，
    (1),(2),(3)などで，「ある変数が欠測であれば，別の変数でも必ず欠測がある」という関係がすべての対象者で成立するとき\textbf{単調欠測}という．
\end{definition}

\begin{example}\mbox{}
    \begin{enumerate}
        \item 各変数レベルでの無返答(item nonresponse)．
        \item 打ち切りと切断(truncation)．後者は閾値を超えた観測数そのものも不明．または部分的なデータで全体を代表させる測定(surrogate measurements)．
        \item 脱落(dropout)やパネルの摩耗(attrition)．またはnoncommpliance．
        \item 調査全体への不参加(unit nonresponse)．
        \item rounding．
        \item 連続データの離散化．
    \end{enumerate}
    狭義の欠測は(3)まで．(4)は選択バイアスを産む．(5),(6)を含めて不完全データともいう．
\end{example}

\begin{definition}[Rubin (76)]
    欠測のメカニズム$p(m|y,\phi)$について，欠測するかどうかは，
    \begin{enumerate}
        \item MCAR: Missing Completely at Random：モデリングに用いている変数に依らない．
        \item MAR: Missing at Random：欠測値には依存せず，観測できた値に依存する．
        \item NMAR: Nonmissing at Random：欠測値そのものと，観測していない他の値にも依存する．
    \end{enumerate}
    の３種類がある．$p(m|y,\phi)$とは，(2)の場合を考えている．
\end{definition}

\begin{example}
    入試成績と，入学後の成績とのデータの組など，選抜効果はMARを産む．
\end{example}

\section{枠組み}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    欠測データに対する最尤推定は，
    「完全尤度」なる量の最大化を考えることとなる．
\end{tcolorbox}

\subsection{選択モデル}

\begin{definition}[selection model, complete data likelihood, distinctness, ignorable missingness]
    $y=(y_1^\perp,y^\perp_2)^\perp$を完全データのベクトルとし，$p(y|\theta)$を完全データのモデル，$p(m|y,\phi)$を欠損の有無を表す確率変数$m:\Om\to2$に関するモデルとする．
    \begin{enumerate}
        \item ２つの同時分布が，直積測度$p(y,m|\theta,\phi)=p(y|\theta)p(m|y,\phi)$によって表せるとき，これを\textbf{選択モデル}という．結合確率$p(y,m|\theta,\phi)$を，\textbf{完全データの尤度}ともいう．
        \item このように，母数$\theta,\phi$について変数分離可能である性質のことを，\textbf{分離性}という．
        \item ランダムな欠損(MARまたはMCAR)であり，かつ，分離性を満たす欠損を\textbf{無視できる欠測}という．
        \item 欠測データについての平均$p(y_1,m|\theta,\phi)=\int p(y|\theta)p(m|y,\phi)dy_2$を，\textbf{完全尤度}という．\footnote{ここでの「完全」は完全データというときの「完全」とは違い，観測されたデータを完全に利用しているという意味である．}
    \end{enumerate}
\end{definition}
\begin{remark}[無視できる欠測について]\mbox{}
    \begin{enumerate}
        \item MARのとき，完全尤度は$p(y_1,m|\theta,\phi)=p(y_1|\theta)p(m|y_1,\phi)$をなる．これを\textbf{観測データの尤度}という．そこでこの場合は，$p(y_1|\theta)$についての最尤推定のみを考えれば良いので，「無視できる」という．
        \item MCARのときも同様に，$p(y_1,m|\theta,\phi)=p(y_1|\theta)p(m|\phi)$が成り立つ．
    \end{enumerate}
\end{remark}

\subsection{パターン混合モデル}

\begin{definition}[pattern mixture model]
    同時分布が$p(y,m|\xi,\om)=p(y|m,\xi)p(m|\om)$．
\end{definition}

\subsection{共有パラメータモデル}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    最近利用される．
\end{tcolorbox}

\begin{definition}[shared parameter model]
    潜在変数として変量効果$\beta$を仮定し，この下で条件付き独立となる$p(y,m|\xi,\om,\beta)=p(y|\beta,\xi)p(m|\beta,\om)$とき，\textbf{共有パラメータモデル}という．
\end{definition}

\chapter{汎関数推定}

\begin{quotation}
    Causal inference is over.

    サンプル数を$n$とすると，$\P$上の汎関数$\psi(P)\in\R$を推定する関数$\wh{\psi}:\X^n\to\R$を作る．
    一般に巨大なモデル$\P$の推定は難しいが，その上の汎関数ならば，随分と豊かな理論が揃ってきた．
\end{quotation}

\section{有効理論導入}

\begin{definition}[influence function for estimator]\label{def-IF-for-estimator}
    推定量$\wh{\psi}$が影響関数$\phi$を持つとは，$\sqrt{n}$-一致性と漸近正規性(CANと略す)
    \[\sqrt{n}(\wh{\psi}-\psi)=\sqrt{n}\bP_n\phi +o_P(1)\Rightarrow N(0,\Var[\phi])\]
    が成り立つことをいう．
\end{definition}
\begin{remark}[plug-in method]
    ノンパラメトリックモデルでは，$\sqrt{n}$-CANは高々1つしか存在しない．
    そこで，最も自然なアプローチは，分布の第一段階推定$\wh{P}$を用いて，
    plug-in $\psi(\wh{P})$とすることである．
    しかしこれは，一般には$\sqrt{n}$-一致性を持たず，また信頼区間を定めない．
    $\wh{P}$が特定の推定量で，強い正則条件を持つ場合に良い振る舞いをする．
\end{remark}

\begin{example}[第一段階によるバイアスが残る]
    plug-inにより汎関数$\psi=\int p(x)^2dx=E[p(X)]$を推定することを考える．
    推定量を$\wh{p}$として，$\wh{\psi}:=\bP_n[\wh{p}(X)]$とする．
    次が成り立つ：
    \[\wh{\psi}-\psi=(\bP_n-P)p+P(\wh{p}-p)+o_P(n^{-1/2}).\]
    初項は中心極限定理より$O_P(n^{-1/2})$である．
    第二項はCauchy-Schwartzによると$\abs{P(\wh{p}-p)}\le\norm{\wh{p}-p}$．
    あるHolder $\beta$に対して，$O_P(n^{-\beta(2\beta+d)})$となる．

    $\wh{p}$が一致性を持つ限り，plug-in推定量も一致性を持つが，$\wh{p}$由来のバイアスが残っており，収束は遅い．

    しかし，$\wh{\psi}$としてkernel estimator
    \[\wh{p}(t):=\bP_n\paren{\frac{1}{h}K\paren{\frac{X-t}{h}}}\]
    を選び，$K$はhigher-order kernelで，$h\sim n^{-1/(\beta+d)}$で，かつ$\beta>d$であるならば，
    第二項は
    \[P(\wh{p}-p)=(\bP_n-P)p+o_P(n^{-1/2})=O_P(n^{-1/2})\]
    と評価できる．

    問題は，
    \begin{enumerate}
        \item $\beta>d$の範囲で，これが最適であるか？
        \item $\beta\le d$の場合どうすればよいのか？
        \item 滑らかさの構造以外に依れないか？
        \item 別のgeneric nuisance estimatorを使いたい場合は？
    \end{enumerate}
    これらの疑問が，nonparametric efficient theoryと影響関数に手招いている．
    これが準備すれば，kernel estimator with careful tuningを用いなくて済み，よりunderlying structureに無頓着になることが出来，double machine learningなどの柔軟な手法が取れる．
\end{example}

\section{セミパラメトリックモデルの有効性}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    理論的に肝要なのは，パラメトリックモデルに対してCramer-Raoの不等式が与えるような下界を，
    セミパラメトリックモデルに対して拡張することである．
\end{tcolorbox}

\subsection{セミパラメトリックモデル}

\begin{definition}
    有限次元の実パラメータで添字付けられない集合$\P\subset P(\R)$を，セミパラメトリックモデルという．
\end{definition}

\begin{example}[semiparametric model]\mbox{}
    \begin{enumerate}
        \item ノンパラメトリックモデル．はじめのセミパラメトリックモデルは，Begun et al. (83)により"parametric-nonparametric models"と呼ばれた．
        \item GEE(Generalized Method of Moment)，$m$-推定量，制限モーメントモデル．これは部分的に回帰関数を仮定するもので，
        $Y=\mu(X;\psi)+\ep$とし，$\psi\in\R^d$を興味のある母数，$\ep$は$E[\ep|X]=0$のみを仮定する局外母数とする．すなわち，$E[Y|X]=\mu(X;\psi)$のみが仮定となる．
        \item Coxモデル：$T$を生存時間，$Z=(X,T)$とし，条件付きhazardの比のみを$\frac{\lambda(t|X=x)}{\lambda(t|X=0)}=\exp(x^\top\psi)$と仮定する．
    \end{enumerate}
    また，非Euclid空間上の汎関数に関するパラメトリックな仮定をおいた場合も，自然にセミパラメトリックモデルを生じる．
    $\gamma(v):=E[Y^1-Y^0|V=v]$とし，共変量$V$を高次元あるいは連続な成分を持つとすると，$\gamma(v;\psi)\;(\psi\in\R^q)$として，パラメトリックな部分を抽出することは自然である．
\end{example}

\begin{remark}
    因果推論問題においては，観察研究とは違って，実験計画（傾向スコアなど）が既知の場合が多い．一方で，反応は全く未知のことが多いので，パラメトリックな仮定をおくことを避けたいのである．
    一方で，実験もしていない場合は，ノンパラメトリックモデルのみが，合理的なモデルとなる．
    この場合でも，影響関数・経験過程論・サンプル分割が必要になるが，機械学習を用いた場合でも，$\sqrt{n}$-収束や，有効な信頼区間を得ることが出来る．
\end{remark}

\subsection{古典的下界の一般化}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    Cramer-Raoの結果は，古典的なパラメトリックモデルに対する下界を与える結果である．
\end{tcolorbox}

\begin{discussion}
    $(P_\theta)_{\theta\in\Theta}$が滑らかで，$T$は$\psi(\theta)$の不偏推定量であるとする．
    このとき，
    \[\Var[T]\ge\frac{\psi'(\theta)^2}{E[s^2_\theta]}.\]
    ただし，$s_{\theta^*}:=\pp{}{\theta}\log p_\theta(z)|_{\theta=\theta^*}$はスコアである．

    This is also a lower bound in \textbf{asymptotic minimax sense}:
    \[\forall_{\wh{\psi}:\X^n\to\R}\quad\inf_{\delta>0}\liminf_{n\to\infty}\sup_{\norm{\theta'-\theta}<\delta}E_{\theta'}\Square{l\paren{\sqrt{n}\paren{\wh{\psi}-\psi(\theta')}}}\ge E\Square{l\paren{N\paren{0,\frac{\psi'(\theta)^2}{E[s^2_\theta]}}}}\]
\end{discussion}

\begin{definition}[parametric submodel]
    ノンパラメトリックモデル$\P$の\textbf{パラメトリックな部分モデル}$\P_\ep:=\{P_\ep\}_{\ep\in\R}\subset\P$とは，真値を含む：$P=P_0$ものをいう．
\end{definition}]
\begin{example}
    $E[h]=0,\norm{h}_\infty<M,\ep<1/M$について，真値$p$に対して
    $p_\ep(z):=p(z)(1+\ep h(z))$とすると，$p_\ep(z)\ge0$．
\end{example}

\subsection{pathwise differentiablity}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    このpathwise differentiabilityが大事で，これを用いてplug-in推定量のバイアスを修正する．
\end{tcolorbox}

\begin{problem}
    部分モデル$\P_\ep$に対する下界$\frac{\psi'(P_\ep)^2}{E[s^2_\ep]}$を考える．ただし，
    \[\psi'(P_\ep)=\pp{}{\ep}\psi(P_\ep)|_{\ep=0},\quad s_\ep=s_\ep(z)=\pp{}{\ep}\log p_\ep(z)|_{\ep=0}.\]
\end{problem}

\begin{discussion}
    仮に，1次のvon Mises-type展開
    \[\psi(Q)-\psi(P)=\int\phi(Q)d(Q-P)+R_2(Q,P),\qquad P[\phi]=0,P[\phi^2]<\infty\]
    が可能だとすると，道ごとの微分可能性
    \[\pp{}{\ep}\psi(P_\ep)|_{\ep=0}=\int\phi(z;P)s_\ep(z)dP(z)\]
    が従う．この時点で$\P$をなんかの多様体だと思ってるよなあ．
\end{discussion}

\begin{definition}[influence function, influence curve, gradient (for a functional)]
    上述のvon Mises展開を許す関数$\psi$を，推定したい汎関数$\psi$に関する\textbf{影響関数}または\textbf{影響曲線}または\textbf{勾配}という．
\end{definition}

\subsection{有効影響関数}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    畳み込み定理の系として得られる$\Var[\phi]$は，Cramer-Raoの下界の，ノンパラメトリックな対応物である．このときの$\phi$を，efficient influence functionという．
    asymptotic minimaxの意味で，$\sqrt{n}(\wh{\psi}-\psi)\Rightarrow N(0,\Var[\phi])$に優る推定量はない．なおこの式は，推定量$\wh{\psi}$の影響関数の定義式\ref{def-IF-for-estimator}と同じ．
\end{tcolorbox}

\begin{problem}
    ノンパラメトリックモデル$\P$の下界を，asymptotic minimaxの意味で与えたい．
    すなわち，すべての部分モデルの下界の中で最大のものを与えたい．
\end{problem}
\begin{remark}
    なお，asymptotic minimax senseとは，統計的決定理論の用語である（駒木先生など）．
    ある特定の点において驚くほど精度がよい推定量などが構成可能であるが，これを病的として見向きしないことが出来る枠組みが決定理論である．
    これが，「最適」という一般的用語が，暗黙のうちには特定の意味(minimax sense)を持つ背景である．
\end{remark}
\begin{discussion}
    $\psi$が道ごとに微分可能ならば，Cauchy-Schwarzの不等式より，
    \[\sup_{\P_\ep}\frac{\psi'(P_\ep)^2}{E[s^2_\ep]}=\sup_{h}\frac{P[\phi h]^2}{P[h^2]}\le P(\phi^2)=\Var[\phi]\]
    等号成立は，$\phi$が有効な部分モデルのスコア(valid submodel score)である場合．ただし，$\phi$がvalid scoreであるとは，$\phi$が接空間内にあることをいう．接空間とは，score spaceの閉包をいう．
\end{discussion}

\begin{definition}[efficient influence function]
    平均$0$で有限な分散を持つ$\phi:\X^n\to\R$であって，任意の部分モデル$\{P_\ep\}\subset\P$に対して
    \[\pp{}{\ep}\psi(P_\ep)|_{\ep=0}=\int\phi(z;P)s_{\ep}(z)dP(z)\]
    を満たすものを，\textbf{有効影響関数}という．
\end{definition}

\begin{theorem}[uniqueness of EIF]
    $\cT$を接空間，$\phi$を影響関数，$\Pi$を射影演算子とする．このとき，$\Pi(\phi|\cT)$がただ一つの有効影響関数である．
\end{theorem}

\subsection{影響関数の例}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    影響関数$\phi$の形がわかっているとき，弱い仮定を置くことで有効な推定量を構成できる．
\end{tcolorbox}

\begin{discussion}[影響関数の求め方]\mbox{}
    \begin{enumerate}
        \item pathwise derivative $\psi'(P_\ep)$を計算し，$\phi$について解く．
        \item データは離散だとして，point mass contaminationの方向に対するGateaux微分を計算するほうがしばしば簡単になる（吉田先生の本にあった方法）．
        \item 単純な影響関数を既知として，連鎖律やLeibniz則によって構成する．
    \end{enumerate}
\end{discussion}

\begin{example}[平均]
    $\psi=E[Z]=\int zdP(z)$の影響関数は，
    \begin{align*}
        \psi'(P_\ep)&=\pp{}{\ep}\int zdP_\ep(z)|_{\ep=0}=\int z\pp{}{\ep}dP_\ep(z)|_{\ep=0}\\
        &=\int z\paren{\pp{}{\ep}\log dP_\ep(z)}dP_\ep(z)|_{\ep=0}\\
        &=\int(z-\psi)\paren{\pp{}{\ep}\log dP_\ep(z)}|_{\ep=0}dP(z)
    \end{align*}
    より，$\psi(z;P)=z-\psi$が有効影響関数で，von Mises展開は$R_2=0$について成り立つ．
    また，最適な推定量は$\wh{\psi}=\bP_n[Z]$で，
    \[\Var[\sqrt{n}(\wh{\psi}-\psi)]=\Var[Z]=\Var[\phi].\]
\end{example}

\section{ノンパラメトリック推定}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    続いて上界を考えたい．つまり，普通の機械学習によっても$P$が推定できるような推定量の構成法を作りたい．
\end{tcolorbox}

\begin{notation}
    $\bP[f]=\bP[f(Z)]=\int f(z)d\bP(z)$とする．
\end{notation}

\subsection{導入}

\begin{problem}
    von Mises展開可能な$\psi$
    \[\psi(\Q)-\psi(\bP)=\int\phi(\Q)d(\Q-\bP)+R_2(\Q,\bP)\]
    を考える．このとき，plug-in推定量は，$\wh{\bP}[\phi(\bP)]=0$より，
    \[\psi(\wh{\bP})-\psi(\bP)=-\int\phi(\wh{\bP})d\bP+R_2(\wh{\bP},\bP)\]
    となり，１次のバイアスを持つ．まずは，これを推定することを考えたい．
\end{problem}
\begin{discussion}
    $\wh{\psi}:=\psi(\wh{\bP})+\bP_n[\phi(\wh{\bP})]$をワンステップ推定量とすると，
    次のように展開できる．
    \begin{align*}
        \wh{\psi}-\psi&:=\Square{\psi(\wh{\bP})+\bP_n[\phi(\wh{\bP})]}-\psi
        =(\bP_n-\bP)\phi(\wh{\bP})+R_2(\wh{\bP},\bP)\\
        &=(\bP_n-\bP)(\phi(\wh{\bP})-\phi(\bP))+(\bP_n-\bP)\phi(\bP)+R_2(\wh{\bP},\bP).
    \end{align*}
    \begin{enumerate}
        \item 分散が縮んでいくような項の標本平均．
        \item 固定された関数に対する標本平均$\bP_n[\phi(\bP)]$に他ならない．これはCLTにより収束する．
        \item NP条件を与えられたとき，無視できる．
    \end{enumerate}
    (1),(3)を処理できれば，最適な推定量が得られる．
    これらは，古典的にいう推定方程式とワンステップ推定(one-step correction)に対応する．
\end{discussion}

\subsection{経験過程と標本分割}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    標本分割法により，経験過程論の精緻な議論はある意味廃された．
    なんというか，精密なリサンプリングアルゴリズムで洗練されていくのは，すごくきれいな計算機科学的な仕事に思える．
\end{tcolorbox}

\begin{problem}
    $\bG_n:=\sqrt{n}(\bP_n-\bP)$としたとき，
    \begin{enumerate}
        \item $R_1:=\bG_n(\phi(\wh{\bP})-\phi(\bP))$．
        \item $R_2:=\sqrt{n}R_2(\wh{\bP},\bP)$．
    \end{enumerate}
    が$0$に$\bP$-確率収束することを示したい．
    これが示せれば，$\sqrt{n}(\wh{\psi}-\psi)\Rightarrow N(0,\Var[\phi])$を得る．これは最適性を意味する．
\end{problem}
\begin{discussion}
    結論から言うと，$R_1$については，次のいずれかが成り立つ時，解決できる．
    \begin{enumerate}
        \item $\phi(\bP)$が複雑すぎない時（経験過程論による）
        \item $\wh{\bP},\bP_n$を分割してoverfittinを防ぐ（標本分割法による）
    \end{enumerate}
    $R_2$については，基本的にはあまり一般的な結論は引き出せず，ケースバイケースで対処する必要がある．
    しかし，今までのbias-corrected estimatorでない方法とは違って，複雑なノンパラメトリックモデルにおいても，$R_2$は結局無視できるほどに収束が早い．
    一般のplug-inなどでは，この項は収束するのが非常に遅い．
\end{discussion}

\begin{lemma}[19.24 van der Vaart (2000)]
    次を仮定する．
    \begin{enumerate}[({A}1)]
        \item $\wh{f},f\in\F$をDonkserクラス$\F$の元とする．
        \item $\norm{\wh{f}-f}^2=o_\bP(1)$．
    \end{enumerate}
    このとき，
    \[\bG_n\wh{f}=\bG_nf+o_\bP(1)\]
\end{lemma}

\begin{lemma}[sample splitting lemma (Kennedy et al.)]\footnote{"Sharp instruments for classifying compilers and generalizing causal effects"}
    $\wh{f}$を標本$Z^N=(Z_{n+1},\cdots,Z_N)$から定めた推定量とし，$\bP_n$で$Z^N$と独立に観測した標本$(Z_1,\cdots,Z_n)$が定める経験過程とする．
    このとき，$\sqrt{n}(\bP_n-\bP)(\wh{f}-f)=O_P(\norm{\wh{f}-f})$．
\end{lemma}
\begin{remarks}
    これにより，$L_2$-一致性を持ち，標本分割数$K$が有限である限り，$R_1=o_P(1)$を得るし，
    モデルの複雑性に制限がないから，機械学習法も使える．

\end{remarks}

\subsection{第二の剰余項}

\begin{example}
    $\psi=E[Z]$のとき，$R_2=0$．
    これは，$f=\frac{A}{\pi}(Y-g)+g$などの，あらゆる$f(Z)$に対して成り立つ．
    すなわち，IPW-スタイルの影響関数は，$\pi$が既知の場合の影響関数に他ならない．
\end{example}

\begin{example}
    $\psi=\int p(z)^2dz$の影響関数は$\phi(zlp)=2(p(z)-\psi)$である．
    一般のplug-in $\bP_n(\wh{p})$は$\sqrt{n}$-一致性を持たない．
    が，$\wh{p}$を標本分割によって得て，IF-based bias-corrected推定量
    \begin{align*}
        \wh{\psi}&=\psi(\wh{\bP})+\bP_n[\phi(\wh{\bP})]\\
        &=\int\wh{p}^2+\bP_n(2\wh{p}-\int\wh{p}^2)=2\bP_n(\wh{p})-\int\wh{p}^2
    \end{align*}
    を考えると，
    \[\wh{\psi}-\psi=2(\bP_n-\bP)(\wh{p}-p)+2(\bP_n-\bP)p+R_2(\wh{p},p),\quad R_2(\wh{p},p)=-\int(\wh{p}-p)^2.\]
    あとはdensity estimationの問題になるが，従来の半分のsmoothnessしか要求しない．
\end{example}

\begin{example}
    $\psi=E[E[Y|X,A=1]]=:E[\mu(X)]$の影響関数は
    \[\phi(Z;P)=\frac{A}{\pi(X)}\paren{Y-\mu(X)}+\mu(X)-\psi.\]
\end{example}

\section{open problems}

\begin{enumerate}
    \item 新たな汎関数についてはどうする？
    \item $\sqrt{n}$-収束が達成不可能な場合はどうする？
    \item $\psi$が道ごとに微分可能でない場合はどうする？
\end{enumerate}

\chapter{適応的実験}

\begin{quotation}
    適応的実験とバンディット問題と，因果推論の関係を考える．
\end{quotation}

\section{離散時間マルチンゲール}

\begin{definition}[filtration]\mbox{}
    \begin{enumerate}
        \item 確率空間$(\Om,\F,P)$上の，$\F$の部分$\sigma$-加法族の増大列$(\F_k)_{k\in\N}$を\textbf{離散時間フィルトレーション}という．
        \item ４つ組$(\Om,\F,(\F_k),P)$を\textbf{離散時間確率基}という．
        \item $(\xi_k)_{k=1,2,\cdots}$が\textbf{離散時間マルチンゲール差分列}とは，$\forall_{k\in\N}\;\xi_k$は$\F_k$可測・可積分で，$E[\xi_k|\F_{k-1}]=0\as$を満たすことをいう．
        \item $(X_k)_{k\in\N}$が\textbf{離散時間マルチンゲール列}とは，$\forall_{k\in\N}\;\xi_k$は$\F_k$可測・可積分で，$E[\xi_k|\F_{k-1}]=X_{k-1}\as$を満たすことをいう．
    \end{enumerate}
\end{definition}

\begin{lemma}
    離散時間マルチンゲール差分列$(\xi_k)_{k=1,2,\cdots}$と，$\F_0$-可測な可積分確率変数$X_0$と，$\F_{k-1}$-可測確率変数$H_{k-1}$であって$E[\abs{H_{k-1}\xi_k}]<\infty$であるようなものが与えられた場合，
    \[X_k=X_0+\sum^k_{j=1}H_{j-1}\xi_j\]
    は離散時間マルチンゲール列になる．
\end{lemma}

\section{停止時間と任意抽出定理}

\begin{definition}\mbox{}
    \begin{enumerate}
        \item $T$が\textbf{停止時刻}であるとは，確率変数$T:\Om\to\N\cup\{\infty\}$であって，$\forall_{k\in\N}\;\Brace{\om\in\Om\mid T(\om)=k}\in\F_k$を満たすことをいう．
        \item 停止時刻が\textbf{有限}であるとは，$\forall_{\om\in\Om}\;T(\om)<\infty$を満たすことをいう．
        \item 停止時刻が\textbf{有界}であるとは，$\exists_{c\in\R}\;\forall_{\om\in\Om}\;T(\om)\le c$を満たすことをいう．
    \end{enumerate}
\end{definition}

\section{離散時間マルチンゲールに関する中心極限定理}

\begin{theorem}
    離散時間確率基$(\Om^n,\F^n,(\F^n_k),P^n)$上の$p$-次元離散時間マルチンゲール差分列$(\xi^n_k)=((\xi^{n,1}_k,\cdots,\xi^{n,p}_k)^T)$と有限停止時刻の列$(T_n)$を考える．
    \begin{enumerate}
        \item $\forall_{i,j\in[p]}\;\sum^{T_n}_{k=1}E^n[\xi^{n,i}_k\xi^{n,j}_k|\F^n_{k-1}]\xrightarrow{p}C^{(i,j)}\in\R$．
        \item $\forall_{\ep>0}\;\sum^{T_n}_{k=1}E^n[\norm{\xi^n_k}^21_{\Brace{\norm{\xi^n_k}>\ep}}|\F^n_{k-1}]\xrightarrow{p}0$．
    \end{enumerate}
    が成り立つならば，
    \[\sum^{T_n}_{k=1}\xi^n_k\xrightarrow{d}\N_p(0,\Sigma)\quad\paren{\Sigma=(C^{(i,j)})_{i,j\in[p]}}\]
    が成り立つ．
\end{theorem}

\begin{lemma}[Lindeberg条件の十分条件 (Lyapunov)]
    $\exists_{\delta>0}\;\sum^{k_n}_{k=1}E^n[\norm{\xi^n_k}^{2+\delta}|\F^n_{k-1}]\xrightarrow{d}0$がなりたてば，(1),(2)が従う．
\end{lemma}

\section{multiarm banditについて}

armにresorceを最適にallocateする問題をいう．
最適制御／確率的スケジューリングの一形式でもあり，特に強化学習の一形式でもある（情報が増えていくので）．
スロットマシンのことをone-armed banditという，洒落た表現だ．
ここから，ギャンブラーの気持ちになればよい．
眼の前のarmの"exploitation"を選ぶか，"exploration"に時間を使うかの選択（これをpolicyという）を迫られるのである．
製薬会社の経営方針などをモデルするのにも使われる．
Robinsが1952年にこの問題の重要性に注目した．
\begin{enumerate}
    \item 製薬会社の経営方針．
    \item 人道的な治験．
    \item インターネット広告配信，推薦システム．
    \item ゲーム木探索．
    \item 通信のルーティング．
\end{enumerate}

\section{傾向スコア}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    arm $w$を時刻$t$に下げる確率$e_t(w),e:T\times\W\to[0,1]$を，傾向スコアという．
\end{tcolorbox}

たとえば、喫煙の影響を知りたい場合を考える。人々を喫煙群に無作為に割り付けることは非倫理的であるため、観察研究が必要である。喫煙群と非喫煙群とを単純に比較することによって処置効果を推定すると、喫煙率に影響する要因（性別や年齢など）によるバイアスが生じる。PSM では、処置群とコントロール群の制御変数（この例では性別や年齢など）を同じくらいにすることによって、これらのバイアスを制御することを目指す。\footnote{\url{https://ja.wikipedia.org/wiki/傾向スコア・マッチング}}

\begin{definition}[propensity score]\mbox{}
    \begin{enumerate}
        \item $Z_i\in 2$は被験者$i$が処置群に割り付けられたか，コントロール群に割り付けられたかを表す．
        \item バックグラウンド変数$X_i$は被験者$i$への割り当て前に観測された種々のデータとする．
        \item バックグラウンドで観測された共変量$X$に対する，処置の条件付き確率を
        \[e(x):=P[Z=1|X=x]\]
        と定め，これを\textbf{傾向スコア}という．
    \end{enumerate}
\end{definition}

\section{notation}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=RosenbaumとRubinを超えていく]
    adaptivityに由来するバイアスによって，不偏推定量がぐるぐる変わってしまう．
    片方のarmのみsampled moreなので，バイアスがかかる．
    これの代表的な解決がpropensity score matching (83)であり，見事にバイアスは消えるが，今度は漸近分布の正規性が消える．
    したがってCLTが出来なかった．
    特に或るアームの引かれる確率(assignment probability)が低い場合は裾の重い極限分布をもち，統計的推論が困難になる．
    あるいは実際の実験は付値確率が極限分布が収束しない状態で終わる．
    平均を整えても分散が爆発する．
    $h_t$が特定の条件を満たすように実験を設計することで，
    バイアスは掛かるが平均が良い推定量を得て，
    それは利用可能性の高いデータとなる．
\end{tcolorbox}

\begin{definition}
    $(\Om,\F,P)$を確率空間とする．$\W,\Y$を距離空間とする．
    \begin{enumerate}
        \item $W_t:H_t\to\W$はarm number，すなわち実現された処置(realized treatment)．歴史に依存する先天的に定義された確率分布(bandit algorithm)に従う．
        \item $Y_t:\W\to\Y;W_t\mapsto Y_t(W_t)$は観測された結果．２値関数に落とし込まれた場合は潜在結果(potential outcome)で，$t$番目の患者が$w$にassignされた場合$1$となる．armならreward．
        \item $m:\W\to\R;\om\to E[Y_t(\om)]$は$Y_t$に関する平均潜在結果(mean potential outcome)を表す有界な可測関数で，$\wh{m}_t:\W\times H_{t-1}\to\R$はその推定量．これは一致性を持たなくても良い．
        論文\cite{Policy Evaluation}では$Q$という文字が用いられている．
        \item $\Delta(w,w'):=E[Y_t(w)]-E[Y_t(w')]$とする．
        \item $H_t:=\Brace{(Y_s,W_s)\in\Y\times\W\mid s\in t+1}=\{(Y_s,W_s)\}_{s=0,1,\cdots,t}$は歴史．$\mathbf{H}:=P(\Y\times\W)$とする．\footnote{\cite{誉賛}第一稿では，歴史$H_t$は$2t$-組として表現されている．論文\cite{Policy Evaluation}では$H^{t}$としている．}
        \item $\H=(\H_t)_{t\in T+1}$は$\H_t=\sigma[H_t]$とするフィルトレーションである．
        \item $e_t(w):=P[W_t=w|H^{t-1}]$はassignment probability，傾向スコアという．\footnote{time-varying and decided via some known algorithm, as it is the case with many popular bandit algorithms such as Thompson sampling\cite{Policy Evaluation}}
        \item $P_{t-1}:H_t\times\B_\W\to\R$は正則条件付き確率とする．
        \item 条件付き期待値$E_{t-1}:\Y\to\R$を，$E[f(W_t)]$を$E[f]$と略記する．
        \item $\psi_{h_{t-1}}:L^2(\W,\B_\W,P_{t-1}(h_{t-1},\cdot))\to\R$は，$H^{t-1}$-条件付き自乗積分可能な関数（平均潜在結果）の空間上の
        有界線形汎関数であり，合成関数$\psi(m)$を推定することを考える．arm毎の平均$\psi:=\ev_w$など，推定したい統計量を表す．
        \item $h_t$はevaluation weightで，傾向スコアを打ち消すことで分散を収束させることを考える．
    \end{enumerate}
\end{definition}

\begin{theorem}
    $\psi$の一意的なRiez-representor $\gamma(-;H^{t-1})\in L_2(P_{t-1})$
    \[\forall_{f\in L_2(P_{t-1})}\;E[\gamma(W_t;H^{t-1})f(W_t)|H^{t-1}]=\psi(f)\]
    が存在する．
\end{theorem}
\begin{remarks}
    $\gamma(W_t)$とは，AIPWの$\frac{1_{\Brace{W_t=w}}}{e_t(w)}$に対応する．
\end{remarks}
\begin{example}
    arm毎の平均$\psi:=\ev_w$のRiez representerは$\gamma_t=\frac{1_{\Brace{-=w}}}{e_t(w)}$である．

\end{example}

\begin{notation}\mbox{}
    \begin{enumerate}
        \item $f(\om)=E_{t-1}[\gamma_t(w_t)f(w_t)]$をRiez-representerとする．
        \item 下付き文字$m_t,\gamma_t$は条件付き$m(-|H^{t-1}),\gamma(-|H^{t-1})$の略記である．同様に，$E[X|H^{t-1}]=E_{t-1}[X]$と表す．
    \end{enumerate}
\end{notation}

\section{Malliavin calculus}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    $Z$のように，漸近分布から未知量を消すために標準偏差の推定量$\wh{\sigma}$で割って規格化することを，Studentizationまたはself-normalized estimatorという．
    $t$-分布を発見したWilliam Gossetによる．
\end{tcolorbox}

\section{adaptive weightの構成}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    適応的実験とは強化学習と実験計画の融合である．
    bandit algorithmで最適化される．banditとは，one-armed banditという別称を持つスロットの攻略法（どの台に賭けるか）の問題として1950sに始まったため．
    スロット台のことをarmと呼ぶのか．
    そうして得た結果の最大活用を考える．

    assignment probabilityが収束しない場合は，そのデータからの推論を困難にする．
    ここで，assignment probabilityが収束し，その極限分布に３つの仮定を課すと，
    頻度主義的な信頼区間が計算できることを提案\cite{Policy Evaluation}．
\end{tcolorbox}

\begin{axiom}[adaptive weightに関する公理]\label{axiom-adaptive-weights}
    evaluation weights $h_t$は
    \begin{enumerate}
        \item (Infinite Sampling) $\frac{\paren{\sum^T_{t=1}h_t}^2}{E\Square{\sum^T_{t=1}h_t^2\gamma_t^2}}\xrightarrow[T\to\infty]{p}\infty$．\footnote{Bandid algorithmはどのarmも無限回assignする．}
        \item (Variance Convergence) $\exists_{p>1}\;\frac{\sum^T_{t=1}h_t^2E_{t-1}[\gamma^2_t]}{E\Square{\sum^T_{t=1}h_t^2\gamma_t^2}}\xrightarrow[T\to\infty]{L_p}1$．\footnote{条件付き期待値が，条件付きでない期待値に一致する．}
        \item (Bounded Moments / Lyapunov condition) 
        $\exists_{\delta>0}\;\frac{\sum^T_{t=1}h_t^{2+\delta}E_{t-1}[\abs{\gamma_t}^{2+\delta}]}{E\Square{\sum^T_{t=1}h^2_t\gamma^2_t}^{1+\delta/2}}\xrightarrow[T\to\infty]{p}0$．\footnote{$e_t=1/\gamma_t$のdecayがいくら速くても，$h_t$も分散を収束させるくらいには十分速い．}
    \end{enumerate}
    を満たす．
    ただし，$\gamma_t$は$\frac{1}{e_t}$などの，荷重によって飼い慣らしたい量となる．
\end{axiom}



\subsection{scoring rule}

\begin{definition}
    $\wh{\Gamma}$が
    $Q(w)$に対するunbiased scoring ruleであるとは，
    $\forall_{w\in\W}\;\forall_{t\in[T]}\;E\Square{\wt{\Gamma}_t(w)|H^{t-1}}=Q(w)$
    が成り立つことをいう．
\end{definition}
\begin{example}\mbox{}\label{exp-AIPW}
    \begin{enumerate}
        \item inverse propensity score weighted $\wh{\Gamma}_t^{IPW}(w):=\frac{1_{\Brace{W_t=w}}}{e_t(w)}Y_t$．
        \item augmented inverse propensity weightedはregression adjustmentを加える(Robins 94)．
        \[\wh{\Gamma}_t^{AIPW}(w):=\frac{1_{\Brace{W_t=w}}}{e_t(w)}Y_t+\paren{1-\frac{1_{\Brace{W_t=w}}}{e_t(w)}}\wh{m}_t(w)\]
    \end{enumerate}
\end{example}

\begin{proposition}
    不偏スコア規則$\wh{\Gamma}$が定める量
    $\wh{Q}_t(w):=\frac{1}{T}\sum^T_{t=1}\wh{\Gamma}_t(w)$は，$Q$について不偏である：
    $E[\wh{Q}_T(w)]=Q(w)$．
    特に，$\wh{\Gamma}_t$が$t\in T$に相関する場合も成り立つ．
\end{proposition}
\begin{proof}
    繰り返し期待値の法則による：
    \begin{align*}
        E\Square{\wh{Q}_T(w)}&=E\Square{\frac{1}{T}\sum^T_{t=1}\wh{\Gamma}_t(w)}\\
        &=\frac{1}{T}\sum^T_{t=1}E\Square{E\Square{\wh{\Gamma}_t(w)|\H^t}}\\
        &=\frac{1}{T}\sum^T_{t=1}E[Q(w)]=Q(w).
    \end{align*}
\end{proof}

\subsection{漸近的正規なtest statistics}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    Qualitatively, what we need for normality is for the variability of the estimator to be deterministic.
    だから，「逆数にしても発散しない，修正された傾向スコア」のようなものが必要になるのだ．
    それは，単にunbiased scoring ruleを平均して推定量とするのではなく，evaluation weight $(h_t)_{t\in T+1}$で荷重する．
    この$(h_t)$をうまく選ぶことで，assignment probability $e_t(w)$を打ち消して挙動を漸近的正規にする．
    \begin{quote}
        With such weights, the adaptively-weighted AIPW estimator (6), when normalized by an estimate of its standard deviation, has a centered and normal asymptotic distribution. Similar “self-normalization” schemes are often key to martingale central limit theorems (see e.g., de la Pen ̃a et al., 2008).\cite{Policy Evaluation}
    \end{quote}
\end{tcolorbox}

\begin{definition}[adaptively-weighted AIPW estimator]
    \[\wh{Q}^h_T(w):=\frac{\sum^T_{t=1}h_t(w)\wh{\Gamma}_t^{AIPW}(w)}{\sum^T_{t=1}h_t(w)}.\]
\end{definition}

\begin{lemma}
    $(h_t)$が$\sum^T_{t=0}h_t=1$を満たすならば，これが定めるadaptively-weighted AIPW推定量$Q:=\sum_{t=0}^Th_t(w)\wh{\Gamma}(w)$は不偏である：$E\Square{h_t(w)\wh{\Gamma}(w)|H^{t-1}}=h_t(w)Q(w)$．
\end{lemma}

\section{General settings}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
    title=]
    傾向スコアによるAIPWを，$\psi$のRiez representerと捉える枠組みは誰が気づいたのか．
\end{tcolorbox}


\begin{definition}
    $(\Om,\F,P)$を確率空間とする．$\W,\Y$を距離空間とする．$\W$は可算とする．$T\in\N$とする．
    \begin{enumerate}
        \item $H_t=((Y_s,W_s);s\in [t])$を歴史とする．これは$\mathbf{H}_t:=(\Y\times\W)^t$の元である．各歴史が定める$\sigma$-加法族を$\H_t:=\sigma[H_t]$とし，$\H:=(\H_t)_{t\in[T]}$をfiltrationとする．
        \item この上の正則条件付き確率を$P_{t-1}:H_t\times\B_\W\to\R$と定める．
        \item $W_t:H_t\to\W$はarm number，すなわち実現された処置(realized treatment)で，歴史に依存する先天的に定義された確率分布(bandit algorithm)に従う．
        その確率$e_t(w):=P[W_t=w|H^{t-1}]$はassignment probability，または傾向スコアという．
        \item $Y_t:\W\to\Y;W_t\mapsto Y_t(W_t)$は観測された結果．２値関数に落とし込まれた場合は潜在結果(potential outcome)で，$t$番目の患者が$w$にassignされた場合$1$となる．armならreward．
        \item $m:\W\to\R;\om\to E[Y_t(\om)]$は$Y_t$に関する平均潜在結果(mean potential outcome)を表す有界な可測関数で，$\wh{m}_t:\W\times H_{t-1}\to\R$はその推定量．これは一致性を持たなくても良い．
        また，因果効果を$\Delta(w,w'):=E[Y_t(w)]-E[Y_t(w')]$とする．
        \item $\psi_{h_{t-1}}:L^2(\W,\B_\W,P_{t-1}(h_{t-1},\cdot))\to\R$は，$H^{t-1}$-条件付き自乗積分可能な関数（平均潜在結果）の空間上の
        有界線形汎関数であり，合成関数$\psi(m)$を推定することを考える．arm毎の平均$\psi:=\ev_w$など，推定したい統計量を表す．
        論文\cite{Policy Evaluation}では$Q$という文字が用いられている．
        \item Riesz representer $\gamma_{h_{t-1}}\in L^2(\W,\B_\W,P_{t-1}(h_{t-1},-))$を，
        \[\forall_{f\in L^2(\W,\B_\W,P_{t-1}(h_{t-1},-))}\;\psi_{h_{t-1}}(f)=\int_\W\gamma_{h_{t-1}}(w)f(w)P_{t-1}(h_{t-1},dw)=E[\gamma_{h_{t-1}}(W_t)f(W_t)|H_{t-1}=h_{t-1}]\]
        を満たすものと定める．
        \item $(h_t)_{t\in[T]}$はevaluation weightと呼ばれる実数列で，傾向スコアを打ち消すことで分散を収束させることを考える．
        \begin{axiom}[adaptive weightに関する公理]\label{axiom-adaptive-weights}
            evaluation weights $h_t$は
            \begin{enumerate}
                \item (Infinite Sampling) $\frac{\paren{\sum^T_{t=1}h_t}^2}{E\Square{\sum^T_{t=1}h_t^2\gamma_t^2}}\xrightarrow[T\to\infty]{p}\infty$．\footnote{Bandid algorithmはどのarmも無限回assignする．}
                \item (Variance Convergence) $\exists_{p>1}\;\frac{\sum^T_{t=1}h_t^2E_{t-1}[\gamma^2_t]}{E\Square{\sum^T_{t=1}h_t^2\gamma_t^2}}\xrightarrow[T\to\infty]{L_p}1$．\footnote{条件付き期待値が，条件付きでない期待値に一致する．}
                \item (Bounded Moments / Lyapunov condition) 
                $\exists_{\delta>0}\;\frac{\sum^T_{t=1}h_t^{2+\delta}E_{t-1}[\abs{\gamma_t}^{2+\delta}]}{E\Square{\sum^T_{t=1}h^2_t\gamma^2_t}^{1+\delta/2}}\xrightarrow[T\to\infty]{p}0$．\footnote{$e_t=1/\gamma_t$のdecayがいくら速くても，$h_t$も分散を収束させるくらいには十分速い．}
            \end{enumerate}
            を満たす．
            ただし，$\gamma_t$は$\frac{1}{e_t}$などの，荷重によって飼い慣らしたい量となる．
        \end{axiom}
    \end{enumerate}
\end{definition}

\begin{notation}\mbox{}
    \begin{enumerate}
        \item 条件付き期待値$E_{t-1}:\Y\to\R$について，$E_{t-1}[f(W_t)]$を$E_{t-1}[f]$と略記する．
        \item $\gamma(W_t;H^{t-1})=\gamma_{H^{t-1}}(W_t)$を$\gamma_{t}(W_t)$と略記する．$E_{t},m_{t}$なども同様．
    \end{enumerate}
\end{notation}

\begin{definition}[一般化された不偏スコア規則]
    一般化された不偏スコア規則$\wh{\Gamma_t}:\W\to\R$を，次のように定める：
    \[\wh{\Gamma_t}(W_t):=\psi(\wh{m})+\gamma(W_t;H^{t-1})(Y_t-\wh{m}(W_t;H^{t-1})).\]
\end{definition}

\begin{lemma}[不偏スコアが不偏推定量となっている]\mbox{}
    \begin{enumerate}
        \item $E[\gamma_{H_{t-1}}(W_t)(Y_t-m(W_t))|\H_{t-1}]=0$．
        \item $\psi_{H_{t-1}}(\wh{m}_t(\cdot,H_{t-1}))-\psi_{H_{t-1}}(m)-\gamma_{H_{t-1}}(W_t)(\wh{m}_t(W_t,H_{t-1})-m(W_t))$もmartingale差分列である．
        \item ２つの和を$\dot{\xi^T_t}$とおくと，$\dot{\xi_t}=\wh{\Gamma}_t-\psi_{H_{t-1}}(m)$と表せて，
        これもmartingale差分列である．
    \end{enumerate}
\end{lemma}

\section{適応的実験のための荷重した統計量の中心極限定理}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    不偏スコア$\wh{\gamma}_t$を荷重$(h_t)$で調整した統計量$\wh{\psi_T}$についての中心的な極限定理を示す．
\end{tcolorbox}

\begin{theorem}[中心的極限定理]\label{thm-CLT-for-adaptive}
    次を仮定する：
    \begin{enumerate}
        \item 見本過程$Y_t(w)$の分散は上に有界で，$0$でない(away from zero)．
        \item $\delta>0$が存在して平均$E[\abs{Y_t(w)}^{2+\delta}]$が$w\in\W$について一様に上に有界．
        \item Riesz表現子$\gamma_t$は$\exists_{b>0}\;\forall_{t\in[T]}\;E_{t-1}[\gamma^2_t]>b$を満たす．
        \item $\wh{m}_t$は$\exists_{m_\infty\in L^2(\W,\B_\W,P_{t-1}(h_{t-1},-))}\;\norm{\wh{m}_t-m_\infty}_{L_\infty(P_{t-1})}\xrightarrow[t\to\infty]{\as}0$を満たす一様有界な推定量の列．
    \end{enumerate}
    荷重$(h_t)_{t\in[T]}$が次のいずれか：
    \begin{enumerate}[(a)]
        \item $\wh{m}_t$の一致性：$\norm{\wh{m}-m}_{L_\infty(P_{t-1})}\xrightarrow[t\to\infty]{\as}0$．
        \item $E_{t-1}[\gamma_t^2]\xrightarrow[t\to\infty]{\as}\o{\gamma}^2_\infty\in(0,\infty]$
    \end{enumerate}
    を満たすならば，
    \begin{enumerate}
        \item (一致性) 推定量
        \[\wh{\psi}_T:=\frac{\sum^T_{t=1}h_t\wh{\Gamma}_t}{\sum^T_{t=1}h_t}\]
        は$\psi(m)$に確率収束し，
        \item (漸近的正規性) student化した統計量は漸近的に正規である：
        \[\frac{\wh{\psi_T}-\psi(m)}{\wh{V}_T^{1/2}}\xrightarrow[T\to\infty]{d}\cN(0,1)\quad\wh{V}_T:=\frac{\sum^T_{t=1}h_t^2(\wh{\Gamma}_t-\wh{\psi}_T)^2}{\paren{\sum^T_{t=1}h_t}^2}.\]
    \end{enumerate}
\end{theorem}
\begin{remarks}
    分散は，「$\wh{\psi}_T$の２乗の，２乗加重平均」と推定する．
\end{remarks}

\begin{theorem}[荷重の構成]
    Riesz表現子の列$(\gamma_t)$が
    \[\exists_{\delta>0}\;\exists_{\delta\in\left[0,\frac{\delta}{2+\delta}\right)}\;\exists_{C,C'>0}\;\Square{\paren{\frac{E_{t-1}[\abs{\gamma_t}^{2+\delta}]}{E_{t-1}[\gamma^2_t]^{2+\delta}}\le C}\land\paren{\forall_{t\in[T]}\;E_{t-1}[\gamma^2_t]\le C't^\al}}\]
    を満たすとする．この時，
    \begin{enumerate}
        \item $\forall_{t<T}\;\gamma_T<1$，
        \item $\gamma_T=1$，
        \item $\exists_{C''>0}\;\frac{1}{1+T-t}\le\lambda_t\le C''\frac{E_{t-1}[\gamma^2_t]^{-1}}{t^{-\al}+T^{1-\al}-t^{1-\al}}$
    \end{enumerate}
    を満たす付値率(allocation rate)について，
    \[h^2_tE_{t-1}[\gamma_t^2]=\paren{1-\sum^{t-1}_{s=1}h_s^2E_{s-1}[\gamma_s^2]}\lambda_t\]
    によって帰納的に定義した荷重$(h_t)$は３つの公理\ref{axiom-adaptive-weights}を満たす．
\end{theorem}

\section{連続martingaleの周りに展開される確率変数の漸近展開}

\begin{tcolorbox}[colframe=ForestGreen, colback=ForestGreen!10!white,breakable,colbacktitle=ForestGreen!40!white,coltitle=black,fonttitle=\bfseries\sffamily,
title=]
    もし推定量$\wh{\psi}_t$が$M_n+r_nN_n$の形を持ち，いくつかの仮定を満たす確率過程だと証明できたならば，吉田先生のMalliavin解析を用いた論文の手法を適用することで漸近展開をすることができる．
\end{tcolorbox}

\begin{definition}
    ある正なpredictableな過程$a^T=(a^T_t)$と$0$に収束する正実数列$(r_n)$について，
    \begin{enumerate}
        \item $M_t:=\sum^T_{t=1}a_t\dot{\xi_t}$．
        \item $Z_T:=M_T+r_TN_T$．
    \end{enumerate}
    と定める．
\end{definition}

\begin{thebibliography}{99}
    \bibitem{清水昌平}
    清水昌平『統計的因果推論』（講談社，機械学習プロフェッショナルシリーズ，2017）．
    \bibitem{Anastasios A. Tsiatis}
    Anastasios A. Tsiatis "Semiparametric Theory and Missing Data" (Springer, 2006).
    \bibitem{Winship}
    Christopher Winship, Stephen L. Morgan "Counterfactuals and Causal Inference: Methods and Principles for Social Research" 2nd ed. (Cambridge University Press, 2014)
    \bibitem{芝孝一郎}
    芝孝一郎さんのブログ記事\href{https://www.krsk-phs.com/entry/counterfactual_assumptions}{データから因果関係をどう導く？：統計的因果推論の基本、「反事実モデル」をゼロから}
    \bibitem{清水08}
    P. O. Hoyer, S. Shimizu, A. J. Kerminen, and M. Palviainen. Estimation of causal effects using linear non-Gaussian causal models with hidden variables. \textit{International Journal of Approximate Reasoning}, 49(2), pp. 362-378, 2008.
    \bibitem{Policy Evaluation}
    "Confidence Intervals for Policy Evaluation in Adaptive Experiments"
    \bibitem{吉田}
    "Malliavin calculus and asymptotic expansion for martingales" (1997).
    \bibitem{星野}
    星野崇宏『調査観察データの統計科学』
\end{thebibliography}

\end{document}